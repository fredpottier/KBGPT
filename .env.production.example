# =====================================================
# Configuration pour deploiement AWS EC2 avec images ECR
# =====================================================
# Copiez ce fichier vers .env.production et configurez vos valeurs
# Usage:
#   - En local: Copiez vers .env pour developpement
#   - Sur EC2: Copiez vers .env pour production

# =====================================================
# AWS CONFIGURATION (OBLIGATOIRE)
# =====================================================
AWS_ACCOUNT_ID=YOUR_AWS_ACCOUNT_ID  # Votre Account ID AWS
AWS_REGION=eu-west-1                # Region AWS de vos ressources ECR

# =====================================================
# PORTS SERVICES (conservez par defaut sauf conflit)
# =====================================================
APP_PORT=8000          # Port FastAPI backend
FRONTEND_PORT=3000     # Port Next.js frontend
APP_UI_PORT=8501       # Port Streamlit UI (legacy)

# =====================================================
# DEBUG MODE (desactiver en production)
# =====================================================
DEBUG_APP=false        # Debug backend FastAPI
DEBUG_WORKER=false     # Debug worker d'ingestion

# =====================================================
# API KEYS LLM (OBLIGATOIRE)
# =====================================================
# Obtenez vos cles:
#   OpenAI: https://platform.openai.com/api-keys
#   Anthropic: https://console.anthropic.com/
OPENAI_API_KEY=sk-proj-VOTRE_CLE_OPENAI_ICI
ANTHROPIC_API_KEY=sk-ant-VOTRE_CLE_ANTHROPIC_ICI

# =====================================================
# CONFIGURATION MODÈLES LLM (optionnel)
# =====================================================
# Par defaut, utilise les modèles configures dans config/llm_models.yaml
# Decommentez pour override :
# MODEL_VISION=gpt-4o
# MODEL_METADATA=gpt-4o
# MODEL_FAST=gpt-4o-mini
# MODEL_LONG_TEXT=claude-3-5-sonnet-20241022
# MODEL_ENRICHMENT=claude-3-5-haiku-20241022

# =====================================================
# NEO4J KNOWLEDGE GRAPH (OBLIGATOIRE)
# =====================================================
# Configure automatiquement pour Docker Compose
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=CHANGEZ_MOT_DE_PASSE_PRODUCTION  # CHANGEZ en production !

# =====================================================
# QDRANT VECTOR DATABASE
# =====================================================
# Configure automatiquement pour Docker Compose
QDRANT_URL=http://qdrant:6333
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION=knowbase
QDRANT_API_KEY=  # Optionnel, pour securisation supplementaire

# =====================================================
# REDIS CACHE & QUEUE
# =====================================================
# Configure automatiquement pour Docker Compose
REDIS_URL=redis://redis:6379/0

# =====================================================
# HUGGINGFACE MODEL CACHE
# =====================================================
HF_HOME=/data/models

# =====================================================
# AUTHENTIFICATION & SÉCURITÉ
# =====================================================
# Clé secrète JWT (OBLIGATOIRE pour authentification)
# Générez une clé aléatoire sécurisée:
#   - Python: python -c "import secrets; print(secrets.token_urlsafe(32))"
#   - OpenSSL: openssl rand -base64 32
JWT_SECRET=GENERER_CLE_SECRETE_ALEATOIRE_ICI

# Mot de passe Grafana admin (monitoring)
GRAFANA_ADMIN_PASSWORD=CHANGER_MOT_DE_PASSE_GRAFANA

# =====================================================
# PERFORMANCE - PARALLÉLISATION MONO-DOCUMENT
# =====================================================
# Nombre de segments traités en parallèle (optimisé pour 8 vCPU)
# Recommandations par instance:
#   - t3.2xlarge / m5.2xlarge (8 vCPU): 5
#   - c5.4xlarge (16 vCPU): 10
#   - c5.9xlarge (36 vCPU): 15
MAX_PARALLEL_SEGMENTS=5

# LLM Rate Limits (OpenAI)
# Tier 1: 500 RPM, Tier 2: 5000 RPM
OPENAI_MAX_RPM=500
ANTHROPIC_MAX_RPM=100
OSMOSE_TIMEOUT_SECONDS=3600

# =====================================================
# FRONTEND API CONFIGURATION
# =====================================================
# IMPORTANT: NEXT_PUBLIC_* vars sont compilees dans l'image Docker frontend
# Le frontend utilise /api/* (proxy Next.js) qui redirige vers http://app:8000
# FRONTEND_API_BASE_URL n'est plus utilisee (obsolete)

# API URLs internes (ne changez pas, pour communication inter-conteneurs)
NEXT_PUBLIC_API_INTERNAL_URL=http://app:8000
BACKEND_URL=http://app:8000

# =====================================================
# CORS CONFIGURATION
# =====================================================
# Format: origines separees par virgules (sans espaces)
# Exemple: http://1.2.3.4:3000,http://1.2.3.4:8501,https://example.com
# Laisser vide pour localhost uniquement
CORS_ORIGINS=

# =====================================================
# WORKER CONFIGURATION (production)
# =====================================================
DEV_MODE=true                # Mode developpement (auto-reload code)
WORKER_CONCURRENCY=4          # Nombre de workers concurrents (ajuster selon CPU)

# =====================================================
# NGROK TUNNEL (optionnel, pour exposition externe)
# =====================================================
# Decommentez si vous utilisez ngrok pour exposer vos services
# Obtenez vos tokens: https://dashboard.ngrok.com/
# NGROK_AUTHTOKEN=your-ngrok-token-here
# NGROK_DOMAIN=your-domain.ngrok.app

# =====================================================
# PYTHON PATHS (ne changez pas)
# =====================================================
PYTHONPATH=/app:/app/src
KNOWBASE_DATA_DIR=/data

# =====================================================
# NOTES IMPORTANTES
# =====================================================
# 1. SECURITE:
#    - Ne commitez JAMAIS ce fichier avec vos vraies cles API
#    - Changez NEO4J_PASSWORD en production
#    - Utilisez des mots de passe forts (>16 caractères)
#
# 2. PERFORMANCE EC2:
#    - Instance recommandee: t3.xlarge minimum (4 vCPU, 16GB RAM)
#    - Pour charges lourdes: t3.2xlarge (8 vCPU, 32GB RAM)
#    - Ajustez WORKER_CONCURRENCY selon vos vCPUs
#
# 3. COÛTS:
#    - Suivez votre consommation OpenAI/Anthropic
#    - Les appels LLM representent 80-90% des coûts operationnels
#    - Voir doc/etudes/ARCHITECTURE_AGENTIQUE_OSMOSE.md pour optimisation
#
# 4. FRONTEND_API_BASE_URL:
#    - En local: http://localhost:8000
#    - Sur EC2: http://<IP_PUBLIQUE_EC2>:8000
#    - Avec domaine: https://api.votre-domaine.com
#
# 5. SECURITY GROUPS EC2:
#    - Port 22 (SSH): Votre IP uniquement
#    - Port 8000 (API): Votre IP ou 0.0.0.0/0 si public
#    - Port 3000 (Frontend): Votre IP ou 0.0.0.0/0 si public
#    - Port 7474 (Neo4j UI): Votre IP uniquement (admin)
#    - Port 6333 (Qdrant UI): Votre IP uniquement (admin)
