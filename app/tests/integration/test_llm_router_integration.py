"""
Tests d'int√©gration pour le syst√®me de routage LLM.
Ces tests n√©cessitent une configuration compl√®te et ne mockent pas les d√©pendances externes.
"""

import pytest
import logging
from pathlib import Path

from knowbase.common.llm_router import LLMRouter, TaskType


class TestLLMRouterIntegration:
    """Tests d'int√©gration pour le routeur LLM."""

    def test_router_initialization_with_real_config(self):
        """Test l'initialisation avec la vraie configuration."""
        try:
            router = LLMRouter()

            # V√©rifications de base
            assert router._config is not None
            assert "task_models" in router._config
            assert "default_model" in router._config

            print("‚úÖ Routeur initialis√© avec succ√®s")
            print(f"üìã Version config: {router._config.get('version', 'N/A')}")
            print(f"üìã Mod√®le par d√©faut: {router._config.get('default_model', 'N/A')}")

        except Exception as e:
            pytest.fail(f"√âchec de l'initialisation du routeur: {e}")

    def test_provider_detection(self):
        """Test la d√©tection des providers disponibles."""
        router = LLMRouter()

        providers = router._available_providers

        # Au moins un provider devrait √™tre disponible
        assert any(providers.values()), "Aucun provider LLM disponible"

        print("üîå Providers d√©tect√©s:")
        for provider, available in providers.items():
            status = "‚úÖ" if available else "‚ùå"
            print(f"  - {provider}: {status}")

    def test_model_resolution_for_all_tasks(self):
        """Test la r√©solution des mod√®les pour toutes les t√¢ches."""
        router = LLMRouter()

        tasks = [
            TaskType.VISION,
            TaskType.METADATA_EXTRACTION,
            TaskType.LONG_TEXT_SUMMARY,
            TaskType.SHORT_ENRICHMENT,
            TaskType.FAST_CLASSIFICATION,
            TaskType.CANONICALIZATION,
        ]

        print("\nüìã R√©solution des mod√®les:")

        for task in tasks:
            try:
                model = router._get_model_for_task(task)
                provider = router._get_provider_for_model(model)
                available = router._is_model_available(model)

                status = "‚úÖ" if available else "‚ö†Ô∏è"
                print(f"  {status} {task.value}: {model} ({provider})")

                # V√©rifications
                assert model is not None
                assert provider in ["openai", "anthropic"]

            except Exception as e:
                pytest.fail(f"Erreur pour la t√¢che {task.value}: {e}")

    @pytest.mark.skipif(
        not Path("config/llm_models.yaml").exists(),
        reason="Fichier de configuration non trouv√©"
    )
    def test_config_file_structure(self):
        """Test la structure du fichier de configuration."""
        import yaml

        config_path = Path("config/llm_models.yaml")

        with config_path.open("r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # V√©rifications de structure
        required_sections = ["task_models", "providers", "default_model"]
        for section in required_sections:
            assert section in config, f"Section manquante: {section}"

        # V√©rifications des t√¢ches
        task_models = config["task_models"]
        expected_tasks = [task.value for task in TaskType]

        for task in expected_tasks:
            assert task in task_models, f"T√¢che manquante dans config: {task}"

        print("‚úÖ Structure du fichier de configuration valid√©e")

    def test_fallback_behavior(self, caplog):
        """Test le comportement de fallback."""
        with caplog.at_level(logging.INFO):
            router = LLMRouter()

            # Forcer un mod√®le indisponible pour tester le fallback
            with router._config.setdefault("task_models", {}).update({"vision": "modele_inexistant"}):
                try:
                    model = router._get_model_for_task(TaskType.VISION)
                    # Devrait utiliser un fallback ou le mod√®le par d√©faut
                    assert model is not None
                except Exception:
                    # C'est acceptable si aucun fallback n'est configur√©
                    pass

        # V√©rifier qu'il y a eu des logs de fallback si n√©cessaire
        fallback_logs = [record for record in caplog.records if "fallback" in record.message.lower()]
        if fallback_logs:
            print("üîÑ Comportement de fallback test√© avec succ√®s")


def test_demo_configuration_change():
    """
    Test de d√©monstration du changement de configuration.
    Ce test montre comment la configuration peut √™tre modifi√©e dynamiquement.
    """
    print("\n=== D√©mo de changement de configuration ===")

    # Cr√©er deux instances avec des configs diff√©rentes
    router1 = LLMRouter()
    model1 = router1._get_model_for_task(TaskType.VISION)
    provider1 = router1._get_provider_for_model(model1)

    print(f"üìã Configuration actuelle pour 'vision': {model1} ({provider1})")

    print("\nüí° Pour changer dynamiquement:")
    print("1. Modifiez config/llm_models.yaml")
    print("2. Red√©marrez l'application")
    print("3. Le nouveau mod√®le sera automatiquement utilis√©")

    print("\nüìù Exemple de modification:")
    print("task_models:")
    print("  vision: \"claude-3-5-sonnet-20241022\"  # Au lieu de gpt-4o")
    print("  metadata: \"gpt-4o-mini\"               # Plus rapide/moins cher")


if __name__ == "__main__":
    """Permet d'ex√©cuter les tests d'int√©gration directement."""
    import sys

    # Configuration du logging
    logging.basicConfig(level=logging.DEBUG)

    print("üß™ Tests d'int√©gration du routeur LLM")
    print("=" * 50)

    try:
        # Tests de base
        test_instance = TestLLMRouterIntegration()

        test_instance.test_router_initialization_with_real_config()
        test_instance.test_provider_detection()
        test_instance.test_model_resolution_for_all_tasks()
        test_instance.test_config_file_structure()

        test_demo_configuration_change()

        print("\nüéâ Tous les tests d'int√©gration sont pass√©s!")

    except Exception as e:
        print(f"\n‚ùå Erreur lors des tests: {e}")
        sys.exit(1)