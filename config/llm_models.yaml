# Configuration des modèles LLM par type de tâche
# Modifiez ce fichier pour changer dynamiquement les modèles utilisés

version: "1.0"

# Mapping tâche -> modèle
# Changez ces valeurs pour utiliser d'autres modèles sans modifier le code
task_models:
  # Analyse d'images + texte (multimodal)
  vision: "gpt-4o"

  # Extraction de métadonnées JSON structurées
  metadata: "gpt-4o"

  # Résumés de textes longs (>10k tokens)
  long_summary: "gpt-4o-mini"

  # Enrichissement de contenu court
  enrichment: "claude-3-haiku-20240307"

  # Classification binaire rapide (oui/non, etc.)
  classification: "gpt-4o-mini"

  # Canonicalisation de noms SAP
  canonicalization: "gpt-4o-mini"

  # Analyse intelligente de questions RFP Excel (filtrage + reformulation + fusion)
  rfp_question_analysis: "gpt-4o"

  # Traduction de langue (tâche simple et peu coûteuse)
  translation: "gpt-4o-mini"

# Paramètres par type de tâche (basés sur les valeurs historiques du code)
task_parameters:
  # Analyse d'images (ex: slides PowerPoint)
  vision:
    temperature: 0.2
    max_tokens: 4000

  # Extraction de métadonnées JSON structurées
  metadata:
    temperature: 0.1  # Plus déterministe pour JSON structuré
    max_tokens: 2000  # Assez pour métadonnées complètes

  # Résumés de textes longs
  long_summary:
    temperature: 0.1  # Cohérence dans les résumés
    max_tokens: 1500  # Résumés substantiels (MAX_PARTIAL_TOKENS et MAX_SUMMARY_TOKENS)

  # Enrichissement de contenu court
  enrichment:
    temperature: 0.3  # Plus de créativité pour l'enrichissement
    max_tokens: 1000  # Suffisant pour enrichir

  # Classification binaire rapide (oui/non)
  classification:
    temperature: 0    # Déterministe pour classification
    max_tokens: 5     # Réponses très courtes (oui/non)

  # Canonicalisation de noms SAP
  canonicalization:
    temperature: 0    # Déterministe pour normalisation
    max_tokens: 100   # Noms courts mais peut inclure reformulation

  # Analyse intelligente de questions RFP Excel
  rfp_question_analysis:
    temperature: 0.1  # Consistance dans l'analyse
    max_tokens: 16000  # Traitement batch de nombreuses questions (augmenté pour éviter troncature)

  # Traduction de langue
  translation:
    temperature: 0.1  # Traduction consistante
    max_tokens: 1000  # Suffisant pour traduire une réponse RFP

# Configuration des providers LLM
providers:
  openai:
    api_key_env: "OPENAI_API_KEY"
    base_url: null  # Utilise l'URL par défaut d'OpenAI
    models:
      - "gpt-4o"
      - "gpt-4o-mini"
      - "o1-preview"
      - "o1-mini"

  anthropic:
    api_key_env: "ANTHROPIC_API_KEY"
    base_url: null  # Utilise l'URL par défaut d'Anthropic
    models:
      - "claude-sonnet-4-20250514"
      - "claude-opus-4-20250514"
      - "claude-3-haiku-20240307"

  sagemaker:
    aws_region: "us-east-1"  # Région AWS par défaut
    models:
      - "llama3.1:70b"
      - "qwen2.5:32b"
      - "qwen2.5:7b"
      - "llava:34b"
      - "phi3:3.8b"

# Fallbacks en cas d'indisponibilité
fallback_strategy:
  # Si le modèle configuré n'est pas disponible, essayer ces alternatives
  vision:
    - "gpt-4o"
    - "claude-sonnet-4-20250514"

  metadata:
    - "gpt-4o"
    - "gpt-4o-mini"

  long_summary:
    - "claude-sonnet-4-20250514"
    - "gpt-4o"
    - "gpt-4o-mini"

  enrichment:
    - "claude-3-haiku-20240307"
    - "gpt-4o-mini"
    - "gpt-4o"

  classification:
    - "gpt-4o-mini"
    - "claude-3-haiku-20240307"

  canonicalization:
    - "gpt-4o-mini"
    - "claude-3-haiku-20240307"

  rfp_question_analysis:
    - "claude-sonnet-4-20250514"
    - "gpt-4o"
    - "gpt-4o-mini"

  translation:
    - "gpt-4o-mini"
    - "claude-3-haiku-20240307"

# Configuration SageMaker Endpoints
# À configurer selon vos déploiements AWS
sagemaker_endpoints:
  "llama3.1:70b": "llama-3-1-70b-instruct-endpoint"
  "qwen2.5:32b": "qwen2-5-32b-instruct-endpoint"
  "qwen2.5:7b": "qwen2-5-7b-instruct-endpoint"
  "llava:34b": "llava-34b-vision-endpoint"
  "phi3:3.8b": "phi3-mini-instruct-endpoint"

# Configuration alternative optimisée pour SageMaker
# Décommentez cette section pour utiliser les modèles SageMaker
alternative_task_models:
  # Analyse d'images + texte (multimodal)
  vision: "llava:34b"

  # Extraction de métadonnées JSON structurées
  metadata: "qwen2.5:32b"

  # Résumés de textes longs (>10k tokens)
  long_summary: "llama3.1:70b"

  # Enrichissement de contenu court
  enrichment: "qwen2.5:32b"

  # Classification binaire rapide (oui/non, etc.)
  classification: "phi3:3.8b"

  # Canonicalisation de noms SAP
  canonicalization: "qwen2.5:7b"

  # Analyse intelligente de questions RFP Excel
  rfp_question_analysis: "llama3.1:70b"

  # Traduction de langue (tâche simple et peu coûteuse)
  translation: "qwen2.5:7b"

# Configuration par défaut si aucune tâche ne correspond
default_model: "gpt-4o"
default_fallbacks:
  - "gpt-4o"
  - "gpt-4o-mini"