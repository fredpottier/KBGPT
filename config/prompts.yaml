# =============================================================================
# KnowWhere / OSMOSE - Prompts Configuration
# Version: 2024-12-18-domain-agnostic
# =============================================================================
#
# Ces prompts sont G√âN√âRIQUES par d√©faut.
# Le contexte m√©tier sp√©cifique est inject√© dynamiquement via le Domain Context
# configur√© par le client (voir: src/knowbase/ontology/domain_context_injector.py)
#
# Architecture:
# - Prompts de base: G√©n√©riques, sans r√©f√©rence √† un domaine sp√©cifique
# - Domain Context: Inject√© automatiquement pour personnaliser l'extraction
# - Ontologies: Apprises progressivement √† partir des documents import√©s
#
# =============================================================================

version: "2024-12-18-domain-agnostic"

# =============================================================================
# ASSERTION-CENTRIC SYNTHESIS (OSMOSE Assertion-Based Answers)
# =============================================================================
#
# Le prompt assertion_synthesis g√©n√®re une r√©ponse structur√©e en assertions.
# Chaque assertion est un claim logique v√©rifiable avec son statut de v√©rit√©.
#
# Output attendu: JSON avec assertions array et open_points array
# =============================================================================

assertion_synthesis:
  id: "assertion_synthesis_v1"
  template: |
    You are OSMOSE, a specialized knowledge synthesis assistant.
    Your task is to produce an **assertion-based answer** where each statement is verifiable.

    ## Question
    {{ question }}

    ## Available Evidence
    {% for evidence in evidences %}
    ### Source {{ evidence.source_id }}: {{ evidence.document_title }}
    - Date: {{ evidence.document_date | default('Unknown') }}
    - Authority: {{ evidence.authority | default('internal') }}
    - Page/Slide: {{ evidence.page_or_slide | default('N/A') }}

    Excerpt:
    {{ evidence.excerpt }}

    {% endfor %}

    ## Your Mission

    Generate an **instrumented answer** as a JSON object with ordered assertions.

    **IMPORTANT**: Only include assertions that DIRECTLY answer the question above.
    Evidence about the same product/topic but unrelated to the question should be IGNORED.
    For example, if asked about "monitoring options", don't include info about firewalls, HA, or security unless they relate to monitoring.

    ### Output Format (JSON only)

    ```json
    {
      "assertions": [
        {
          "id": "A1",
          "text_md": "Markdown text with **bold** and *italic* allowed.",
          "kind": "FACT",
          "evidence_used": ["S1", "S2"],
          "derived_from": [],
          "notes": null
        },
        {
          "id": "A2",
          "text_md": "This is an inference based on A1.",
          "kind": "INFERRED",
          "evidence_used": [],
          "derived_from": ["A1"],
          "notes": "Logically follows from A1 because..."
        }
      ],
      "open_points": [
        "Information about X was not found in the sources."
      ]
    }
    ```

    ### Rules

    0. **CRITICAL - Relevance Filter**:
       - ONLY generate assertions that DIRECTLY answer the question
       - IGNORE evidence that is off-topic or tangentially related
       - If a source mentions the same product/topic but doesn't address the question, skip it
       - Better to have 3 relevant assertions than 10 irrelevant ones

    1. **FACT**: A claim explicitly supported by at least one excerpt from the evidence.
       - MUST include `evidence_used` with source IDs (S1, S2, etc.)
       - `derived_from` is empty
       - `notes` is null

    2. **INFERRED**: A logical deduction from FACT assertions (not from raw evidence).
       - MUST include `derived_from` with parent assertion IDs (A1, A2, etc.)
       - `evidence_used` is empty
       - `notes` explains the inference in one sentence

    3. **Assertion count**: Generate between 3 and 10 assertions total.
       - Each assertion = ONE logical claim that answers the question
       - Quality over quantity: fewer relevant assertions > many off-topic ones
       - If evidence doesn't answer the question well, generate only 2-3 assertions + open_points

    4. **Markdown in text_md**:
       - Allowed: **bold**, *italic*, simple lists, links
       - Forbidden: headings (#), code blocks (```), tables

    5. **Open points**: List questions or information gaps that evidence could not answer.
       - If most evidence is off-topic, add: "Limited relevant information found for this specific question."
       - Be honest about what the sources don't cover

    6. **Language**: Respond in the same language as the question ({{ language | default('fr') }}).

    7. **No invention**: Never cite a source ID that does not exist in the evidence.

    ### Critical Constraints

    - Return ONLY the JSON object, no markdown code blocks, no explanation
    - All source IDs must match the format: S1, S2, S3... (as provided in evidence)
    - All assertion IDs must be sequential: A1, A2, A3...
    - INFERRED assertions can only derive from FACT assertions, not from other INFERRED

    Generate the JSON now:

families:
  default:
    deck:
      id: "deck_default_v2"
      template: |
        You are given a global text summary from a PowerPoint slide deck.

        Return a single JSON object with two fields:
        - "summary": a concise thematic summary (3-5 sentences) of the deck's main purpose and intended audience.
        - "metadata": a JSON object with the following fields:
          - title
          - objective
          - main_solution
          - supporting_solutions
          - mentioned_solutions
          - document_type
          - audience
          - source_date
          - language

        IMPORTANT:
        - For 'main_solution', use the official canonical product/solution name.
        - For 'supporting_solutions', list related products/solutions with their official names.
        - Do not use acronyms, abbreviations, or local variants unless no official name exists.
        - If unsure, leave the field empty or null.
        - Return only the JSON object ‚Äî no explanation.

        Global summary text:
        {{ summary_text }}

    slide:
      id: "slide_default_v5_unified_kg_facts"
      template: |
        Global deck summary:
        {{ deck_summary }}

        Section {{ slide_index }} content (extracted via MegaParse):
        {{ megaparse_content | default(text) }}

        Original text (legacy):
        {{ text }}

        Notes:
        {{ notes }}

        Analyze section {{ slide_index }} from '{{ source_name }}' (PowerPoint document).

        **CRITICAL DUAL-SOURCE ANALYSIS**: You have TWO complementary information sources:
        1. **MegaParse content**: Structured text preserving tables, lists, formatting, and textual hierarchy
        2. **Visual image**: The actual slide with diagrams, charts, visual layouts, colors, and spatial relationships

        **YOUR QUAD MISSION** - Extract FOUR types of information from this slide in a single pass:

        ## üéØ TASK 1: CONCEPTS for Semantic Search (Qdrant)

        SYNTHESIZE both sources to extract ALL GENUINE DISTINCT CONCEPTS (if any). Each concept should combine:
        - Textual details from MegaParse (precise data, lists, technical specs)
        - Visual context from the image (diagrams, flows, spatial relationships, visual emphasis)

        **Multi-concept extraction**: Extract ONLY genuine, distinct concepts present in the content. This could be:
        - 0 concepts (if title/transition/agenda slide with no substantial content)
        - 1 concept (if slide focuses on one topic)
        - Multiple concepts (if slide genuinely covers several distinct topics)

        **Quality over quantity**: Better one accurate concept than multiple fabricated ones.

        ## üìä TASK 2: STRUCTURED FACTS for Knowledge Graph (Neo4j)

        Extract ONLY **objective, verifiable facts with precise numeric values or dates**.

        **Fact Types Priority**:
        - SERVICE_LEVEL: SLA, uptime, performance guarantees, response time
        - CAPACITY: Limits, quotas, max sizes (users, storage, transactions/sec, concurrent sessions)
        - PRICING: Prices, costs, rates (‚Ç¨, $, credits, pricing tiers)
        - FEATURE: Enabled/disabled features, available modules
        - COMPLIANCE: Certifications, standards (ISO 27001, SOC2, GDPR, HIPAA)
        - GENERAL: Other measurable facts

        **Strict Rules for Facts**:
        1. ‚ùå Ignore opinions, vague estimates, marketing promises ("up to", "about", "potentially")
        2. ‚úÖ Require PRECISE numeric value for SERVICE_LEVEL, CAPACITY, PRICING
        3. ‚úÖ Indicate confidence 0.9-1.0 if explicit in text/image, 0.6-0.8 if inferred
        4. ‚úÖ Capture valid_from if date/period mentioned (2024, Q1 2024, Jan 2024, since 2023)
        5. ‚úÖ Reference slide_number for traceability
        6. ‚úÖ Indicate fact source: "text" (slide text), "image" (visual diagram/table), "both"
        7. ‚ùå DO NOT extract generic facts without value ("offers high availability" ‚Üí skip)
        8. ‚úÖ Normalize units: % for SLA, GB/TB for storage, users for capacity

        ## üîó TASK 3: ENTITIES for Knowledge Graph (Neo4j)

        Extract **named entities** mentioned in the slide - concrete, identifiable things.

        **Entity Types Priority**:
        - SOLUTION: Products, solutions, platforms (use official canonical names)
        - COMPONENT: Technical components, modules, services
        - ORGANIZATION: Companies, business units mentioned
        - PERSON: People, roles, job titles mentioned
        - TECHNOLOGY: Technologies, frameworks, standards (Docker, Kubernetes, OAuth 2.0)
        - CONCEPT: Key business/technical concepts (Digital Transformation, Cloud Migration)

        **Entity Extraction Rules**:
        1. ‚úÖ Extract ONLY entities explicitly mentioned in text/image
        2. ‚úÖ Use official canonical names when known
        3. ‚úÖ Include brief description (10-50 words) from slide context
        4. ‚úÖ Indicate confidence: 1.0 if explicit name, 0.7-0.9 if inferred from context
        5. ‚ùå DO NOT create entities for generic terms ("the system", "the platform")
        6. ‚úÖ Capture attributes if mentioned (version, vendor, category)

        ## üîó TASK 4: RELATIONS for Knowledge Graph (Neo4j)

        Extract **relationships between entities** - how things connect/interact.

        **Relation Types Priority**:
        - INTEGRATES_WITH: System A integrates/connects with System B
        - PART_OF: Component is part of larger system
        - USES: Entity A uses/depends on Entity B
        - PROVIDES: Entity A provides service/capability to Entity B
        - REPLACES: Entity A replaces/supersedes Entity B
        - REQUIRES: Entity A requires Entity B (dependency)
        - INTERACTS_WITH: Generic interaction between entities

        **Relation Extraction Rules**:
        1. ‚úÖ Extract ONLY relations explicitly stated or clearly shown in diagrams
        2. ‚úÖ Use entity names exactly as extracted in entities array
        3. ‚úÖ Include description explaining the relationship (10-30 words)
        4. ‚úÖ Indicate confidence: 1.0 if explicit arrow/text, 0.7-0.9 if inferred from context
        5. ‚ùå DO NOT invent relations not shown in slide
        6. ‚úÖ Prefer specific relation types over generic INTERACTS_WITH

        **UNIFIED JSON OUTPUT FORMAT** - Return ALL FOUR outputs in a single JSON object:

        ```json
        {
          "concepts": [
            {
              "full_explanation": "Complete explanation synthesizing MegaParse text + visual analysis.",
              "meta": {
                "scope": "solution-specific" | "general" | "industry-specific",
                "type": "capability" | "architecture" | "process" | "feature" | "benefit" | "use-case" | "integration",
                "level": "strategic" | "tactical" | "operational",
                "tags": ["tag1", "tag2"],
                "slide_role": "title" | "content" | "summary" | "transition" | "agenda",
                "concept_confidence": 0.1-1.0,
                "has_visuals": true | false,
                "visual_type": "table" | "diagram" | "chart" | "screenshot" | "flow" | null
              }
            }
          ],
          "facts": [
            {
              "subject": "Product Name",
              "predicate": "SLA_uptime",
              "object": "99.7%",
              "value": 99.7,
              "unit": "%",
              "value_type": "numeric",
              "fact_type": "SERVICE_LEVEL",
              "confidence": 0.95,
              "valid_from": "2024-01-01",
              "source_slide_number": {{ slide_index }},
              "extraction_context": "SLA table column",
              "source_type": "both"
            }
          ],
          "entities": [
            {
              "name": "Product Name",
              "entity_type": "SOLUTION",
              "description": "Brief description of the product/entity",
              "confidence": 1.0,
              "attributes": {
                "vendor": "Vendor Name",
                "category": "Category"
              }
            }
          ],
          "relations": [
            {
              "source": "Product A",
              "target": "Product B",
              "relation_type": "INTEGRATES_WITH",
              "description": "Product A integrates with Product B for data exchange",
              "confidence": 0.95
            }
          ]
        }
        ```

        **Concept Quality Requirements**:
        - Minimum 50 words per concept explanation (or return empty concepts array [] if no substantial content)
        - Maximum 500 words per concept explanation
        - Focus on actionable, searchable knowledge
        - Preserve technical details and visual descriptions
        - **NEVER make up information or force concepts where none exist**

        **IMPORTANT RULES**:
        - Return `{"concepts": [], "facts": [], "entities": [], "relations": []}` for empty/title/transition slides
        - Always extract all 4 types when relevant data is present
        - Entities and relations work together: relations MUST reference entity names from entities array

        Return **only the JSON object**, no extra text, no markdown code blocks.

  technical:
    deck:
      id: "deck_technical_v2"
      template: |
        You are given raw text extracted from a technical/architecture PowerPoint deck.

        Return ONE JSON object with exactly these keys:
        {
          "summary": "<3‚Äì6 sentences describing scope, main technical themes, intended audience>",
          "metadata": {
            "title": null | "<string>",
            "objective": null | "<string>",
            "main_solution": null | "<official product name>",
            "supporting_solutions": ["<canonical name>", ...],
            "mentioned_solutions": ["<brand or product name>", ...],
            "document_type": "technical",
            "audience": null | "<string>",
            "source_date": null | "<string as shown>",
            "language": null | "fr" | "en",
            "product_version": null | "<string>",
            "document_subtype": null | "reference_architecture" | "deployment_guide" | "design_doc" | "operations_runbook" | "integration_guide" | "overview",
            "architecture_focus": ["<short bullet>", ...],
            "architecture_domains": ["compute" | "network" | "storage" | "data" | "integration" | "security" | "identity" | "observability", ...],
            "key_patterns": ["client-server" | "event-driven" | "microservices" | "layered" | "<as written>", ...],
            "tech_stack": ["<runtime/db/middleware/tool name>", ...],
            "quality_attributes": ["performance" | "reliability" | "security" | "scalability", ...]
          }
        }

        Extraction rules:
        - **Explicit-only**: extract only what appears in THIS deck's text; if absent ‚Üí use null or [].
        - **Naming**: Use official canonical product names when known.
        - **Limits**: keep lists concise (‚â§ 12 items), deduplicate, no invented content.
        - **Output**: return ONLY the JSON object; no comments, no trailing text.

        Raw deck text:
        {{ summary_text }}

    slide:
      id: "slide_technical_v4_unified_kg_facts"
      template: |
        Global deck summary:
        {{ deck_summary }}

        Section {{ slide_index }} content (extracted via MegaParse):
        {{ megaparse_content | default(text) }}

        Original text (legacy):
        {{ text }}

        Notes:
        {{ notes }}

        Analyze technical section {{ slide_index }} from '{{ source_name }}' (PowerPoint document).

        **TECHNICAL ANALYSIS**: Extract technical information from this slide.

        **YOUR QUAD MISSION** - Extract FOUR types of technical information:

        ## üéØ TASK 1: TECHNICAL CONCEPTS for Semantic Search
        Extract technical concepts combining text specifications and visual architecture diagrams.

        ## üìä TASK 2: STRUCTURED FACTS for Knowledge Graph
        Extract precise technical facts (SLA, capacity, performance metrics).

        ## üîó TASK 3: TECHNICAL ENTITIES for Knowledge Graph
        Extract named technical entities (solutions, components, technologies, infrastructure).

        ## üîó TASK 4: TECHNICAL RELATIONS for Knowledge Graph
        Extract technical relationships (INTEGRATES_WITH, PART_OF, DEPLOYS_TO, COMMUNICATES_WITH).

        **JSON OUTPUT FORMAT**:
        ```json
        {
          "concepts": [{"full_explanation": "...", "meta": {...}}],
          "facts": [{"subject": "...", "predicate": "...", "value": ..., ...}],
          "entities": [{"name": "...", "entity_type": "...", ...}],
          "relations": [{"source": "...", "target": "...", "relation_type": "...", ...}]
        }
        ```

        Return **only the JSON object**, no extra text.

  functional:
    deck:
      id: "deck_functional_v2"
      template: |
        You are given raw text extracted from a business/functional PowerPoint deck.

        Return a single JSON object with two fields:
        - "summary": 3‚Äì6 sentences describing the deck's purpose, target audience, and what a reader will learn.
        - "metadata": object with:
          - title
          - objective
          - main_solution
          - supporting_solutions
          - mentioned_solutions
          - document_type
          - audience
          - source_date
          - language
          - product_version
          - lob_coverage (Lines of Business)
          - industry_coverage
          - capabilities_summary
          - innovations_summary

        IMPORTANT:
        - Use official canonical product names when known.
        - If a value is not clearly present, set it to null or [].
        - Return only the JSON object, no extra text.

        Raw deck text:
        {{ summary_text }}

    slide:
      id: "slide_functional_v4_unified_kg_facts"
      template: |
        Global deck summary:
        {{ deck_summary }}

        Section {{ slide_index }} content (extracted via MegaParse):
        {{ megaparse_content | default(text) }}

        Original text (legacy):
        {{ text }}

        Notes:
        {{ notes }}

        Analyze functional section {{ slide_index }} from '{{ source_name }}' (PowerPoint document).

        **BUSINESS ANALYSIS**: Extract business/functional information from this slide.

        **YOUR QUAD MISSION** - Extract FOUR types of business information:

        ## üéØ TASK 1: BUSINESS CONCEPTS for Semantic Search
        Extract business concepts (capabilities, processes, features, benefits).

        ## üìä TASK 2: STRUCTURED FACTS for Knowledge Graph
        Extract measurable business facts (KPIs, cycle times, cost savings).

        ## üîó TASK 3: BUSINESS ENTITIES for Knowledge Graph
        Extract named business entities (solutions, business processes, roles, organizations).

        ## üîó TASK 4: BUSINESS RELATIONS for Knowledge Graph
        Extract business relationships (PROVIDES, SUPPORTS, ENABLES, USES).

        **JSON OUTPUT FORMAT**:
        ```json
        {
          "concepts": [{"full_explanation": "...", "meta": {...}}],
          "facts": [{"subject": "...", "predicate": "...", "value": ..., ...}],
          "entities": [{"name": "...", "entity_type": "...", ...}],
          "relations": [{"source": "...", "target": "...", "relation_type": "...", ...}]
        }
        ```

        Return **only the JSON object**, no extra text.

# =============================================================================
# MVP V1 - PASS 1.3 INFORMATION-FIRST EXTRACTION
# =============================================================================
#
# Prompt pour extraction d'assertions avec exact_quote obligatoire.
# Utilis√© par: src/knowbase/stratified/pass1/assertion_extractor.py
# Reference: SPEC_TECHNIQUE_MVP_V1_USAGE_B.md
# =============================================================================

pass1_3_mvp_v1:
  id: "pass1_3_mvp_v1"
  system: |
    Tu es un extracteur de faits documentaires. Tu extrais TOUTES les assertions
    factuelles explicites du texte, sans interpr√©tation ni inf√©rence.

    R√àGLES ABSOLUES:
    1. Citation exacte OBLIGATOIRE (exact_quote = verbatim du texte)
    2. Position OBLIGATOIRE (page, paragraphe si visible)
    3. Ne JAMAIS rejeter une assertion pour "pas de concept"
    4. Ne JAMAIS inf√©rer ce qui n'est pas explicitement √©crit

    TYPES:
    - PRESCRIPTIVE: Obligations ("must", "shall", "required")
    - DEFINITIONAL: D√©finitions ("is", "uses", "provides")
    - CAUSAL: Cause-effet ("because", "therefore")
    - COMPARATIVE: Comparaisons ("more than", "unlike")

    RHETORICAL ROLES:
    - fact: Assertion factuelle
    - example: Illustration (PAS de ClaimKey)
    - definition: D√©finition de terme
    - instruction: Proc√©dure
    - claim: Affirmation non v√©rifi√©e
    - caution: Avertissement

  user: |
    Document: {{ document_title }}
    Page: {{ page_number }}
    Context: {{ doc_context_frame }}

    Texte:
    """
    {{ chunk_text }}
    """

    Extrais TOUTES les assertions factuelles. JSON:
    {
      "assertions": [
        {
          "text": "assertion claire",
          "exact_quote": "verbatim exact",
          "type": "PRESCRIPTIVE|DEFINITIONAL|CAUSAL|COMPARATIVE",
          "rhetorical_role": "fact|example|definition|instruction|claim|caution",
          "span": {"page": int, "paragraph": int},
          "context_override": {"edition": "", "region": ""},
          "confidence": 0.0-1.0
        }
      ]
    }

# =============================================================================
# VERIFICATION PROMPTS - Text Verification Against Knowledge Graph
# =============================================================================
#
# Prompts pour la v√©rification de texte contre la base de connaissances.
# Utilis√©s par le module src/knowbase/verification/
#
# =============================================================================

verification_split_assertions:
  id: "verification_split_assertions_v1"
  description: "D√©coupe un texte en assertions v√©rifiables avec leurs positions"
  template: |
    Tu es un analyseur de texte. Ta t√¢che est d'identifier chaque AFFIRMATION
    ou FAIT v√©rifiable dans le texte donn√©.

    Une assertion est une phrase ou partie de phrase qui:
    - Affirme quelque chose de factuel
    - Peut √™tre vraie ou fausse
    - Est v√©rifiable contre une source documentaire

    NE PAS inclure:
    - Les questions
    - Les opinions subjectives
    - Les connecteurs logiques seuls

    Retourne un JSON array avec pour chaque assertion:
    - text: le texte exact de l'assertion
    - start: position de d√©but dans le texte original
    - end: position de fin dans le texte original

verification_compare_claim:
  id: "verification_compare_claim_v1"
  description: "Compare une assertion utilisateur avec des claims document√©s"
  template: |
    Tu compares une ASSERTION utilisateur avec un CLAIM document√©.

    D√©termine la relation:
    - SUPPORTS: le claim confirme l'assertion
    - CONTRADICTS: le claim contredit l'assertion
    - PARTIAL: le claim parle du m√™me sujet mais l'info est incompl√®te

    Retourne un JSON:
    {
      "relationship": "SUPPORTS|CONTRADICTS|PARTIAL",
      "confidence": 0.0-1.0,
      "explanation": "..."
    }

verification_correct_text:
  id: "verification_correct_text_v1"
  description: "Corrige un texte bas√© sur les claims de la base de connaissances"
  template: |
    Tu es un correcteur de texte bas√© sur des sources documentaires.

    On te donne:
    1. Un texte original
    2. Une liste d'assertions probl√©matiques avec les claims corrects

    R√©√©cris le texte en:
    - Corrigeant les affirmations contredites
    - Compl√©tant les informations incompl√®tes
    - Gardant le style et le ton du texte original
    - Pr√©servant les parties correctes

    Retourne:
    {
      "corrected_text": "...",
      "changes": [
        {"original": "...", "corrected": "...", "reason": "..."}
      ]
    }
