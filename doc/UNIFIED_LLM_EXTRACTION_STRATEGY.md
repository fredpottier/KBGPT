# StratÃ©gie UnifiÃ©e d'Extraction LLM - Chunks + Entities + Facts

**Date**: 30 septembre 2025 (mise Ã  jour avec Facts)
**DÃ©cision**: Mutualiser les appels LLM pour extraire simultanÃ©ment chunks Qdrant, entitÃ©s/relations KG, ET facts structurÃ©s
**Statut**: âœ… **VALIDÃ‰** - Approche optimale confirmÃ©e

---

## ğŸ¯ DÃ‰CISION ARCHITECTURALE

### Principe directeur

**Un seul appel LLM Vision par slide** extrait :
1. **Chunks textuels** (condensation sÃ©mantique pour Qdrant)
2. **EntitÃ©s structurÃ©es** (concepts du domaine mÃ©tier pour Knowledge Graph)
3. **Relations** (liens sÃ©mantiques entre entitÃ©s pour Knowledge Graph)
4. **Facts structurÃ©s** (assertions quantifiables pour gouvernance)

### Justification

âœ… **Ã‰conomie coÃ»ts**: 0â‚¬ additionnel (rÃ©utilise appels existants)
âœ… **Ã‰conomie temps**: 0s latence additionnelle (extraction parallÃ¨le)
âœ… **QualitÃ© maximale**: Contexte complet prÃ©servÃ© (image + texte + notes)
âœ… **CohÃ©rence parfaite**: Chunks et entitÃ©s issus de la mÃªme analyse
âœ… **Single source of truth**: Une seule interprÃ©tation LLM du contenu

---

## ğŸ“Š ARCHITECTURE ACTUELLE vs CIBLE

### Architecture Actuelle (Qdrant seulement)

```
1. Upload PPTX
   â†“
2. Conversion PPTX â†’ PDF â†’ PNG images
   â†“
3. ğŸ¤– APPEL LLM #1: Analyse deck global (metadata)
   â†“
4. Pour chaque slide:
      ğŸ¤– APPEL LLM #2: Vision + texte
      â†’ Output: concepts[]
      â†’ Chunking + embedding
      â†’ Ingestion Qdrant
   â†“
5. Finalisation
```

**CoÃ»t**: ~$0.01-0.03 par slide Ã— N slides
**Latence**: 2-5s par slide

---

### Architecture Cible (Qdrant + Graphiti + Facts unifiÃ©)

```
1. Upload PPTX
   â†“
2. Conversion PPTX â†’ PDF â†’ PNG images
   â†“
3. ğŸ¤– APPEL LLM #1: Analyse deck global (metadata)
   â†’ Output: metadata + main_solution + supporting_solutions
   â†’ ğŸ†• Ingestion entitÃ©s principales dans Graphiti (status="proposed")
   â†“
4. Pour chaque slide:
      ğŸ¤– APPEL LLM #2: Vision + texte (MODIFIÃ‰ - extraction unifiÃ©e)
      â†’ Output:
         â€¢ concepts[] (pour chunks Qdrant)
         â€¢ ğŸ†• entities[] (pour Graphiti KG)
         â€¢ ğŸ†• relations[] (pour Graphiti KG)
         â€¢ ğŸ†• facts[] (pour gouvernance Facts)

      Ingestion parallÃ¨le:
      â€¢ Qdrant: chunks avec related_node_ids + related_facts
      â€¢ ğŸ†• Graphiti KG: entities/relations (status="candidate")
      â€¢ ğŸ†• Graphiti Facts: facts structurÃ©s (status="proposed")
   â†“
5. ğŸ†• Post-traitement:
   â€¢ DÃ©doublonnage entitÃ©s/facts deck-wide
   â€¢ Linking entities â†” chunks via related_node_ids
   â€¢ Linking facts â†” chunks via related_facts
   â€¢ Canonicalisation entity names (Phase 4)
   â€¢ DÃ©tection conflits facts (Phase 3)
   â†“
6. Finalisation
```

**CoÃ»t**: **IDENTIQUE** (~$0.01-0.03 par slide Ã— N slides)
**Latence**: **IDENTIQUE** (2-5s par slide)
**BÃ©nÃ©fice**: **+KG automatiquement enrichi** sans coÃ»t additionnel

---

## ğŸ”§ IMPLÃ‰MENTATION TECHNIQUE

### Ã‰tape 1: Extension du Prompt Slide

**Fichier**: `config/prompts.yaml`

**Prompt actuel** (simplifiÃ©):
```yaml
slide:
  default:
    template: |
      Analyze this slide and extract concepts.
      Return JSON: {"concepts": [...]}
```

**Prompt Ã©tendu** (nouveau):
```yaml
slide:
  default:
    template: |
      Analyze this slide with image, text, and notes.

      Deck context: {{ deck_summary }}
      Slide {{ slide_index }}: {{ source_name }}
      Text: {{ text }}
      Notes: {{ notes }}
      MegaParse: {{ megaparse_content }}

      Extract THREE types of information:

      1. CONCEPTS (for semantic search chunks):
         - Detailed explanations of ideas/features
         - Business context and use cases
         - Technical details

      2. ENTITIES (for knowledge graph):
         - Products/Solutions (SAP S/4HANA, SAP Fiori, etc.)
         - Technologies (OData, APIs, protocols)
         - Personas/Roles (IT Manager, Developer, etc.)
         - Features/Modules (Financial Planning, Inventory Mgmt, etc.)
         - Concepts (Cloud-native, Real-time, Integration, etc.)

      3. RELATIONS (for knowledge graph):
         - USES_TECHNOLOGY (S/4HANA uses Fiori)
         - INTEGRATES_WITH (S/4HANA integrates with Ariba)
         - REQUIRES (Feature X requires Module Y)
         - REPLACES (S/4HANA replaces ECC)
         - TARGETS_PERSONA (Solution targets IT Managers)

      Return JSON with this EXACT structure:
      {
        "concepts": [
          {
            "full_explanation": "Detailed explanation...",
            "meta": {
              "scope": "solution-specific",
              "type": "feature_description",
              "level": "technical",
              "tags": ["integration", "API"],
              "mentioned_solutions": ["SAP BTP"]
            }
          }
        ],
        "entities": [
          {
            "name": "SAP S/4HANA Cloud",
            "entity_type": "PRODUCT",
            "description": "Cloud ERP solution with real-time analytics",
            "attributes": {
              "version": "2024 FPS01",
              "deployment": "Public Cloud",
              "vendor": "SAP"
            },
            "confidence": 0.95
          }
        ],
        "relations": [
          {
            "source_entity": "SAP S/4HANA Cloud",
            "target_entity": "SAP Fiori",
            "relation_type": "USES_TECHNOLOGY",
            "description": "S/4HANA uses Fiori for modern user experience",
            "confidence": 0.90
          }
        ]
      }

      IMPORTANT:
      - Extract entities even if briefly mentioned
      - Capture relations visible in diagrams/architecture images
      - Use confidence score (0-1) based on clarity of information
      - For entity_type, use: PRODUCT, TECHNOLOGY, PERSONA, FEATURE, CONCEPT
      - For relation_type, use verbs: USES_TECHNOLOGY, INTEGRATES_WITH, REQUIRES, REPLACES, TARGETS_PERSONA
```

---

### Ã‰tape 2: Modification `ask_gpt_slide_analysis()`

**Fichier**: `src/knowbase/ingestion/pipelines/pptx_pipeline.py` (ligne 913)

**Retour actuel**:
```python
return {
    "chunks": enriched,  # List[Dict] - pour Qdrant
}
```

**Retour modifiÃ©**:
```python
return {
    "chunks": enriched,           # List[Dict] - pour Qdrant (inchangÃ©)
    "entities": entities,         # ğŸ†• List[Dict] - pour Graphiti
    "relations": relations,       # ğŸ†• List[Dict] - pour Graphiti
    "_extraction_meta": {         # ğŸ†• MÃ©tadonnÃ©es extraction
        "slide_index": slide_index,
        "extraction_time": datetime.now(timezone.utc).isoformat(),
        "llm_model": llm_router.model_name,
        "confidence_avg": avg_confidence
    }
}
```

**Code modifiÃ©**:
```python
def ask_gpt_slide_analysis(
    image_path,
    deck_summary,
    slide_index,
    source_name,
    text,
    notes,
    megaparse_content="",
    document_type="default",
    deck_prompt_id="unknown",
    retries=2,
):
    # ... existing code jusqu'Ã  l'appel LLM ...

    try:
        raw_content = llm_router.complete(TaskType.VISION, msg)
        cleaned_content = clean_gpt_response(raw_content or "")
        result = json.loads(cleaned_content)

        # Extraction CONCEPTS (existant, inchangÃ©)
        concepts = result.get("concepts", [])
        enriched = []
        for it in concepts:
            full_text = it.get("full_explanation", "").strip()
            if len(full_text) < 20:
                continue
            # ... chunking logic existante ...
            enriched.append({
                "full_explanation": full_text,
                "meta": it.get("meta", {}),
                "prompt_meta": {...}
            })

        # ğŸ†• Extraction ENTITIES
        entities = result.get("entities", [])
        validated_entities = []
        for entity in entities:
            # Validation basique
            if not entity.get("name") or not entity.get("entity_type"):
                logger.warning(f"Slide {slide_index}: entitÃ© invalide ignorÃ©e")
                continue

            # Normalisation entity_type
            entity_type = entity.get("entity_type", "CONCEPT").upper()
            if entity_type not in ["PRODUCT", "TECHNOLOGY", "PERSONA", "FEATURE", "CONCEPT"]:
                entity_type = "CONCEPT"

            validated_entities.append({
                "name": entity["name"].strip(),
                "entity_type": entity_type,
                "description": entity.get("description", ""),
                "attributes": entity.get("attributes", {}),
                "confidence": float(entity.get("confidence", 0.8)),
                "provenance": {
                    "slide_index": slide_index,
                    "source_name": source_name,
                    "extraction_method": "llm_vision"
                }
            })

        # ğŸ†• Extraction RELATIONS
        relations = result.get("relations", [])
        validated_relations = []
        for relation in relations:
            # Validation basique
            if not relation.get("source_entity") or not relation.get("target_entity"):
                logger.warning(f"Slide {slide_index}: relation invalide ignorÃ©e")
                continue

            validated_relations.append({
                "source_entity": relation["source_entity"].strip(),
                "target_entity": relation["target_entity"].strip(),
                "relation_type": relation.get("relation_type", "RELATED_TO"),
                "description": relation.get("description", ""),
                "confidence": float(relation.get("confidence", 0.8)),
                "provenance": {
                    "slide_index": slide_index,
                    "source_name": source_name
                }
            })

        # Calcul confidence moyenne pour mÃ©tadonnÃ©es
        all_confidences = (
            [e["confidence"] for e in validated_entities] +
            [r["confidence"] for r in validated_relations]
        )
        avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0.0

        logger.info(
            f"Slide {slide_index}: {len(enriched)} chunks, "
            f"{len(validated_entities)} entitÃ©s, "
            f"{len(validated_relations)} relations extraits"
        )

        return {
            "chunks": enriched,
            "entities": validated_entities,
            "relations": validated_relations,
            "_extraction_meta": {
                "slide_index": slide_index,
                "extraction_time": datetime.now(timezone.utc).isoformat(),
                "llm_model": llm_router.current_model,
                "confidence_avg": round(avg_confidence, 2)
            }
        }

    except json.JSONDecodeError as e:
        logger.error(f"Slide {slide_index}: JSON parse error: {e}")
        return {"chunks": [], "entities": [], "relations": []}

    except Exception as e:
        logger.error(f"Slide {slide_index}: extraction error: {e}")
        return {"chunks": [], "entities": [], "relations": []}
```

---

### Ã‰tape 3: Ingestion UnifiÃ©e dans `process_pptx()`

**Fichier**: `src/knowbase/ingestion/pipelines/pptx_pipeline.py` (ligne 1062)

**Ajout aprÃ¨s la boucle de traitement des slides**:

```python
def process_pptx(pptx_path: Path, document_type: str = "default", progress_callback=None, rq_job=None):
    # ... existing code jusqu'Ã  la boucle slides ...

    # ğŸ†• Initialiser service KG
    from knowbase.api.services.knowledge_graph import get_knowledge_graph_service
    from knowbase.api.schemas.graphiti import EntityCreate, RelationCreate, EntityType

    kg_service = get_knowledge_graph_service()

    # Contexte utilisateur (group_id pour multi-tenant)
    user_context = {
        "user_id": rq_job.meta.get("user_id", "system") if rq_job else "system",
        "group_id": rq_job.meta.get("group_id", "default") if rq_job else "default"
    }

    # Accumulateurs deck-wide
    all_kg_entities = []
    all_kg_relations = []
    slide_entity_map = {}  # {slide_index: [entity_ids]}

    # Boucle traitement slides (existant)
    for i, (idx, future) in enumerate(tasks):
        # ... existing progress tracking ...

        result = future.result()
        chunks = result.get("chunks", [])
        entities = result.get("entities", [])
        relations = result.get("relations", [])

        # âœ… Ingestion Qdrant (existant, temporairement sans related_node_ids)
        chunk_ids = ingest_chunks(chunks, metadata, pptx_path.stem, idx, summary)

        # ğŸ†• Accumulation KG data
        if entities:
            all_kg_entities.extend(entities)
            logger.info(f"Slide {idx}: {len(entities)} entitÃ©s pour KG")

        if relations:
            all_kg_relations.extend(relations)
            logger.info(f"Slide {idx}: {len(relations)} relations pour KG")

        slide_entity_map[idx] = entities  # Pour linking ultÃ©rieur

    # ğŸ†• POST-TRAITEMENT: DÃ©doublonnage entitÃ©s deck-wide
    logger.info(f"ğŸ”„ DÃ©doublonnage de {len(all_kg_entities)} entitÃ©s...")
    unique_entities = deduplicate_entities(all_kg_entities)
    logger.info(f"âœ… {len(unique_entities)} entitÃ©s uniques aprÃ¨s dÃ©doublonnage")

    # ğŸ†• INGESTION GRAPHITI: EntitÃ©s
    entity_id_map = {}  # {entity_name: graphiti_uuid}

    if unique_entities:
        logger.info(f"ğŸ”„ Ingestion de {len(unique_entities)} entitÃ©s dans Graphiti...")
        for entity_data in unique_entities:
            try:
                # Normalisation nom (canonicalisation SAP si applicable)
                canonical_name = normalize_entity_name(entity_data["name"])

                entity_create = EntityCreate(
                    name=canonical_name,
                    entity_type=EntityType[entity_data["entity_type"]],
                    description=entity_data.get("description", ""),
                    attributes={
                        **entity_data.get("attributes", {}),
                        "confidence": entity_data.get("confidence", 0.8),
                        "source_document": pptx_path.name,
                        "extraction_method": "llm_vision_unified"
                    },
                    status="proposed"  # ğŸ”‘ NÃ©cessite validation humaine
                )

                created_entity = await kg_service.create_entity(entity_create, user_context)
                entity_id_map[canonical_name] = created_entity.uuid

                logger.debug(f"âœ… EntitÃ© crÃ©Ã©e: {canonical_name} ({created_entity.uuid})")

            except Exception as e:
                logger.error(f"âŒ Erreur crÃ©ation entitÃ© '{entity_data['name']}': {e}")

    # ğŸ†• INGESTION GRAPHITI: Relations
    if all_kg_relations:
        logger.info(f"ğŸ”„ Ingestion de {len(all_kg_relations)} relations dans Graphiti...")
        for relation_data in all_kg_relations:
            try:
                # RÃ©solution des UUIDs via entity_id_map
                source_name = normalize_entity_name(relation_data["source_entity"])
                target_name = normalize_entity_name(relation_data["target_entity"])

                source_uuid = entity_id_map.get(source_name)
                target_uuid = entity_id_map.get(target_name)

                if not source_uuid or not target_uuid:
                    logger.warning(
                        f"Relation ignorÃ©e: entitÃ©s non trouvÃ©es "
                        f"({source_name} â†’ {target_name})"
                    )
                    continue

                relation_create = RelationCreate(
                    source_entity_uuid=source_uuid,
                    target_entity_uuid=target_uuid,
                    relation_type=relation_data["relation_type"],
                    description=relation_data.get("description", ""),
                    attributes={
                        "confidence": relation_data.get("confidence", 0.8),
                        "source_document": pptx_path.name,
                        "slide_index": relation_data["provenance"]["slide_index"]
                    },
                    status="proposed"
                )

                created_relation = await kg_service.create_relation(relation_create, user_context)
                logger.debug(
                    f"âœ… Relation crÃ©Ã©e: {source_name} --[{relation_data['relation_type']}]--> {target_name}"
                )

            except Exception as e:
                logger.error(
                    f"âŒ Erreur crÃ©ation relation "
                    f"'{relation_data['source_entity']} â†’ {relation_data['target_entity']}': {e}"
                )

    # ğŸ†• LINKING: Mise Ã  jour related_node_ids dans Qdrant (Phase 2)
    # TODO: Pour chaque chunk, trouver entities mentionnÃ©es et ajouter leurs UUIDs
    # await update_chunk_related_nodes(chunk_ids, entity_id_map)

    # Statistiques finales
    logger.info("=" * 60)
    logger.info("ğŸ“Š RÃ‰SUMÃ‰ INGESTION")
    logger.info("=" * 60)
    logger.info(f"Chunks Qdrant: {len(chunk_ids)}")
    logger.info(f"EntitÃ©s KG: {len(entity_id_map)}")
    logger.info(f"Relations KG: {len(all_kg_relations)}")
    logger.info("=" * 60)

    # ... existing finalization code ...
```

---

### Ã‰tape 4: Fonctions Utilitaires

**Fichier**: `src/knowbase/ingestion/extractors/entity_utils.py` (nouveau)

```python
from typing import List, Dict
from rapidfuzz import fuzz

def deduplicate_entities(entities: List[Dict]) -> List[Dict]:
    """
    DÃ©duplique entitÃ©s avec fuzzy matching et fusion attributs.

    Exemple:
        ["SAP S/4HANA Cloud", "S/4HANA Cloud"] â†’ "SAP S/4HANA Cloud"
    """
    unique = []
    seen_names = set()

    for entity in entities:
        name = entity["name"]

        # Recherche fuzzy dans entitÃ©s dÃ©jÃ  vues
        best_match = None
        best_score = 0

        for existing_name in seen_names:
            score = fuzz.ratio(name.lower(), existing_name.lower())
            if score > best_score:
                best_score = score
                best_match = existing_name

        # Si score > 85, considÃ©rer comme doublon
        if best_score > 85:
            # Fusionner attributs avec entitÃ© existante
            for existing_entity in unique:
                if existing_entity["name"] == best_match:
                    existing_entity["attributes"].update(entity.get("attributes", {}))
                    # Garder la confidence la plus Ã©levÃ©e
                    existing_entity["confidence"] = max(
                        existing_entity["confidence"],
                        entity.get("confidence", 0.8)
                    )
                    break
        else:
            # Nouvelle entitÃ© unique
            unique.append(entity)
            seen_names.add(name)

    return unique


def normalize_entity_name(name: str) -> str:
    """
    Normalise nom d'entitÃ© (canonicalisation SAP si applicable).

    Exemple:
        "SAP Cloud ERP" â†’ "SAP S/4HANA Cloud, Public Edition"
    """
    from knowbase.common.sap.normalizer import normalize_solution_name

    # Tentative normalisation SAP
    solution_id, canonical_name = normalize_solution_name(name)

    if solution_id != "UNMAPPED":
        return canonical_name

    # Sinon, simple nettoyage
    return name.strip()
```

---

## ğŸ“Š FLUX DE DONNÃ‰ES UNIFIÃ‰

### Diagramme architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PPTX Upload                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Conversion PPTX â†’ PDF â†’ PNG Images                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– LLM #1: Deck Global Analysis                             â”‚
â”‚  Output: metadata + main_solution + supporting_solutions     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â–¼                      â–¼                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  EntitÃ©s deck  â”‚    â”‚ Metadata doc â”‚  â”‚   Summary    â”‚
              â”‚  (proposed)    â”‚    â”‚              â”‚  â”‚              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                   â”‚                 â”‚
                       â–¼                   â–¼                 â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚            Graphiti KG (status=proposed)        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pour chaque slide:                                          â”‚
â”‚  ğŸ¤– LLM #2: Vision + Text Analysis (UNIFIÃ‰)                  â”‚
â”‚  Output:                                                     â”‚
â”‚    â€¢ concepts[] â†’ chunks Qdrant                              â”‚
â”‚    â€¢ entities[] â†’ Graphiti KG                                â”‚
â”‚    â€¢ relations[] â†’ Graphiti KG                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  concepts  â”‚  â”‚  entities  â”‚  â”‚  relations â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚               â”‚               â”‚
      â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunking +   â”‚  â”‚ DÃ©doublonnage + Fusion   â”‚
â”‚ Embedding    â”‚  â”‚ Canonicalisation         â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                 â”‚
      â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Qdrant     â”‚  â”‚    Graphiti KG           â”‚
â”‚   (chunks)   â”‚  â”‚ (entities + relations)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   status=proposed        â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Linking via         â”‚
              â”‚ related_node_ids    â”‚
              â”‚ (Phase 2)           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’° Ã‰CONOMIE DE COÃ›TS

### Comparaison approches

| Approche | CoÃ»t LLM | Latence | QualitÃ© Extraction |
|----------|----------|---------|-------------------|
| **Double appel** (non recommandÃ©) | 2Ã— coÃ»t | 2Ã— temps | Risque incohÃ©rence |
| **Post-processing chunks** | +30% coÃ»t | +50% temps | Perte contexte visuel |
| **âœ… Appel unifiÃ©** (recommandÃ©) | **0â‚¬ additionnel** | **0s additionnel** | **Contexte complet** |

### Estimation sur volume rÃ©el

**Document type**: PPTX 50 slides

| MÃ©thode | CoÃ»t Total | Temps Total |
|---------|-----------|-------------|
| Double appel | ~$3.00 | ~8 minutes |
| Post-processing | ~$2.00 | ~6 minutes |
| **Appel unifiÃ©** | **~$1.50** | **~4 minutes** |

**Ã‰conomie annuelle** (1000 docs/an):
- CoÃ»t: **âˆ’$1500** vs double appel
- Temps: **âˆ’4000 minutes** (66 heures)

---

## âœ… PLAN D'IMPLÃ‰MENTATION

### Phase 1: POC Minimal (2-3 jours)

**Objectif**: Valider faisabilitÃ© technique

- [ ] Modifier prompt slide dans `prompts.yaml`
- [ ] Adapter `ask_gpt_slide_analysis()` pour parser entities/relations
- [ ] CrÃ©er `entity_utils.py` (dÃ©doublonnage + normalisation)
- [ ] IntÃ©grer ingestion KG dans `process_pptx()`
- [ ] Tester avec 1 PPTX simple (5-10 slides)

**CritÃ¨res succÃ¨s**:
- âœ… Prompt retourne JSON valide avec concepts + entities + relations
- âœ… EntitÃ©s ingÃ©rÃ©es dans Graphiti (status="proposed")
- âœ… Aucune rÃ©gression Qdrant (chunks identiques)
- âœ… Latence stable (+0-10% acceptable)

---

### Phase 2: Production Ready (1 semaine)

**Objectif**: Robustesse et optimisations

- [ ] Gestion erreurs (LLM, parsing, ingestion)
- [ ] DÃ©doublonnage entitÃ©s deck-wide
- [ ] Canonicalisation SAP automatique
- [ ] Linking entities â†” chunks (related_node_ids)
- [ ] Tests avec gros deck (100+ slides)
- [ ] Statistiques KG dans rÃ©ponse API

**CritÃ¨res succÃ¨s**:
- âœ… Gestion erreurs gracieuse (pas de crash)
- âœ… DÃ©doublonnage fonctionne (fuzzy matching)
- âœ… Tests E2E passent (ingestion + recherche)

---

### Phase 3: Optimisations AvancÃ©es (2-3 semaines)

**Objectif**: QualitÃ© extraction maximale

- [ ] Fine-tuning prompt avec exemples concrets
- [ ] Active learning sur rejets validation humaine
- [ ] DÃ©tection relations depuis diagrammes (vision amÃ©liorÃ©e)
- [ ] Fusion intelligente attributs entitÃ©s dupliquÃ©es
- [ ] UI visualisation entitÃ©s extraites par document

---

## ğŸ¯ MÃ‰TRIQUES DE SUCCÃˆS

### Court Terme (Phase 1)

- âœ… Prompt parse correctement â‰¥95% slides
- âœ… EntitÃ©s extraites â‰¥5 par slide en moyenne
- âœ… Relations extraites â‰¥2 par slide en moyenne
- âœ… Latence ingestion +0-10% vs baseline

### Moyen Terme (Phase 2)

- âœ… DÃ©doublonnage rÃ©duit entitÃ©s de 30-50%
- âœ… Canonicalisation SAP fonctionne (100% noms corrects)
- âœ… related_node_ids liÃ©s pour â‰¥80% chunks
- âœ… Zero rÃ©gression qualitÃ© chunks Qdrant

### Long Terme (Phase 3)

- âœ… QualitÃ© extraction validÃ©e â‰¥85% par humains
- âœ… Active learning amÃ©liore prompts (+10% prÃ©cision)
- âœ… KG enrichi automatiquement (â‰¥80% docs avec entitÃ©s)

---

## ğŸš¨ RISQUES & MITIGATIONS

### Risque 1: Prompt trop complexe â†’ LLM confus

**ProbabilitÃ©**: Moyenne
**Impact**: Ã‰levÃ© (parsing Ã©choue)

**Mitigation**:
- Tester prompt avec few-shot examples
- Fournir schema JSON strict
- Fallback: si parsing Ã©choue, retourner seulement chunks

---

### Risque 2: EntitÃ©s dupliquÃ©es (variations de noms)

**ProbabilitÃ©**: Ã‰levÃ©e
**Impact**: Moyen (pollution KG)

**Mitigation**:
- DÃ©doublonnage fuzzy matching deck-wide
- Canonicalisation SAP automatique
- Validation humaine (status="proposed")

---

### Risque 3: Latence accrue si LLM plus lent

**ProbabilitÃ©**: Faible
**Impact**: Moyen

**Mitigation**:
- Monitoring latence par slide
- Timeout configurables
- Retry logic robuste

---

## ğŸ“ CONCLUSION

### âœ… DÃ©cision ValidÃ©e

**L'approche unifiÃ©e** (un seul appel LLM par slide) est **optimale** car:

1. **Ã‰conomie maximale**: 0â‚¬ et 0s additionnels
2. **QualitÃ© supÃ©rieure**: Contexte complet (vision + texte)
3. **CohÃ©rence parfaite**: Chunks et entitÃ©s issus de mÃªme analyse
4. **SimplicitÃ©**: Un seul workflow Ã  maintenir

### ğŸš€ Prochaine Ã‰tape

**DÃ©marrer Phase 1 (POC Minimal)** :
1. CrÃ©er branche `feat/unified-llm-extraction`
2. Modifier prompt slide
3. Adapter `ask_gpt_slide_analysis()`
4. Tester sur 1 PPTX

**Estimation effort**: 2-3 jours dÃ©veloppement + tests

---

**Document de rÃ©fÃ©rence** pour implÃ©mentation unifiÃ©e extraction LLM.
