# TODO : Migration Authentification JWT + Correction Pipeline PDF

**Date de cr√©ation** : 2025-10-11
**Statut** : √Ä faire
**Priorit√©** : Haute

---

## üìã Vue d'ensemble

Ce document d√©crit deux t√¢ches majeures √† r√©aliser :
1. **Migration Frontend** : Centraliser l'authentification JWT avec intercepteur axios (15 fichiers)
2. **Correction Pipeline PDF** : Utiliser MegaParse pour d√©coupage intelligent au lieu du traitement page par page

---

# PARTIE 1 : Migration Authentification JWT Frontend

## üéØ Objectif

Migrer toutes les pages frontend pour utiliser l'intercepteur axios centralis√© (`/lib/axios.ts`) afin de :
- ‚úÖ Injection automatique du JWT token dans toutes les requ√™tes
- ‚úÖ Auto-refresh du token en cas de 401
- ‚úÖ Gestion centralis√©e des erreurs d'authentification
- ‚úÖ Code plus propre et maintenable

## ‚úÖ √âtat Actuel

**D√©j√† migr√© :**
- ‚úÖ `frontend/src/lib/axios.ts` - Intercepteur cr√©√© avec request/response interceptors
- ‚úÖ `frontend/src/app/admin/document-types/new/page.tsx` - Page migr√©e et test√©e

**Architecture actuelle probl√©matique :**
- üî¥ 2 clients HTTP coexistent : ancien `api.ts` (classe ApiClient) + nouveau `axios.ts`
- üî¥ Beaucoup de `fetch()` manuels avec `authService.getAccessToken()`
- üî¥ Code dupliqu√© pour gestion JWT dans chaque page
- üî¥ Risque d'oublier le token lors de cr√©ation de nouvelles fonctionnalit√©s

## üìä Statistiques

- **Total fichiers √† migrer** : 15 fichiers
- **API Routes Next.js** : ~40 fichiers (d√©j√† corrects, pas de migration n√©cessaire)
- **Temps estim√©** : 8-11 heures

---

## üî¥ PHASE 1 : Pages Critiques (Priorit√© URGENTE)

### 1.1 `frontend/src/app/admin/dynamic-types/[typeName]/page.tsx`
**Priorit√©** : üî¥ CRITIQUE (plus gros fichier)

**Statistiques** :
- 20+ endpoints diff√©rents
- 300+ lignes de code
- Pattern r√©p√©t√© : `authService.getAccessToken()` + `fetch()` partout

**Endpoints concern√©s** :
```typescript
GET    /api/entity-types/:typeName/ontology-proposal
GET    /api/entity-types
GET    /api/entity-types/:typeName/snapshots
GET    /api/entity-types/:typeName
GET    /api/entities?entity_type=...&status=...
POST   /api/entity-types/:typeName/ontology-proposal
POST   /api/entity-types/:typeName/generate-ontology
POST   /api/entity-types/:typeName/normalize-entities
GET    /api/jobs/:id/status
POST   /api/entities/:uuid/approve
POST   /api/entities/:uuid/change-type
POST   /api/entity-types/:typeName/undo-normalization
POST   /api/entity-types/:typeName/merge-into/:targetType
POST   /api/entities/:uuid/merge
POST   /api/entities/bulk-change-type
// ... et bien d'autres
```

**Migration** :
- Remplacer tous les `authService.getAccessToken()` + `fetch()` par `axiosInstance.get/post/put/delete()`
- Supprimer l'import `authService`
- Adapter la gestion des r√©ponses (axios retourne `response.data`)

---

### 1.2 `frontend/src/app/admin/dynamic-types/page.tsx`
**Priorit√©** : üî¥ CRITIQUE

**Endpoints concern√©s** :
```typescript
GET    /api/entity-types?status=...
POST   /api/entity-types/:typeName/approve
POST   /api/entity-types/:typeName/reject
POST   /api/entity-types/import-yaml (FormData)
GET    /api/entity-types/export-yaml?status=...
```

**Lignes concern√©es** : 14, 83, 100, 123, 135, 171, 183, 228, 244, 281, 294

**Migration** :
- Supprimer pattern r√©p√©t√© `authService.getAccessToken()` + `fetch()`
- Remplacer par `axiosInstance`

---

### 1.3 `frontend/src/app/documents/import/page.tsx`
**Priorit√©** : üî¥ CRITIQUE (page d'import principale)

**Endpoints concern√©s** :
```typescript
GET    /api/document-types?is_active=true
POST   /api/dispatch (FormData - upload de documents)
```

**Lignes concern√©es** : 30, 116, 131, 172, 174

**Migration** :
- Remplacer fetch manuel par `axiosInstance.post()` pour upload
- FormData g√©r√© automatiquement par axios

---

### 1.4 `frontend/src/app/rfp-excel/page.tsx`
**Priorit√©** : üî¥ CRITIQUE (workflow RFP complet)

**Endpoints concern√©s** :
```typescript
POST   /api/documents/analyze-excel (FormData)
POST   /api/documents/upload-excel-qa (FormData)
POST   /api/documents/fill-rfp-excel (FormData)
```

**Lignes concern√©es** : 50, 147, 163, 311, 341

**Migration** :
- Remplacer tous les fetch FormData par `axiosInstance.post()`

---

### 1.5 `frontend/src/app/chat/page.tsx`
**Priorit√©** : üî¥ CRITIQUE (interface chat principale)

**Endpoints concern√©s** :
```typescript
GET    /api/solutions
POST   /api/search (body: {question, language, mime, solution})
```

**Lignes concern√©es** : 23, 47, 111

**Migration** :
- Remplacer `api.search.solutions()` par `axiosInstance.get('/api/solutions')`
- Remplacer `api.chat.send()` par `axiosInstance.post('/api/search', data)`

---

### 1.6 `frontend/src/app/documents/status/page.tsx`
**Priorit√©** : üî¥ CRITIQUE (suivi des imports)

**Endpoints concern√©s** :
```typescript
GET    /api/imports/history
GET    /api/imports/active
GET    /api/status/:uid
DELETE /api/imports/:uid
```

**Lignes concern√©es** : 47, 96, 108, 168, ~250+

**Migration** :
- Remplacer tous les helpers `api.imports.*` et `api.status.*` par `axiosInstance`

---

## üü° PHASE 2 : Pages Secondaires (Priorit√© HAUTE)

### 2.1 `frontend/src/app/admin/document-types/page.tsx`
**Endpoints** : `GET /api/document-types`, `DELETE /api/document-types/:id`
**Lignes** : 29, 53, 60

### 2.2 `frontend/src/app/admin/documents/page.tsx`
**Endpoints** : `GET /api/documents/list?status=...&document_type=...&limit=...&offset=...`
**Lignes** : 44, 62-67

### 2.3 `frontend/src/app/admin/documents/[id]/compare/page.tsx`
**Endpoints** : `GET /api/documents/:id/versions`

### 2.4 `frontend/src/app/admin/documents/[id]/timeline/page.tsx`
**Endpoints** : `GET /api/documents/:id`, `GET /api/documents/:id/versions`

### 2.5 `frontend/src/app/admin/page.tsx`
**Endpoints** : `GET /api/admin/monitoring/stats`

### 2.6 `frontend/src/app/documents/upload/page.tsx`
**Endpoints** : `POST /api/dispatch` (FormData)

### 2.7 `frontend/src/app/documents/[id]/page.tsx`
**Endpoints** : `GET /api/documents/:id`, `DELETE /api/documents/:id`

---

## üü¢ PHASE 3 : Composants UI (Priorit√© MOYENNE)

### 3.1 `frontend/src/components/ui/SAPSolutionSelector.tsx`
**Endpoints** :
```typescript
GET    /api/sap-solutions
GET    /api/sap-solutions/with-chunks?extend_search=...
POST   /api/sap-solutions/resolve
```

**Lignes** : 30, 72, 91, 143, 156

---

## üßπ PHASE 4 : Nettoyage Final

1. **Supprimer l'ancien client** : `frontend/src/lib/api.ts` (classe ApiClient)
2. **Mettre √† jour CLAUDE.md** : Documenter que `axiosInstance` est le client HTTP unique
3. **V√©rifier imports** : S'assurer qu'aucun fichier n'importe encore `api.ts`
4. **Tests de r√©gression** : Tester toutes les pages migr√©es

---

## üìù Pattern de Migration Type

### ‚ùå AVANT (ancien pattern avec api.ts)
```typescript
import { api } from '@/lib/api'

const { data } = useQuery({
  queryKey: ['documents'],
  queryFn: () => api.documents.list({ status: 'active' })
})
```

### ‚ùå AVANT (fetch manuel avec authService)
```typescript
import { authService } from '@/lib/auth'

const token = authService.getAccessToken()
const response = await fetch('/api/entity-types', {
  headers: {
    'Authorization': `Bearer ${token}`
  }
})
```

### ‚úÖ APR√àS (avec intercepteur centralis√©)
```typescript
import axiosInstance from '@/lib/axios'

const { data } = useQuery({
  queryKey: ['documents'],
  queryFn: async () => {
    const response = await axiosInstance.get('/api/documents', {
      params: { status: 'active' }
    })
    return response.data
  }
})
```

**Avantages** :
- ‚úÖ JWT inject√© automatiquement
- ‚úÖ Auto-refresh du token si 401
- ‚úÖ Gestion centralis√©e des erreurs
- ‚úÖ Timeout configurable par requ√™te
- ‚úÖ Code plus propre et maintenable

---

## ‚ö†Ô∏è Points d'Attention

### 1. Gestion des FormData
L'intercepteur axios g√®re automatiquement le `Content-Type` pour FormData :
```typescript
// Pas besoin de d√©finir Content-Type manuellement
const formData = new FormData()
formData.append('file', file)

await axiosInstance.post('/api/dispatch', formData)
// axios d√©tecte FormData et configure les headers automatiquement
```

### 2. Gestion des Erreurs
L'intercepteur g√®re d√©j√† les 401 (auto-refresh) :
```typescript
try {
  const response = await axiosInstance.get('/api/data')
  // Si 401, le refresh est automatique
} catch (error) {
  // G√©rer uniquement les autres erreurs
}
```

### 3. Contexte AuthContext
Le `AuthContext` utilise toujours `authService` pour login/logout/register - **NE PAS MIGRER** ces appels car ce sont des op√©rations d'authentification initiales qui ne peuvent pas utiliser l'intercepteur.

---

## ‚úÖ Checklist de Migration par Fichier

Pour chaque fichier :
- [ ] Remplacer `import { api/apiClient } from '@/lib/api'` par `import axiosInstance from '@/lib/axios'`
- [ ] Supprimer `import { authService } from '@/lib/auth'` (sauf pour login/logout)
- [ ] Remplacer tous les `authService.getAccessToken()` + `fetch()` par `axiosInstance.get/post/put/delete()`
- [ ] Remplacer tous les `api.xxx()` ou `apiClient.xxx()` par `axiosInstance.xxx()`
- [ ] Adapter la gestion des r√©ponses (axios retourne `response.data` directement)
- [ ] Tester la page/composant manuellement
- [ ] V√©rifier que le JWT est bien inject√© (DevTools Network)
- [ ] V√©rifier que le refresh automatique fonctionne en cas de token expir√©

---

# PARTIE 2 : Correction Pipeline PDF avec MegaParse

## üö® Probl√®me Identifi√©

### Sympt√¥mes (logs worker)
```
Page 21 [TEXT-ONLY]: 0 concepts + 0 facts + 16 entities + 0 relations
Page 22 [TEXT-ONLY]: 0 concepts + 0 facts + 19 entities + 0 relations
Page 23 [TEXT-ONLY]: 0 concepts + 0 facts + 17 entities + 0 relations
...
```

### Causes Racines

#### 1. Traitement Page par Page (Anti-Pattern)
**Fichier** : `src/knowbase/ingestion/pipelines/pdf_pipeline.py`
**Lignes** : 420-441

```python
# ‚ùå PROBL√àME : Boucle sur chaque page individuellement
for page_index, img_path in image_paths.items():
    logger.info(f"Page {page_index}/{len(image_paths)}")

    if use_vision:
        chunks = ask_gpt_slide_analysis(img_path, pdf_text, pdf_path.name, page_index, custom_prompt)
    else:
        # Mode TEXT-ONLY : Analyse page par page sans contexte coh√©rent
        result = ask_gpt_page_analysis_text_only(pdf_text, pdf_path.name, page_index, custom_prompt)
        concepts = result.get("concepts", [])
```

**Probl√®me** : Une page PDF ne correspond pas √† un bloc s√©mantique coh√©rent :
- Une page peut contenir plusieurs sections sans rapport
- Un concept peut s'√©tendre sur plusieurs pages
- Les relations entre entit√©s sont perdues si elles sont sur des pages diff√©rentes

#### 2. MegaParse Install√© mais Non Utilis√©
**MegaParse est disponible** (confirm√© par logs startup) :
```
2025-10-11 21:20:48,165 INFO: ‚úÖ MegaParse disponible
```

**Mais jamais utilis√©** dans le pipeline PDF !

MegaParse permet de :
- ‚úÖ D√©couper le PDF en **blocs s√©mantiques coh√©rents** (sections, paragraphes, tableaux)
- ‚úÖ Pr√©server la **structure du document**
- ‚úÖ Identifier les **titres, sous-titres, listes**
- ‚úÖ Extraire les **tableaux structur√©s**
- ‚úÖ G√©rer les **multi-colonnes**

#### 3. Mod√®le LLM Trop Faible
**Fichier** : `src/knowbase/ingestion/pipelines/pdf_pipeline.py:225`

```python
# Utilise TaskType.LONG_TEXT_SUMMARY qui mappe vers gpt-4o-mini
raw = llm_router.complete(TaskType.LONG_TEXT_SUMMARY, messages, temperature=0.2, max_tokens=8000)
```

**Configuration** : `config/llm_models.yaml:16`
```yaml
long_summary: "gpt-4o-mini"
```

**Probl√®me** :
- `gpt-4o-mini` est un mod√®le √©conomique mais **moins performant** pour extraction complexe
- Le prompt demande concepts + facts + entities + relations
- Mais le mod√®le ne trouve que les entit√©s (cas le plus simple)

#### 4. Prompt Non Adapt√© au Contexte Page-by-Page
**Fichier** : `src/knowbase/ingestion/pipelines/pdf_pipeline.py:189-212`

Le prompt demande :
```
Extract structured knowledge from this page and return a JSON object with 4 keys:
- concepts: Array of concept blocks
- facts: Array of factual statements
- entities: Array of named entities
- relations: Array of relationships between entities
```

**Probl√®me** : Le prompt ne peut pas fonctionner correctement car :
- ‚ùå Il analyse une **page arbitraire** sans contexte des pages pr√©c√©dentes/suivantes
- ‚ùå Les concepts qui s'√©tendent sur plusieurs pages sont fragment√©s
- ‚ùå Les relations entre entit√©s sur diff√©rentes pages sont impossibles √† d√©tecter
- ‚ùå Le mod√®le gpt-4o-mini n'est pas assez puissant pour cette t√¢che complexe

---

## ‚úÖ Solution Propos√©e : Pipeline PDF Intelligent avec MegaParse

### Architecture Cible

```
PDF Input
    ‚Üì
MegaParse Parser
    ‚Üì (d√©coupe en blocs s√©mantiques)
Blocs Coh√©rents
    ‚îú‚îÄ Sections (avec titres)
    ‚îú‚îÄ Paragraphes (texte continu)
    ‚îú‚îÄ Tableaux (structur√©s)
    ‚îî‚îÄ Listes (ordonn√©es/non ordonn√©es)
    ‚Üì
LLM Analysis (par bloc coh√©rent, pas par page)
    ‚Üì (extraction concepts/facts/entities/relations)
Knowledge Graph
    ‚Üì
Qdrant Vectorization
```

### √âtapes de Refactoring

#### √âtape 1 : Int√©grer MegaParse pour D√©coupage Intelligent

**Cr√©er** : `src/knowbase/ingestion/parsers/megaparse_pdf.py`

```python
from megaparse import MegaParse
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

def parse_pdf_with_megaparse(pdf_path: Path) -> list[dict]:
    """
    Utilise MegaParse pour d√©couper un PDF en blocs s√©mantiques coh√©rents.

    Returns:
        List of semantic blocks with:
        - block_type: "section" | "paragraph" | "table" | "list"
        - title: Optional heading (for sections)
        - content: Text content
        - page_range: (start_page, end_page)
        - metadata: Additional structural info
    """
    logger.info(f"üîç MegaParse: analyse {pdf_path.name}")

    parser = MegaParse()
    document = parser.parse(str(pdf_path))

    blocks = []
    for elem in document.elements:
        block = {
            "block_type": elem.type,  # section, paragraph, table, list
            "title": getattr(elem, "title", None),
            "content": elem.text,
            "page_range": (elem.start_page, elem.end_page),
            "metadata": {
                "level": getattr(elem, "level", 0),  # Heading level (H1, H2, etc.)
                "style": getattr(elem, "style", None),
            }
        }
        blocks.append(block)

    logger.info(f"‚úÖ MegaParse: {len(blocks)} blocs s√©mantiques extraits")
    return blocks
```

#### √âtape 2 : Refactoriser `process_pdf()` pour Utiliser les Blocs

**Modifier** : `src/knowbase/ingestion/pipelines/pdf_pipeline.py:363-469`

```python
def process_pdf(pdf_path: Path, document_type_id: str | None = None, use_vision: bool = True):
    logger.info(f"üöÄ Traitement: {pdf_path.name}")
    logger.info(f"üìã Document Type ID: {document_type_id or 'default'}")
    logger.info(f"üîç Mode extraction: {'VISION (GPT-4 avec images)' if use_vision else 'TEXT-ONLY (MegaParse + LLM)'}")

    # ... (m√©tadonn√©es, prompt contextualis√© - inchang√©)

    if use_vision:
        # ‚úÖ Mode VISION : Conserver traitement page par page avec images
        logger.info("üñºÔ∏è Mode VISION: G√©n√©ration PNG des pages")
        images = convert_from_path(str(pdf_path))
        image_paths = {}
        for i, img in enumerate(images, start=1):
            img_path = SLIDES_PNG / f"{pdf_path.stem}_page_{i}.png"
            img.save(img_path, "PNG")
            image_paths[i] = img_path

        total_chunks = 0
        for page_index, img_path in image_paths.items():
            logger.info(f"Page {page_index}/{len(image_paths)}")
            chunks = ask_gpt_slide_analysis(img_path, pdf_text, pdf_path.name, page_index, custom_prompt)
            logger.info(f"Page {page_index} [VISION]: chunks = {len(chunks)}")
            ingest_chunks(chunks, doc_meta, pdf_path.stem, page_index)
            total_chunks += len(chunks)
    else:
        # ‚úÖ Mode TEXT-ONLY : Utiliser MegaParse pour d√©coupage intelligent
        from knowbase.ingestion.parsers.megaparse_pdf import parse_pdf_with_megaparse

        logger.info("üìö Mode TEXT-ONLY: D√©coupage MegaParse en blocs s√©mantiques")
        semantic_blocks = parse_pdf_with_megaparse(pdf_path)

        total_chunks = 0
        for block_index, block in enumerate(semantic_blocks, start=1):
            logger.info(f"Bloc {block_index}/{len(semantic_blocks)} [{block['block_type']}]: {block.get('title', 'Sans titre')}")

            # Analyser chaque bloc s√©mantique (au lieu de chaque page)
            result = ask_gpt_block_analysis_text_only(
                block_content=block['content'],
                block_type=block['block_type'],
                block_title=block.get('title'),
                source_name=pdf_path.name,
                block_index=block_index,
                custom_prompt=custom_prompt
            )

            concepts = result.get("concepts", [])
            facts = result.get("facts", [])
            entities = result.get("entities", [])
            relations = result.get("relations", [])

            logger.info(f"Bloc {block_index} [TEXT-ONLY]: {len(concepts)} concepts + {len(facts)} facts + {len(entities)} entities + {len(relations)} relations")

            # Ingest avec r√©f√©rence au bloc s√©mantique (pas √† une page)
            chunks_compat = [
                {
                    "text": c.get("full_explanation", ""),
                    "meta": {
                        **c.get("meta", {}),
                        "block_type": block['block_type'],
                        "block_title": block.get('title'),
                        "page_range": block['page_range']
                    }
                }
                for c in concepts
            ]
            ingest_chunks(chunks_compat, doc_meta, pdf_path.stem, block_index)
            total_chunks += len(chunks_compat)

            # Heartbeat tous les 5 blocs
            if block_index % 5 == 0:
                try:
                    from knowbase.ingestion.queue.jobs import send_worker_heartbeat
                    send_worker_heartbeat()
                except Exception:
                    pass

    # ... (d√©placement fichier, status - inchang√©)
```

#### √âtape 3 : Cr√©er `ask_gpt_block_analysis_text_only()`

**Ajouter** : `src/knowbase/ingestion/pipelines/pdf_pipeline.py`

```python
def ask_gpt_block_analysis_text_only(
    block_content: str,
    block_type: str,
    block_title: str | None,
    source_name: str,
    block_index: int,
    custom_prompt: str | None = None,
):
    """
    Analyse un BLOC S√âMANTIQUE (section, paragraph, table, list) extrait par MegaParse.
    Plus intelligent que l'analyse page par page.
    """
    logger.info(f"üß† GPT [TEXT-ONLY]: analyse bloc #{block_index} [{block_type}]")

    try:
        # Prompt adapt√© au type de bloc
        if custom_prompt:
            analysis_text = custom_prompt.replace("{slide_content}", block_content).replace("{slide_index}", str(block_index)).replace("{source_name}", source_name)
        else:
            block_context = f"Block type: {block_type}"
            if block_title:
                block_context += f"\nBlock title: {block_title}"

            analysis_text = (
                f"You are analyzing a semantic block (block #{block_index}) from '{source_name}'.\n"
                f"{block_context}\n\n"
                f"Block content:\n{block_content}\n\n"
                "Extract structured knowledge from this semantic block and return a JSON object with 4 keys:\n"
                "- `concepts`: Array of concept blocks (main ideas, explanations)\n"
                "  - `full_explanation`: string (detailed description)\n"
                "  - `meta`: object with `type`, `level`, `topic`\n"
                "- `facts`: Array of factual statements\n"
                "  - `subject`: string\n"
                "  - `predicate`: string\n"
                "  - `value`: string/number\n"
                "  - `confidence`: number (0-1)\n"
                "  - `fact_type`: string (optional)\n"
                "- `entities`: Array of named entities\n"
                "  - `name`: string\n"
                "  - `entity_type`: string (e.g., 'SOLUTION', 'COMPONENT', 'TECHNOLOGY')\n"
                "  - `description`: string (optional)\n"
                "- `relations`: Array of relationships between entities\n"
                "  - `source`: string (entity name)\n"
                "  - `relation_type`: string (e.g., 'PART_OF', 'USES', 'IMPLEMENTS')\n"
                "  - `target`: string (entity name)\n"
                "  - `description`: string (optional)\n\n"
                "Important: This is a SEMANTIC BLOCK, not just a page. "
                "Extract all concepts, facts, entities, and relations that are COMPLETE within this block. "
                "If a concept spans multiple blocks, extract only the part contained in this block.\n\n"
                "Return only valid JSON."
            )

        system_message: ChatCompletionSystemMessageParam = {
            "role": "system",
            "content": "You are an expert knowledge extraction assistant. Analyze semantic blocks (sections, paragraphs, tables, lists) to extract concepts, facts, entities, and relations. Focus on completeness and accuracy.",
        }
        user_message: ChatCompletionUserMessageParam = {
            "role": "user",
            "content": analysis_text,
        }
        messages: list[ChatCompletionMessageParam] = [system_message, user_message]

        # ‚úÖ Utiliser un mod√®le plus puissant pour extraction complexe
        # Option 1: Garder gpt-4o-mini mais avec max_tokens plus √©lev√©
        # Option 2: Utiliser gpt-4o (plus co√ªteux mais meilleur)
        # Option 3: Utiliser claude-3-haiku (bon compromis co√ªt/qualit√©)
        raw = llm_router.complete(
            TaskType.LONG_TEXT_SUMMARY,  # Ou cr√©er TaskType.KNOWLEDGE_EXTRACTION
            messages,
            temperature=0.2,
            max_tokens=12000  # Augment√© pour permettre extraction compl√®te
        )

        logger.debug(f"Bloc {block_index} [TEXT-ONLY]: LLM raw response (first 500 chars): {raw[:500] if raw else 'EMPTY'}")

        cleaned = clean_gpt_response(raw)
        response_data = json.loads(cleaned) if cleaned else {}

        # Parser la r√©ponse
        if isinstance(response_data, dict):
            concepts = response_data.get("concepts", [])
            facts_data = response_data.get("facts", [])
            entities_data = response_data.get("entities", [])
            relations_data = response_data.get("relations", [])
        else:
            logger.warning(f"Bloc {block_index} [TEXT-ONLY]: Format JSON inattendu: {type(response_data)}")
            concepts = []
            facts_data = []
            entities_data = []
            relations_data = []

        logger.debug(f"Bloc {block_index} [TEXT-ONLY]: {len(concepts)} concepts + {len(facts_data)} facts + {len(entities_data)} entities + {len(relations_data)} relations")

        # üìä Ingest facts, entities, relations dans le knowledge graph
        # TODO: Impl√©menter l'ingestion dans Neo4j/Graphiti si activ√©

        return {
            "concepts": concepts,
            "facts": facts_data,
            "entities": entities_data,
            "relations": relations_data,
        }
    except Exception as e:
        logger.error(f"‚ùå GPT bloc {block_index} [TEXT-ONLY] error: {e}")
        return {
            "concepts": [],
            "facts": [],
            "entities": [],
            "relations": [],
        }
```

#### √âtape 4 : Optimiser la Configuration LLM

**Option A** : Augmenter les capacit√©s de `long_summary`

Modifier `config/llm_models.yaml:16` :
```yaml
# Avant
long_summary: "gpt-4o-mini"

# Apr√®s (meilleur compromis)
long_summary: "claude-3-haiku-20240307"
```

**OU cr√©er une nouvelle t√¢che sp√©cifique** :

Ajouter dans `config/llm_models.yaml` :
```yaml
task_models:
  # ... (existant)

  # Nouvelle t√¢che pour extraction de connaissance structur√©e
  knowledge_extraction: "claude-3-haiku-20240307"  # Ou "gpt-4o" pour qualit√© max

task_parameters:
  # ... (existant)

  knowledge_extraction:
    temperature: 0.2
    max_tokens: 12000  # Permettre extraction compl√®te de blocs longs
```

Puis ajouter dans `src/knowbase/common/llm_router.py` :
```python
class TaskType(str, Enum):
    # ... (existant)
    KNOWLEDGE_EXTRACTION = "knowledge_extraction"
```

---

## üéØ R√©sultats Attendus Apr√®s Correction

### Avant (page par page)
```
Page 21 [TEXT-ONLY]: 0 concepts + 0 facts + 16 entities + 0 relations
Page 22 [TEXT-ONLY]: 0 concepts + 0 facts + 19 entities + 0 relations
Page 23 [TEXT-ONLY]: 0 concepts + 0 facts + 17 entities + 0 relations
```

### Apr√®s (blocs s√©mantiques)
```
Bloc 1 [section]: "Introduction to SAP S/4HANA"
  ‚Üí 3 concepts + 8 facts + 12 entities + 15 relations

Bloc 2 [paragraph]: "Architecture Overview"
  ‚Üí 2 concepts + 5 facts + 8 entities + 10 relations

Bloc 3 [table]: "Module Comparison Table"
  ‚Üí 1 concept + 12 facts + 6 entities + 8 relations

Bloc 4 [list]: "Deployment Options"
  ‚Üí 1 concept + 4 facts + 4 entities + 3 relations
```

**Am√©liorations** :
- ‚úÖ Concepts et facts extraits correctement
- ‚úÖ Relations entre entit√©s d√©tect√©es
- ‚úÖ Structuration coh√©rente du knowledge graph
- ‚úÖ Meilleure qualit√© de recherche vectorielle

---

## ‚úÖ Checklist d'Impl√©mentation

### Phase 1 : Pr√©paration
- [ ] V√©rifier que MegaParse est bien install√© dans requirements.txt
- [ ] Tester MegaParse sur un PDF sample pour valider l'API
- [ ] Cr√©er `src/knowbase/ingestion/parsers/megaparse_pdf.py`

### Phase 2 : Refactoring
- [ ] Cr√©er `ask_gpt_block_analysis_text_only()` dans `pdf_pipeline.py`
- [ ] Modifier `process_pdf()` pour utiliser MegaParse en mode TEXT-ONLY
- [ ] Conserver le mode VISION (page par page) inchang√©
- [ ] Ajouter `TaskType.KNOWLEDGE_EXTRACTION` dans `llm_router.py`
- [ ] Configurer `knowledge_extraction` dans `llm_models.yaml`

### Phase 3 : Tests
- [ ] Tester import PDF avec `use_vision=False` sur document sample
- [ ] V√©rifier logs : concepts + facts + entities + relations > 0
- [ ] V√©rifier Qdrant : chunks avec m√©tadonn√©es de blocs s√©mantiques
- [ ] Comparer qualit√© de recherche avant/apr√®s

### Phase 4 : Optimisation
- [ ] Ajuster `max_tokens` si besoin
- [ ] Tester diff√©rents mod√®les LLM (gpt-4o-mini vs claude-haiku vs gpt-4o)
- [ ] Optimiser d√©coupage MegaParse si n√©cessaire

---

## üìä Comparaison Performance Attendue

| M√©thode | Concepts | Facts | Entities | Relations | Co√ªt | Qualit√© |
|---------|----------|-------|----------|-----------|------|---------|
| **VISION (GPT-4o + images)** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞üí∞üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **TEXT-ONLY Page-by-Page (ACTUEL)** | ‚ùå | ‚ùå | ‚≠ê‚≠ê‚≠ê | ‚ùå | üí∞ | ‚≠ê‚≠ê |
| **TEXT-ONLY MegaParse (PROPOS√â)** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê |

**Conclusion** : MegaParse offre un excellent compromis co√ªt/qualit√© pour le mode TEXT-ONLY.

---

## üîó R√©f√©rences

- **MegaParse Documentation** : https://github.com/quivrhq/megaparse
- **LLM Router** : `src/knowbase/common/llm_router.py`
- **Configuration LLM** : `config/llm_models.yaml`
- **Pipeline PDF Actuel** : `src/knowbase/ingestion/pipelines/pdf_pipeline.py`

---

**Fin du document** - Derni√®re mise √† jour : 2025-10-11
