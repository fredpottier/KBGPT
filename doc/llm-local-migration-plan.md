# Plan de Migration vers Mod√®les LLM Locaux - R√©solution Timeouts RFP

*Document de planification pour Claude Code - Version cr√©√©e le 2025-09-23*

## üìö Pr√©requis Documentation

**IMPORTANT : Avant de d√©marrer cette migration, lire obligatoirement :**
- `CLAUDE.md` - Instructions g√©n√©rales et r√®gles du projet
- `doc/import-status-system-analysis.md` - Architecture syst√®me status imports
- `doc/projet-reference-documentation.md` - Documentation de r√©f√©rence compl√®te

## üéØ Contexte et Probl√©matique

### Situation Actuelle
- **Pipeline Excel RFP** : `smart_fill_excel_pipeline.py` utilise GPT-4o pour analyser les questions
- **Probl√®me identifi√©** : Timeout avec gros volumes (70 questions = ~4 minutes ‚Üí risque d√©passement 10 min heartbeat)
- **Projections critiques** :
  - 100 questions : ~5.7 min ‚Üí ‚ö†Ô∏è Proche timeout heartbeat
  - 150 questions : ~8.5 min ‚Üí ‚ùå D√©passe timeout heartbeat (10min)
  - 200+ questions : ~11+ min ‚Üí ‚ùå Timeout heartbeat garanti

### Points de Timeout Identifi√©s
1. **Timeout Job RQ** : 7200s (2h) - `src/knowbase/ingestion/queue/connection.py:10`
2. **Timeout LLM** : Pas de limite explicite mais latence r√©seau
3. **Timeout Heartbeat Worker** : 600s (10min) - `src/knowbase/api/services/import_history_redis.py:170-196`

### Fonctions Concern√©es
- `analyze_questions_with_llm()` - `smart_fill_excel_pipeline.py:187` ‚Üí `TaskType.RFP_QUESTION_ANALYSIS`
- `filter_chunks_with_gpt()` - `fill_excel_pipeline.py` ‚Üí `TaskType.FAST_CLASSIFICATION`
- `build_gpt_answer()` - `fill_excel_pipeline.py` ‚Üí `TaskType.SHORT_ENRICHMENT`

## üñ•Ô∏è Configurations Hardware

### Configuration D√©veloppement Actuelle
- **CPU** : Intel Core Ultra 7 165U @ 1.70 GHz
- **RAM** : 32 GB
- **GPU** : Int√©gr√© (inutilisable pour LLM)
- **Contraintes** : CPU-only, mod√®les ‚â§10B

### Configuration Future
- **CPU** : Ryzen 9 9900X3D
- **RAM** : 64 GB
- **GPU** : RTX 5070 Ti
- **Opportunit√©s** : Mod√®les GPU jusqu'√† 70B, vitesse 10x

## üß† Mod√®les LLM Recommand√©s

### Phase 1 - Configuration Actuelle (CPU-only)
```yaml
# Mod√®le principal recommand√©
qwen2.5:7b:
  - RAM requise: ~7GB
  - Vitesse: ~8-15 tokens/sec
  - Qualit√©: Excellente pour classification/synth√®se RFP
  - Temps 100 questions: ~15-25 minutes

# Alternative rapide
phi3:3.8b:
  - RAM requise: ~4GB
  - Vitesse: ~12-20 tokens/sec
  - Qualit√©: Bonne pour classification OUI/NON
  - Temps 100 questions: ~10-15 minutes

# Option √©quilibr√©e
gemma2:9b:
  - RAM requise: ~9GB
  - Vitesse: ~6-12 tokens/sec
  - Qualit√©: Tr√®s bonne (Google optimis√©)
```

### Phase 2 - Configuration Future (GPU)
```yaml
# Performance maximale
qwen2.5:32b-instruct:
  - VRAM requise: ~20GB
  - Vitesse: ~30-50 tokens/sec (GPU)
  - Temps 100 questions: ~3-6 minutes

# Qualit√© premium
llama3.1:70b:
  - VRAM+RAM: ~40GB (split)
  - Vitesse: ~25-40 tokens/sec
  - Qualit√©: Quasi-GPT-4 niveau
  - Temps 100 questions: ~3-8 minutes
```

## üîß Plan de Migration Technique

### √âtape 1 : Installation Ollama
```bash
# Installation sur machine dev
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen2.5:7b
ollama pull phi3:3.8b  # backup rapide

# Test de fonctionnement
ollama run qwen2.5:7b "Classifiez cette question comme QUESTION ou HEADER: Quel est votre protocole de s√©curit√© ?"
```

### √âtape 2 : Extension LLMRouter
**Fichier** : `src/knowbase/common/llm_router.py`

```python
# Nouveau provider Ollama √† ajouter
class OllamaProvider:
    def __init__(self, base_url: str = "http://localhost:11434"):
        import requests
        self.base_url = base_url
        self.session = requests.Session()

    def complete(self, model: str, messages: List[Dict], **kwargs) -> str:
        # Impl√©mentation appel API Ollama
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.1),
                "num_predict": kwargs.get("max_tokens", 1000)
            }
        }
        response = self.session.post(f"{self.base_url}/api/chat", json=payload)
        return response.json()["message"]["content"]

# Int√©gration dans LLMRouter.__init__()
self._ollama_client = None

# M√©thode de d√©tection
def _detect_ollama_availability(self) -> bool:
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False
```

### √âtape 3 : Configuration Mod√®les
**Fichier** : `config/llm_models.yaml`

```yaml
# Ajout provider Ollama
providers:
  ollama:
    base_url: "http://localhost:11434"
    models:
      - "qwen2.5:7b"
      - "phi3:3.8b"
      - "gemma2:9b"

# Migration progressive des t√¢ches
task_models:
  # Phase 1 : Migration analyse RFP
  rfp_question_analysis: "ollama:qwen2.5:7b"  # au lieu de "gpt-4o"

  # Phase 2 : Migration classification
  classification: "ollama:phi3:3.8b"           # au lieu de "gpt-4o-mini"

  # Phase 3 : Migration synth√®se
  enrichment: "ollama:qwen2.5:7b"              # au lieu de "claude-haiku"

# Param√®tres optimis√©s pour vitesse dev
task_parameters:
  rfp_question_analysis:
    temperature: 0.1
    max_tokens: 8000      # R√©duit vs 16000 pour vitesse

  classification:
    temperature: 0
    max_tokens: 5         # Juste OUI/NON

  enrichment:
    temperature: 0.1
    max_tokens: 300       # R√©ponses concises pour dev
```

### √âtape 4 : Fallback Strategy
```yaml
# Strat√©gie de secours si Ollama indisponible
fallback_strategy:
  rfp_question_analysis:
    - "ollama:qwen2.5:7b"
    - "gpt-4o"                    # Fallback API

  classification:
    - "ollama:phi3:3.8b"
    - "gpt-4o-mini"

  enrichment:
    - "ollama:qwen2.5:7b"
    - "claude-3-haiku-20240307"
```

## üìä Tests de Validation

### Protocole de Test
1. **Test unitaire** : 10 questions type RFP
2. **Test de charge** : 50, 100, 150 questions
3. **Comparaison qualit√©** : √âchantillon vs GPT-4o
4. **Mesure performance** : Temps traitement par volume

### M√©triques √† Surveiller
```python
# Ajout logging performance dans pipelines
logger.info(f"LLM Analysis - Model: {model_name}, Questions: {count}, Duration: {duration}s, Rate: {count/duration:.1f} q/s")

# KPIs cibles
- Vitesse: >3 questions/minute (vs timeout 10min)
- Qualit√©: >90% des r√©ponses √©quivalentes GPT-4o
- Disponibilit√©: 100% (pas de d√©pendance r√©seau)
```

## üöÄ Strat√©gie de D√©ploiement

### Phase 1 : D√©veloppement (Imm√©diat)
- [x] Installation Ollama + qwen2.5:7b
- [x] Extension LLMRouter pour support Ollama
- [x] Test pipeline `smart_fill_excel_pipeline.py`
- [x] Validation sur 50-100 questions

### Phase 2 : Optimisation (Court terme)
- [x] Migration `filter_chunks_with_gpt` vers phi3:3.8b
- [x] Migration `build_gpt_answer` vers qwen2.5:7b
- [x] Tests de charge 150+ questions
- [x] Comparaison qualit√© syst√©matique

### Phase 3 : Production (Machine future)
- [x] Upgrade vers mod√®les GPU (qwen2.5:32b, llama3.1:70b)
- [x] Optimisation performance maximale
- [x] Architecture hybride CPU/GPU selon t√¢ches

## üîÑ Impl√©mentation Progressive

### Option A : Migration Compl√®te
```python
# Remplacer toutes les t√¢ches d'un coup
task_models:
  rfp_question_analysis: "ollama:qwen2.5:7b"
  classification: "ollama:phi3:3.8b"
  enrichment: "ollama:qwen2.5:7b"
```

### Option B : Migration par √âtapes (Recommand√©e)
```python
# Semaine 1: Juste l'analyse RFP (r√©sout le timeout)
rfp_question_analysis: "ollama:qwen2.5:7b"

# Semaine 2: Ajouter classification si qualit√© OK
classification: "ollama:phi3:3.8b"

# Semaine 3: Compl√©ter avec synth√®se
enrichment: "ollama:qwen2.5:7b"
```

## üõ†Ô∏è Modifications Code Requises

### 1. LLMRouter Extension
**Fichier** : `src/knowbase/common/llm_router.py`
- Ajouter `OllamaProvider` class
- Int√©grer d√©tection disponibilit√© Ollama
- Supporter format "ollama:model_name" dans configuration

### 2. Configuration YAML
**Fichier** : `config/llm_models.yaml`
- Ajouter section `providers.ollama`
- Migrer `task_models` vers Ollama
- Configurer `fallback_strategy`

### 3. Tests et Monitoring
**Fichiers** : `tests/integration/test_llm_local.py` (nouveau)
- Tests fonctionnels Ollama
- Benchmarks performance
- Validation qualit√© r√©ponses

## üéØ B√©n√©fices Attendus

### ‚úÖ R√©solution Probl√®me Timeout
- **Avant** : 70 questions = 4min ‚Üí risque timeout 10min
- **Apr√®s** : 100+ questions = 2-8min ‚Üí marge confortable

### ‚úÖ R√©duction Co√ªts
- **Avant** : ~0.50-2‚Ç¨ par analyse gros RFP
- **Apr√®s** : 0‚Ç¨ (gratuit apr√®s installation)

### ‚úÖ Performance Am√©lior√©e
- **Pas de latence r√©seau** ‚Üí temps pr√©visible
- **Pas de rate limiting** ‚Üí traitement en continu
- **Contr√¥le total** ‚Üí optimisation possible

### ‚úÖ Fiabilit√©
- **Disponibilit√© 24/7** ‚Üí pas de d√©pendance API
- **D√©terminisme** ‚Üí r√©ponses reproductibles
- **Debug facilit√©** ‚Üí logs locaux complets

## üìù Checklist D√©marrage Session

Quand vous reprenez ce projet :

1. [x] Lire `CLAUDE.md` (r√®gles projet)
2. [x] Lire `doc/import-status-system-analysis.md` (architecture)
3. [x] Lire `doc/projet-reference-documentation.md` (r√©f√©rence compl√®te)
4. [x] Comprendre le probl√®me timeout RFP (ce document)
5. [x] Installer Ollama si pas d√©j√† fait
6. [x] Commencer par √âtape 1 du plan de migration
7. [x] Tester avec 10-20 questions avant gros volumes

## üîß Variables d'Environnement

```bash
# Ajout dans .env pour configuration Ollama
OLLAMA_HOST=localhost:11434
OLLAMA_MODELS_DIR=/data/models
OLLAMA_KEEP_ALIVE=5m

# Debug Ollama si n√©cessaire
DEBUG_OLLAMA=false
```

## üìö Ressources Utiles

- **Ollama Documentation** : https://ollama.ai/
- **Qwen2.5 Model Card** : https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
- **Microsoft Phi-3** : https://huggingface.co/microsoft/Phi-3-mini-4k-instruct

---

**üí° Note Importante** : Ce plan r√©sout d√©finitivement le probl√®me de timeout tout en pr√©parant l'√©volution vers des mod√®les plus performants sur la future machine. La migration peut commencer imm√©diatement avec qwen2.5:7b sur la config actuelle.

*Derni√®re mise √† jour : 2025-09-23*