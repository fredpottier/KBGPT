# Analyse des Probl√®mes Qwen2.5-14B-AWQ (vLLM Burst Mode)

**Date:** 2026-01-27
**Contexte:** Pipeline OSMOSE Pass 1/2/3 - Extraction de connaissances
**Mod√®le:** `Qwen/Qwen2.5-14B-Instruct-AWQ` (quantized, 8192 context)
**Infrastructure:** EC2 Spot g5.xlarge (1x A10G 24GB) via vLLM

---

## 1. Configuration Actuelle

### 1.1 T√¢ches utilisant Qwen (via vLLM Burst)

| T√¢che | Fichier | Token Limit | Usage |
|-------|---------|-------------|-------|
| `knowledge_extraction` | Pass 1.1/1.2/1.3 | 4000 | Extraction assertions, concepts, liens |
| `coref_llm_arbiter` | `linguistic/coref_llm_arbiter.py` | 1000 | R√©solution de cor√©f√©rences |
| `llm_merge_gate` | `entity_resolution/llm_merge_gate.py` | 500 | Validation fusions entit√©s |
| `corpus_er_pipeline` | `consolidation/corpus_er_pipeline.py` | 1000 | Entity Resolution corpus |

### 1.2 Configuration `llm_models.yaml`

```yaml
knowledge_extraction:
  temperature: 0.2
  max_tokens: 4000  # R√©duit pour compatibilit√© Qwen2.5-14B (8192 context)
```

**Contexte effectif Qwen2.5-14B-AWQ:** 8192 tokens total (input + output)
**Limite output configur√©e:** 4000 tokens (pour laisser ~4000 √† l'input)

---

## 2. Probl√®mes Identifi√©s

### üî¥ Probl√®me 1: Troncature JSON (CRITIQUE)

**Sympt√¥me observ√© (2026-01-26 23:24):**
```
ERROR: [OSMOSE:Pass1:1.2] TRONCATURE D√âTECT√âE - JSON incomplet
Fin: ...{"term": "SAP Cloud ERP Private", "reason": "G√©n√©rique"}, {"term
ERROR: LLM Contract Violation: JSON tronqu√© d√©tect√©. Le mod√®le a probablement atteint sa limite de tokens.
```

**Cause:**
- Le prompt Phase 1.2 (concept_identification) demande une liste de concepts + termes refus√©s
- Qwen g√©n√®re des output verbeux et atteint la limite de 4000 tokens
- Le JSON est tronqu√© en plein milieu d'un objet

**Impact:**
- Pipeline crash complet
- Aucune phase ult√©rieure ex√©cut√©e (ancrage, enrichissement, etc.)
- 0% anchor rate final

**Donn√©es:**
- Input: 1529 tokens
- Output demand√©: 4000 tokens (limite atteinte)
- Total: 5529 tokens (d√©passe le budget de 4000 output)

### üî¥ Probl√®me 2: Reformulation malgr√© instruction "verbatim"

**Prompt (pass1_prompts.yaml ligne 155):**
```yaml
"text": "Le texte EXACT de l'assertion (copi√© du texte)"
```

**Comportement observ√©:**
Qwen reformule le texte au lieu de le copier verbatim, ce qui:
1. Modifie le sens original (perte de nuance)
2. Emp√™che l'ancrage (le texte reformul√© ne matche plus le DocItem)
3. Cr√©e des doublons s√©mantiques (m√™me assertion reformul√©e diff√©remment)

**Exemple typique:**
- **Source:** "Customer manages configuration, implementation, integration, monitoring, application support etc"
- **Qwen output:** "The customer is responsible for managing configuration, implementation, integration, monitoring, and application support"

**Cause probable:**
- Qwen est entra√Æn√© pour √™tre "helpful" et reformule naturellement
- L'instruction "texte EXACT" n'est pas assez forte
- Pas de contrainte structurelle (JSON schema avec regex)

### üü† Probl√®me 3: Verbosit√© excessive

**Observation:**
- Qwen g√©n√®re ~2x plus d'assertions que GPT-4o-mini pour le m√™me document
- Beaucoup sont des assertions de faible qualit√© (fragments, r√©p√©titions)

**Donn√©es test (m√™me document):**
| Mod√®le | Assertions | PROMOTED | Rate |
|--------|-----------|----------|------|
| GPT-4o-mini | ~600 | ~100 | ~16% |
| Qwen-14B | ~1126 | ~135 | ~12% |

**Impact:**
- Plus de tokens consomm√©s pour un r√©sultat √©quivalent ou pire
- Plus de bruit √† filtrer en aval
- Risque accru de troncature JSON

### üü† Probl√®me 4: Mauvais suivie des formats JSON

**Observations:**
- Tendance √† ajouter du texte explicatif avant/apr√®s le JSON
- Parfois utilise `'''json` au lieu de ` ```json `
- Inclut parfois des commentaires dans le JSON (invalide)

**Exemple:**
```
Voici les concepts extraits du document:
```json
{
  "concepts": [...]
}
```
```

**Impact:**
- Parser JSON √©choue
- N√©cessite un post-processing regex pour extraire le JSON

### üü° Probl√®me 5: Co√ªt/B√©n√©fice EC2 Spot

**Co√ªt actuel:**
- g5.xlarge: ~$0.60/heure (Spot)
- Ingestion 1 document: ~30 min ‚Üí ~$0.30

**Comparaison GPT-4o-mini:**
- ~$0.15 / 1M input tokens, ~$0.60 / 1M output tokens
- Ingestion 1 document: ~$0.02-0.05

**Conclusion:** vLLM/Qwen n'est rentable que pour des batches massifs (>100 documents).

---

## 3. T√¢ches o√π Qwen fonctionne bien

| T√¢che | Performance | Commentaire |
|-------|-------------|-------------|
| `coref_llm_arbiter` | ‚úÖ Bon | R√©ponses courtes (oui/non), pas de JSON complexe |
| `llm_merge_gate` | ‚úÖ Acceptable | Validation binaire, output limit√© |
| Classification simple | ‚úÖ Bon | Temp√©rature 0, r√©ponses courtes |

---

## 4. Alternatives √† Explorer

### Option A: Augmenter les limites Qwen

- Passer √† Qwen2.5-32B (meilleur suivi instructions)
- N√©cessite GPU plus gros (g5.2xlarge ou A100)
- Co√ªt x2

### Option B: Prompts plus stricts

- Ajouter des contraintes JSON Schema
- R√©duire verbosit√© des prompts
- Forcer format compact sans d√©finitions

### Option C: Mod√®le hybride

- Qwen pour t√¢ches simples (coref, merge_gate)
- GPT-4o-mini pour extraction JSON complexe (Pass 1.2)

### Option D: Autres mod√®les vLLM

| Mod√®le | Context | Qualit√© JSON | Verbosit√© |
|--------|---------|--------------|-----------|
| Mistral-7B-Instruct | 32K | Moyenne | Faible |
| Llama-3.1-8B-Instruct | 128K | Bonne | Moyenne |
| DeepSeek-Coder-7B | 16K | Excellente | Tr√®s faible |
| Phi-3-medium-128k | 128K | Bonne | Faible |

---

## 5. Recommandations Imm√©diates

1. **Limite output r√©duite √† 2000 tokens** pour Pass 1.2 (concepts) - forcer frugalit√©
2. **Prompt renforc√©** avec "INTERDIT de reformuler" + "COPIE VERBATIM OBLIGATOIRE"
3. **Fallback OpenAI** si JSON tronqu√© d√©tect√© (retry avec GPT-4o-mini)
4. **Validation JSON** avant parsing avec regex extraction

---

## 6. M√©triques de Comparaison √† Collecter

Pour √©valuer correctement Qwen vs GPT-4o-mini:

| M√©trique | Description |
|----------|-------------|
| Anchor Rate | % assertions ancr√©es sur DocItem |
| JSON Truncation Rate | % appels avec JSON tronqu√© |
| Verbatim Accuracy | % assertions copi√©es exactement vs reformul√©es |
| Tokens/Assertion | Efficacit√© output |
| Latency p50/p95 | Temps de r√©ponse |
| Cost/Document | Co√ªt total par document |

---

## 7. Historique des Tests

| Date | Cache | Mod√®le | Assertions | PROMOTED | Anchor Rate | Notes |
|------|-------|--------|------------|----------|-------------|-------|
| 2026-01-26 | Vision | Qwen-14B | 621 | 94 | 15.1% | Test avec Vision cache |
| 2026-01-26 | TEXT-ONLY | Qwen-14B | 1126 | 135 | 0% | Pipeline crash (troncature) |

---

*Document g√©n√©r√© pour analyse comparative avec ChatGPT*
