# Fix Batch Canonicalization - 2025-10-21

**Date** : 2025-10-21 01:40
**Commit** : 7efaa59
**Status** : ‚úÖ D√âPLOY√â - PR√äT POUR TEST UTILISATEUR

---

## üìã R√©sum√© Ex√©cutif

**Probl√®me Identifi√©** : La m√©thode `canonicalize_batch()` √©tait appel√©e par `gatekeeper.py:720` mais **n'existait pas** dans `llm_canonicalizer.py`, causant un `AttributeError` masqu√© par l'erreur "All JSON parsing attempts failed".

**Solution Impl√©ment√©e** : Ajout complet de la m√©thode `canonicalize_batch()` avec 216 lignes de code incluant :
- Traitement batch de 20 concepts par appel LLM
- Diagnostic logging raw LLM response (1000 premiers caract√®res)
- Fallback robuste per-concept ET global
- Int√©gration circuit breaker pour r√©silience

**Impact Attendu** :
- ‚úÖ R√©duction 547 appels LLM ‚Üí 28 batch calls (20 concepts/batch)
- ‚úÖ Temps canonicalization : 18 min ‚Üí < 1 min
- ‚úÖ Concepts avec `canonical_name=None` : 100 (18%) ‚Üí 0 (0%)
- ‚úÖ Co√ªt LLM : $0.82 ‚Üí $0.084 (10x moins cher)

---

## üîß Modifications Techniques

### Fichier : `src/knowbase/ontology/llm_canonicalizer.py`

#### 1. M√©thode `canonicalize_batch()` (lignes 254-388)

```python
def canonicalize_batch(
    self,
    concepts: List[Dict[str, str]],
    timeout: int = 30
) -> List[CanonicalizationResult]:
    """
    Canonicalise un batch de concepts via LLM (batch processing).

    Args:
        concepts: Liste de dicts avec cl√©s {raw_name, context, domain_hint}
        timeout: Timeout max LLM call en secondes

    Returns:
        Liste de CanonicalizationResult (m√™me ordre que concepts)
    """
    if not concepts:
        return []

    logger.debug(
        f"[LLMCanonicalizer:Batch] Canonicalizing batch of {len(concepts)} concepts"
    )

    # Construire prompt batch
    prompt = self._build_batch_canonicalization_prompt(concepts)

    try:
        # P0: Appel LLM via circuit breaker
        def _llm_call():
            from knowbase.common.llm_router import TaskType

            # Appel LLM via router
            response_content = self.llm_router.complete(
                task_type=TaskType.CANONICALIZATION,
                messages=[
                    {"role": "system", "content": CANONICALIZATION_BATCH_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                response_format={"type": "json_object"}
            )

            # Fix 2025-10-21: Log RAW response for diagnostic
            logger.info(
                f"[LLMCanonicalizer:Batch] üîç RAW LLM response (first 1000 chars):\n"
                f"{response_content[:1000]}"
            )

            # Parse r√©sultat JSON
            result_json = self._parse_json_robust(response_content)

            # Extraire r√©sultats pour chaque concept
            results = []
            concepts_results = result_json.get("concepts", [])

            for idx, concept_result in enumerate(concepts_results):
                try:
                    results.append(CanonicalizationResult(**concept_result))
                except Exception as e:
                    logger.error(
                        f"[LLMCanonicalizer:Batch] Failed to parse result {idx}: {e}, "
                        f"using fallback for '{concepts[idx]['raw_name']}'"
                    )
                    # Fallback pour ce concept
                    results.append(CanonicalizationResult(
                        canonical_name=concepts[idx]["raw_name"].strip().title(),
                        confidence=0.5,
                        reasoning="Batch parsing failed, fallback to title case",
                        aliases=[],
                        concept_type="Unknown",
                        domain=None,
                        ambiguity_warning="Batch canonicalization partial failure",
                        possible_matches=[],
                        metadata={"error": str(e)}
                    ))

            return results

        # Appel via circuit breaker
        results = self.circuit_breaker.call(_llm_call)

        logger.info(
            f"[LLMCanonicalizer:Batch] ‚úÖ Batch completed: {len(results)} concepts canonicalized"
        )

        return results

    except Exception as e:
        logger.error(
            f"[LLMCanonicalizer:Batch] ‚ùå Batch canonicalization failed: {e}, "
            f"falling back to individual processing"
        )

        # Fallback global : appel individuel pour chaque concept
        results = []
        for concept in concepts:
            try:
                individual_result = self.canonicalize(
                    raw_name=concept.get("raw_name", ""),
                    context=concept.get("context"),
                    domain_hint=concept.get("domain_hint")
                )
                results.append(individual_result)
            except Exception as fallback_error:
                logger.error(
                    f"[LLMCanonicalizer:Batch] Fallback failed for '{concept.get('raw_name')}': {fallback_error}"
                )
                # Dernier fallback : title case
                results.append(CanonicalizationResult(
                    canonical_name=concept.get("raw_name", "Unknown").strip().title(),
                    confidence=0.3,
                    reasoning="Batch and individual canonicalization failed",
                    aliases=[],
                    concept_type="Unknown",
                    domain=None,
                    ambiguity_warning="Complete failure, using title case",
                    possible_matches=[],
                    metadata={"error": str(e), "fallback_error": str(fallback_error)}
                ))

        return results
```

**Points Cl√©s** :
- ‚úÖ Logging diagnostic raw LLM response (ligne 305-308)
- ‚úÖ Fallback per-concept si JSON parsing √©choue pour un concept sp√©cifique
- ‚úÖ Fallback global vers individual processing si batch √©choue compl√®tement
- ‚úÖ Circuit breaker integration pour r√©silience API
- ‚úÖ Retour des r√©sultats dans le M√äME ORDRE que les concepts input

#### 2. M√©thode `_build_batch_canonicalization_prompt()` (lignes 390-429)

```python
def _build_batch_canonicalization_prompt(
    self,
    concepts: List[Dict[str, str]]
) -> str:
    """Construit prompt batch pour LLM."""
    concept_lines = []

    for idx, concept in enumerate(concepts, 1):
        raw_name = concept.get("raw_name", "")
        context = concept.get("context", "")
        domain_hint = concept.get("domain_hint")

        line = f"{idx}. **Name:** {raw_name}"

        if context:
            context_snippet = self._truncate_context(context, max_length=200)
            line += f" | **Context:** {context_snippet}"

        if domain_hint:
            line += f" | **Domain:** {domain_hint}"

        concept_lines.append(line)

    concepts_text = "\n".join(concept_lines)

    return f"""
**Task:** Canonicalize the following {len(concepts)} concepts.

{concepts_text}

Return a JSON object with format:
{{
  "concepts": [
    {{"canonical_name": "...", "confidence": 0.95, "reasoning": "...", ...}},
    ...
  ]
}}

IMPORTANT: Return results in SAME ORDER as input (1-{len(concepts)}).
"""
```

**Points Cl√©s** :
- ‚úÖ Truncation du contexte √† 200 chars par concept pour √©conomiser tokens
- ‚úÖ Format num√©rot√© clair pour tracking ordre
- ‚úÖ Instructions explicites pour retourner dans le m√™me ordre

#### 3. Prompt Syst√®me Batch (lignes 634-671)

```python
CANONICALIZATION_BATCH_SYSTEM_PROMPT = """You are a concept canonicalization expert specialized in batch processing.

Your task is to find the OFFICIAL CANONICAL NAME for multiple concepts extracted from documents.

# Guidelines (same as single canonicalization)

1. **Official Names**: Use official product/company/standard names
2. **Acronyms**: Expand acronyms to full official names
3. **Possessives**: Remove possessive forms ('s, 's)
4. **Casing**: Preserve official casing
5. **Variants**: List common aliases/variants
6. **Ambiguity**: If uncertain, set ambiguity_warning and list possible_matches
7. **Type Detection**: Classify concept type

# Batch Output Format (JSON)

{
  "concepts": [
    {
      "canonical_name": "Official name 1",
      "confidence": 0.95,
      "reasoning": "Brief explanation",
      "aliases": ["variant1", "variant2"],
      "concept_type": "Product|Acronym|...",
      "domain": "enterprise_software|...",
      "ambiguity_warning": null,
      "possible_matches": [],
      "metadata": {}
    },
    {
      "canonical_name": "Official name 2",
      ...
    }
  ]
}

CRITICAL: Return results in SAME ORDER as input concepts. The array "concepts" must have EXACTLY the same number of elements as the input.
"""
```

**Points Cl√©s** :
- ‚úÖ M√™mes guidelines que canonicalization individuelle
- ‚úÖ Format JSON strict pour batch processing
- ‚úÖ Emphasis sur l'ordre des r√©sultats

---

## üéØ Design Syst√®me (Clarification Utilisateur)

### Fonctionnement Attendu du Batch Processing

**Phase 1 : Check Ontology Dictionaries**
```python
# gatekeeper.py - Avant batch LLM
for concept_name in extracted_concepts:
    if concept_name in adaptive_ontology_cache:
        canonical_name = adaptive_ontology_cache[concept_name]
        # Pas d'appel LLM n√©cessaire
    else:
        # Ajouter au batch pour LLM call
        batch_for_llm.append(concept_name)
```

**Phase 2 : Batch LLM Call**
```python
# Batch processing : 20 concepts par call
# Exemple : 100 concepts ‚Üí 5 batch calls (au lieu de 100 individual calls)
batches = chunk_list(batch_for_llm, batch_size=20)
for batch in batches:
    results = llm_canonicalizer.canonicalize_batch(batch)
    # 1 appel LLM traite 20 concepts
```

**Phase 3 : Store Results in Ontology**
```python
# Stocker r√©sultats dans Redis pour futurs imports
for concept_name, canonical_name in results:
    adaptive_ontology_manager.store(concept_name, canonical_name)
```

### √âvolution Progressive

**Premier Import (Syst√®me Vierge)** :
- Ontology cache vide ‚Üí 100% concepts envoy√©s au LLM
- 547 concepts ‚Üí 28 batch calls (20 concepts/batch)
- Temps : ~56 secondes (28 √ó 2s)
- Co√ªt : ~$0.084 (28 √ó $0.003)

**Deuxi√®me Import (Ontology Partielle)** :
- Ontology cache contient 300 concepts connus
- 547 concepts - 300 cached = 247 concepts ‚Üí 13 batch calls
- Temps : ~26 secondes (13 √ó 2s)
- Co√ªt : ~$0.039 (13 √ó $0.003)

**Apr√®s 5-10 Imports (Ontology Mature)** :
- Ontology cache contient 80% concepts courants
- 547 concepts - 437 cached = 110 concepts ‚Üí 6 batch calls
- Temps : ~12 secondes (6 √ó 2s)
- Co√ªt : ~$0.018 (6 √ó $0.003)

**Objectif** : R√©duire progressivement les appels LLM via apprentissage ontologique.

---

## üìä M√©triques Avant/Apr√®s

| M√©trique | Avant Fix | Apr√®s Fix (Attendu) |
|----------|-----------|---------------------|
| **M√©thode existe** | ‚ùå Non (AttributeError) | ‚úÖ Oui (216 lignes) |
| **Batch calls** | 0 (erreur) | 28 (20 concepts/batch) |
| **Temps canonicalization** | 18 min (547 individual) | < 1 min (28 batch) |
| **Concepts canonical_name=None** | 100 (18%) | 0 (0%) |
| **Co√ªt LLM** | $0.82 (547 calls) | $0.084 (28 calls) |
| **JSON parsing success** | 0% (erreur) | 100% (attendu) |
| **Diagnostic logging** | ‚ùå Non | ‚úÖ Oui (raw response) |
| **Fallback robustesse** | ‚ùå Non | ‚úÖ Per-concept + global |

---

## üß™ Instructions Test Utilisateur

### Pr√©requis
- ‚úÖ Worker rebuilded avec commit 7efaa59
- ‚úÖ Worker red√©marr√© (`docker-compose restart ingestion-worker`)
- ‚úÖ Monitoring logs en cours (background bash 7d215c)

### √âtapes Test

1. **Aller sur l'interface d'import** :
   ```
   http://localhost:3000/documents/import
   ```

2. **Uploader un document** (PPTX ou PDF) :
   - Exemple : RISE_with_SAP_Cloud_ERP_Private.pptx
   - Ou tout autre document de test

3. **Observer les logs en temps r√©el** :
   Les logs suivants devraient appara√Ætre dans le terminal de monitoring :

   **Logs Batch Processing** :
   ```
   [GATEKEEPER:Batch] üîÑ Batch canonicalizing 547 concepts (batch_size=20)...
   [LLMCanonicalizer:Batch] Canonicalizing batch of 20 concepts
   [LLMCanonicalizer:Batch] üîç RAW LLM response (first 1000 chars):
   {
     "concepts": [
       {"canonical_name": "Content Owner", "confidence": 0.95, ...},
       ...
     ]
   }
   [LLMCanonicalizer:Batch] ‚úÖ Batch completed: 20 concepts canonicalized
   ```

   **Logs Attendus (Succ√®s)** :
   - `[GATEKEEPER:Batch]` : D√©marrage batch processing
   - `[LLMCanonicalizer:Batch]` : Logs de la nouvelle m√©thode
   - `üîç RAW LLM response` : Diagnostic logging du JSON retourn√©
   - `‚úÖ Batch completed: X concepts` : Confirmation succ√®s
   - **PAS de** `canonical_name.*None` warnings
   - **PAS de** `AttributeError` errors

4. **V√©rifier r√©sultats Neo4j** :
   ```bash
   docker exec knowbase-neo4j cypher-shell -u neo4j -p graphiti_neo4j_pass --format plain \
     "MATCH (c:CanonicalConcept) WHERE c.tenant_id = 'default' RETURN c.canonical_name, c.surface_form LIMIT 20"
   ```

   **Attendu** :
   - 0 concepts avec `canonical_name = null`
   - Tous les concepts ont un `canonical_name` valide

5. **V√©rifier m√©triques de performance** :
   - Temps total canonicalization : < 1 min (au lieu de 18 min)
   - Nombre de batch calls : ~28 (affich√©s dans logs)
   - 0 warnings "canonical_name is None"

### Logs en Temps R√©el

Un monitoring est actif en arri√®re-plan (bash 7d215c) qui filtre les logs pertinents :
- `[LLMCanonicalizer:Batch]`
- `[GATEKEEPER:Batch]`
- `Batch canonicalizing`
- `RAW LLM response`
- `canonical_name.*None`
- `AttributeError`

---

## üîç Troubleshooting

### Si AttributeError Persiste

**Sympt√¥me** :
```
AttributeError: 'LLMCanonicalizer' object has no attribute 'canonicalize_batch'
```

**Cause** : Worker pas encore rebuilded avec nouvelle version

**Solution** :
```bash
docker-compose build ingestion-worker
docker-compose restart ingestion-worker
```

### Si JSON Parsing √âchoue

**Sympt√¥me** :
```
[LLMCanonicalizer:Batch] ‚ùå Batch canonicalization failed: All JSON parsing attempts failed
```

**Diagnostic** :
1. Chercher log `üîç RAW LLM response` dans les logs
2. V√©rifier le JSON retourn√© par LLM
3. V√©rifier si le format correspond au sch√©ma attendu

**Logs √† capturer** :
```bash
docker-compose logs ingestion-worker | grep "RAW LLM response" -A 50
```

### Si Fallback Individuel Activ√©

**Sympt√¥me** :
```
[GATEKEEPER:Canonicalization:Batch] ‚ö†Ô∏è Cache MISS for 'Content Owner', fallback to individual LLM call
```

**Interpr√©tation** :
- **NORMAL** si batch LLM parsing a √©chou√© pour certains concepts
- Fallback individuel assure que tous les concepts sont canonicalis√©s
- V√©rifier pourquoi batch parsing a √©chou√© (voir section pr√©c√©dente)

---

## üìù Prochaines √âtapes

Apr√®s validation du test utilisateur :

1. **Si succ√®s** :
   - ‚úÖ Marquer Phase A.2 compl√©t√©e
   - ‚è≠Ô∏è Passer √† Phase B.5 : Fixer `surface_forms` pour Phase 2
   - üìä Documenter m√©triques r√©elles observ√©es

2. **Si √©chec batch parsing** :
   - üîç Analyser raw LLM response
   - üîß Ajuster prompt syst√®me ou parser JSON
   - ‚è≠Ô∏è Passer √† Phase A.4 : Fixer parser JSON + am√©liorer prompt

3. **Optimisations futures (Phase A.5-A.7)** :
   - Fuzzy deduplication (85% similarity)
   - Mise √† jour sch√©ma Neo4j pour stocker `aliases` (liste)
   - Mise √† jour Redis ontology pour stocker `aliases`

---

## üéØ Validation Checklist

- [ ] Worker rebuilded avec commit 7efaa59
- [ ] Worker red√©marr√© et actif
- [ ] Monitoring logs actif (bash 7d215c)
- [ ] Upload test document via http://localhost:3000
- [ ] Logs batch processing visibles
- [ ] Logs raw LLM response visibles
- [ ] 0 concepts avec `canonical_name=None`
- [ ] Temps canonicalization < 1 min
- [ ] ~28 batch calls observ√©s (pour 547 concepts)
- [ ] Concepts visibles dans Neo4j avec canonical_name valide

---

**Cr√©√© par** : Claude Code
**Pour** : Fix critique batch canonicalization
**Priorit√©** : üî¥ CRITIQUE
**Status** : ‚úÖ D√âPLOY√â - EN ATTENTE TEST UTILISATEUR
**Commit** : 7efaa59 - "feat(canonicalization): Implement missing canonicalize_batch() method with diagnostic logging"
