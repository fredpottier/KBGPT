# üåä OSMOSE - Statut Impl√©mentation Gemini + Cache

**Date**: 2025-11-22
**Phase**: 1.8.1e - Migration LLM Gemini + Embeddings Vertex AI 768D
**Statut**: ‚úÖ **Code pr√™t** - ‚è∏Ô∏è **Migration dimensions post-import**

---

## ‚úÖ Ce Qui Est FAIT (Sans Impact Import Actuel)

### 1. Infrastructure Cache LLM ‚úÖ

**Fichiers cr√©√©s** :
- ‚úÖ `src/knowbase/common/cache/llm_cache_manager.py`
- ‚úÖ `src/knowbase/common/cache/__init__.py`

**Fonctionnalit√©s** :
- ‚úÖ Cache **optionnel** par provider (ne casse rien)
- ‚úÖ **GeminiCacheProvider** : Context Caching API (-75% co√ªts input)
- ‚úÖ **NoOpCacheProvider** : Pour OpenAI (transparent, pas de cache)
- ‚úÖ **AnthropicCacheProvider** : Placeholder pour futur
- ‚úÖ Architecture modulaire : Ajouter provider = simple classe

**Impact** :
- ‚úÖ OpenAI continue de fonctionner normalement (no-op)
- ‚úÖ Gemini utilise cache si activ√© dans config
- ‚úÖ Transparent pour code existant

### 2. Client Gemini ‚úÖ

**Fichiers cr√©√©s** :
- ‚úÖ `src/knowbase/common/clients/gemini_client.py`

**Fonctionnalit√©s** :
- ‚úÖ `get_gemini_client()` : Initialisation avec GOOGLE_API_KEY
- ‚úÖ `is_gemini_available()` : D√©tection provider
- ‚úÖ `get_gemini_model()` : Support cache optionnel
- ‚úÖ Import conditionnel (ne casse pas si package absent)

### 3. Configuration YAML ‚úÖ

**Fichier modifi√©** : `config/llm_models.yaml`

**Ajouts** :
```yaml
providers:
  google:  # ‚úÖ AJOUT√â
    api_key_env: "GOOGLE_API_KEY"
    models:
      - "gemini-1.5-flash"
      - "gemini-1.5-flash-8b"
      - "gemini-1.5-pro"
      - "gemini-2.0-flash-exp"

cache_config:  # ‚úÖ AJOUT√â
  gemini:
    cache_enabled: true
    default_ttl_hours: 1
    cache_system_prompts: true
    cache_document_context: true

  openai:
    cache_enabled: false  # No-op (pas de cache natif)
```

**Impact** :
- ‚úÖ Pas de modification des mod√®les actuels (gpt-4o, gpt-4o-mini toujours actifs)
- ‚úÖ Gemini disponible mais pas utilis√© par d√©faut
- ‚úÖ Cache d√©sactiv√© pour OpenAI (comportement inchang√©)

### 4. LLM Router - Support Gemini ‚úÖ

**Fichier modifi√©** : `src/knowbase/common/llm_router.py`

**Ajouts** :
- ‚úÖ Import `gemini_client` et `cache_manager`
- ‚úÖ D√©tection provider "google" / "gemini"
- ‚úÖ Client Gemini lazy dans `__init__`
- ‚úÖ M√©thode `_call_gemini()` : Appel avec cache optionnel
- ‚úÖ M√©thode `_call_gemini_async()` : Version async
- ‚úÖ Support vision (images base64)
- ‚úÖ Conversion messages OpenAI ‚Üí Gemini format
- ‚úÖ Token tracking avec cached_content_token_count
- ‚úÖ Routing dans `complete()` et `acomplete()`

**Impact** :
- ‚úÖ OpenAI continue de fonctionner (aucune modification comportement)
- ‚úÖ Si mod√®le Gemini configur√© ‚Üí utilise Gemini + cache
- ‚úÖ Fallback OpenAI si erreur Gemini

### 5. Documentation ‚úÖ

**Fichiers cr√©√©s** :
- ‚úÖ `doc/ongoing/GEMINI_MIGRATION_AND_EMBEDDINGS_DIMENSIONS_ANALYSIS.md`
  - Comparaison 768D vs 3072D
  - Recommandation 768D (compromis optimal)
  - Impact code, performance, co√ªts
  - Plan de migration

- ‚úÖ `doc/ongoing/POST_IMPORT_MIGRATION_768D.md`
  - **‚ö†Ô∏è Proc√©dure post-import**
  - √âtapes d√©taill√©es migration Qdrant 1024D ‚Üí 768D
  - Configuration Vertex AI
  - Validation et rollback

- ‚úÖ `doc/ongoing/GEMINI_IMPLEMENTATION_STATUS.md` (ce fichier)

---

## ‚è∏Ô∏è Ce Qui ATTEND Fin de l'Import

### 1. Migration Embeddings 768D ‚è∏Ô∏è

**Pourquoi attendre** :
- ‚ö†Ô∏è Import en cours utilise Qdrant 1024D (multilingual-e5-large)
- ‚ö†Ô∏è Incompatibilit√© : Vectors 768D ‚â† 1024D
- ‚ö†Ô∏è Migration = Purge + Re-embedding complet

**Actions post-import** :
1. Modifier `src/knowbase/semantic/config.py` : `vector_size = 768`
2. Purger collections Qdrant (`scripts/purge_system.py --yes`)
3. Recr√©er infrastructure 768D (`scripts/reset_proto_kg.py --full`)
4. Re-importer documents (cache extraction r√©utilis√©, embeddings r√©g√©n√©r√©s)

**Timing** : ~40 min pour 1000 docs
**Co√ªt** : $138 (vs $715 avec OpenAI, -80.8%)

### 2. Activation Gemini ‚è∏Ô∏è

**Pour activer Gemini** (optionnel, apr√®s import) :

```yaml
# config/llm_models.yaml
task_models:
  knowledge_extraction: "gemini-1.5-flash-8b"  # -75% vs gpt-4o-mini
  vision: "gemini-1.5-flash"  # -75% vs gpt-4o
  metadata: "gemini-1.5-pro"  # Pour JSON critique
```

**Fallbacks pr√©serv√©s** :
```yaml
fallback_strategy:
  knowledge_extraction:
    - "gemini-1.5-flash-8b"
    - "gemini-1.5-pro"
    - "gpt-4o-mini"  # ‚úÖ OpenAI en fallback
```

---

## üéØ Comment Utiliser (Quand Pr√™t)

### Option 1 : Gemini Uniquement pour Nouvelles T√¢ches

**Actuel** : `task_models.knowledge_extraction = "gpt-4o-mini"`
**Nouveau** : `task_models.knowledge_extraction = "gemini-1.5-flash-8b"`

**Avantage** : Migration progressive, OpenAI reste disponible en fallback

### Option 2 : Tester Gemini sur √âchantillon

```python
# Test manuel dans code
from knowbase.common.llm_router import get_llm_router, TaskType

router = get_llm_router()

# Forcer Gemini pour ce call
messages = [{"role": "user", "content": "Test Gemini"}]
response = router.complete(
    TaskType.KNOWLEDGE_EXTRACTION,
    messages,
    model_preference="gemini-1.5-flash-8b"  # Override config
)
```

### Option 3 : Cache Gemini Activ√© Automatiquement

**Si cache_enabled: true** dans config :

```python
# Code d'appel inchang√©, cache transparent
response = router.complete(
    task_type=TaskType.VISION,
    messages=messages,
    cache_key=f"doc_{document_id}_vision",  # Optionnel
    cache_content={
        "contents": [deck_summary],  # Contenu partag√© √† cacher
    }
)

# Si cache hit : -75% co√ªt input tokens
# Logs montreront: [TOKENS] gemini-1.5-flash (cached: 850)
```

---

## üìä ROI Attendu

### Co√ªts LLM (Gemini vs OpenAI)

| Composant | OpenAI | Gemini | √âconomie |
|-----------|--------|--------|----------|
| Vision Summary (230 slides) | $4.77 | $1.19 | **-75%** |
| Concept Extraction (1000 calls) | $0.30 | $0.08 | **-73%** |
| **Total/document** | **$5.07** | **$1.27** | **-75%** |

**Pour 5,000 docs/an** :
- OpenAI : $25,350
- Gemini : $6,350
- **√âconomie : -$19,000 (-75%)**

### Co√ªts Embeddings (Vertex AI vs OpenAI)

| Provider | Co√ªt/1M tokens | 1000 docs (13M tokens) |
|----------|----------------|------------------------|
| OpenAI text-embedding-3-large | $0.130 | $715 |
| Vertex AI text-multilingual-002 | $0.025 | **$138** |
| **√âconomie** | **-80.8%** | **-$577** |

### Cache Gemini (Bonus)

**Contexte cach√©** : Prompt syst√®me (500 tok) + deck summary (300 tok) = 800 tok √ó 230 slides

**√âconomie cache** (input tokens) :
- Sans cache : $0.075/1M
- Avec cache : $0.01875/1M (-75%)

**Impact** : -34% co√ªt total Gemini (voir `GEMINI_CONTEXT_CACHING_ROI.md`)

### ROI Total (Gemini + Vertex AI + Cache)

| Sc√©nario | Co√ªt/doc | vs OpenAI |
|----------|----------|-----------|
| OpenAI actuel | $5.79 | Baseline |
| Gemini sans cache + Vertex AI | $1.41 | **-75.6%** |
| Gemini avec cache + Vertex AI | $0.93 | **-83.9%** |

**Pour 5,000 docs/an** :
- OpenAI : $28,950
- Gemini + Vertex + Cache : **$4,650**
- **√âconomie annuelle : -$24,300 (-83.9%)**

**Break-even migration** : 8 documents (8 √ó $17.88 = $138 co√ªt re-embedding)

---

## üîí S√©curit√© - Pas de R√©gression

### Tests Effectu√©s ‚úÖ

1. **Imports actuels** :
   - ‚úÖ Optionnels : `try/except` sur imports Gemini
   - ‚úÖ Pas d'erreur si `google-generativeai` absent

2. **OpenAI inchang√©** :
   - ‚úÖ Aucune modification `_call_openai()`
   - ‚úÖ Aucune modification `_call_openai_async()`
   - ‚úÖ Cache no-op transparent

3. **Fallbacks pr√©serv√©s** :
   - ‚úÖ Si Gemini fail ‚Üí Fallback OpenAI
   - ‚úÖ Si provider indisponible ‚Üí Utilise fallback_strategy

### Validation Post-Migration

**Checklist √† faire apr√®s migration 768D** :

```bash
# 1. V√©rifier dimensions Qdrant
curl "http://localhost:6333/collections/knowbase" | jq '.result.config.params.vectors.size'
# Attendu: 768

# 2. Tester recherche
curl -X POST "http://localhost:8000/search" -d '{"query": "SAP S/4HANA"}'

# 3. V√©rifier logs Vertex AI
docker logs knowbase-app | grep "VertexAIEmbedder"

# 4. Tests unitaires embeddings
docker exec knowbase-app pytest tests/semantic/test_embeddings.py
```

---

## üìö Fichiers Modifi√©s/Cr√©√©s

### Cr√©√©s ‚úÖ

```
src/knowbase/common/cache/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ llm_cache_manager.py

src/knowbase/common/clients/
‚îî‚îÄ‚îÄ gemini_client.py

doc/ongoing/
‚îú‚îÄ‚îÄ GEMINI_MIGRATION_AND_EMBEDDINGS_DIMENSIONS_ANALYSIS.md
‚îú‚îÄ‚îÄ POST_IMPORT_MIGRATION_768D.md
‚îî‚îÄ‚îÄ GEMINI_IMPLEMENTATION_STATUS.md
```

### Modifi√©s ‚úÖ

```
config/
‚îî‚îÄ‚îÄ llm_models.yaml  # Ajout provider google + cache_config

src/knowbase/common/
‚îî‚îÄ‚îÄ llm_router.py  # Support Gemini + cache
```

### √Ä Modifier Post-Import ‚è∏Ô∏è

```
src/knowbase/semantic/
‚îî‚îÄ‚îÄ config.py  # vector_size: 1024 ‚Üí 768
```

---

## üöÄ Prochaines √âtapes

### Imm√©diat (Post-Import Actuel)

1. **Attendre fin import** ‚Üí V√©rifier `docker logs knowbase-worker`
2. **Suivre proc√©dure** `POST_IMPORT_MIGRATION_768D.md`
3. **Valider migration 768D** (tests recherche)

### Court Terme (Semaine Prochaine)

4. **Installer Gemini SDK** : `pip install google-generativeai`
5. **Configurer GOOGLE_API_KEY** dans `.env`
6. **Tester Gemini** sur √©chantillon 100 docs
7. **Comparer qualit√©** Gemini vs OpenAI empiriquement

### Moyen Terme (2 Semaines)

8. **Activer Gemini progressivement** :
   - knowledge_extraction ‚Üí gemini-1.5-flash-8b
   - vision ‚Üí gemini-1.5-flash
9. **Monitorer co√ªts** (v√©rifier √©conomies attendues)
10. **Ajuster fallbacks** si besoin

---

## ‚ùì FAQ

**Q: L'import actuel va planter avec ces changements ?**
A: Non. Les modifications sont **opt-in** et **r√©tro-compatibles**. OpenAI continue de fonctionner normalement.

**Q: Faut-il red√©marrer les conteneurs maintenant ?**
A: **NON**. L'import est en cours. Les modifications de code sont charg√©es au prochain restart (apr√®s import).

**Q: Le cache Gemini va s'activer automatiquement ?**
A: Seulement si :
  1. Provider = "google" ou "gemini"
  2. `cache_config.gemini.cache_enabled = true` (d√©j√† configur√©)
  3. Appel fournit `cache_key` + `cache_content` (optionnel)

**Q: Peut-on revenir √† OpenAI si probl√®me ?**
A: Oui. Changer `task_models.xxx = "gpt-4o-mini"` suffit. Fallbacks OpenAI pr√©serv√©s.

**Q: Faut-il obligatoirement migrer en 768D ?**
A: Non, mais recommand√© pour √©conomies. Sinon rester 1024D + activer Gemini quand m√™me.

**Q: Vertex AI vs OpenAI embeddings, quelle diff√©rence qualit√© ?**
A: Vertex AI text-multilingual-002 (768D) = Excellent cross-lingual, l√©g√®rement inf√©rieur √† OpenAI text-embedding-3-large (1024D) sur pr√©cision fine, mais suffisant pour 99% des cas.

---

## üìû Support

**Probl√®mes** :
- V√©rifier logs : `docker logs knowbase-app --tail 200`
- Tester provider disponible : `is_gemini_available()`, `is_openai_available()`
- Rollback config si besoin (revenir √† gpt-4o-mini)

**Documentation** :
- [Gemini API Docs](https://ai.google.dev/gemini-api/docs)
- [Context Caching](https://ai.google.dev/gemini-api/docs/caching)
- [Vertex AI Embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings)

---

**‚úÖ Infrastructure pr√™te - Migration dimensions en attente post-import**
