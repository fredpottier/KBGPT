# üåä OSMOSE - Migration Gemini + Analyse Dimensions Embeddings

**Date**: 2025-11-22
**Objectif**: Migrer vers Gemini pour LLM et Vertex AI pour embeddings
**Question cl√©**: 768D ou 3072D pour les embeddings ?

---

## üìã Table des Mati√®res

1. [Migration Gemini (LLM)](#1-migration-gemini-llm)
2. [Analyse Dimensions Embeddings (768D vs 3072D)](#2-analyse-dimensions-embeddings-768d-vs-3072d)
3. [Impact sur le Code Existant](#3-impact-sur-le-code-existant)
4. [Recommandations Finales](#4-recommandations-finales)
5. [Plan de Migration](#5-plan-de-migration)

---

## 1. Migration Gemini (LLM)

### 1.1 Configuration Propos√©e

```yaml
# config/llm_models.yaml
providers:
  google:
    api_key_env: "GOOGLE_API_KEY"
    base_url: null
    models:
      - "gemini-1.5-flash"
      - "gemini-1.5-flash-8b"
      - "gemini-1.5-pro"
      - "gemini-2.0-flash-exp"

task_models:
  # Extraction de concepts structur√©s
  knowledge_extraction: "gemini-1.5-flash-8b"

  # Vision: r√©sum√© riche et narratif
  vision: "gemini-1.5-flash"

  # Extraction structur√©e (concepts, facts, entities, relations)
  metadata: "gemini-1.5-pro"

fallback_strategy:
  knowledge_extraction:
    - "gemini-1.5-flash-8b"
    - "gemini-1.5-pro"
    - "gpt-4o-mini"  # Fallback OpenAI si Gemini down

  vision:
    - "gemini-1.5-flash"
    - "gemini-1.5-pro"
    - "gpt-4o"  # Fallback OpenAI pour vision critique

  metadata:
    - "gemini-1.5-pro"
    - "gemini-2.0-flash-exp"  # Exp√©rimental GRATUIT
    - "gpt-4o"  # Fallback si JSON non r√©parable
```

### 1.2 Modifications Code Requises

**A. Cr√©er nouveau client Gemini** (`src/knowbase/common/clients/gemini_client.py`)

```python
"""Client Google Gemini pour appels LLM."""
import os
import logging
import google.generativeai as genai
from typing import Optional

logger = logging.getLogger(__name__)

def get_gemini_client():
    """Initialise le client Google Gemini."""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY not found in environment")

    genai.configure(api_key=api_key)
    logger.info("[GEMINI] Client configured")
    return genai

def is_gemini_available() -> bool:
    """V√©rifie si Gemini est disponible."""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        return api_key is not None and len(api_key) > 0
    except Exception:
        return False
```

**B. Ajouter provider dans `llm_router.py`**

Ajouter m√©thodes :
- `_call_gemini()` : Appel synchrone Gemini
- `_call_gemini_async()` : Appel async Gemini
- D√©tection provider pour mod√®les `gemini-*`

**Format conversion requis** :
- OpenAI messages ‚Üí Gemini parts format
- Support vision (base64 images)
- Gestion `response_format` pour JSON structur√©

**C. Token tracking**

Gemini fournit `usage_metadata` :
```python
response.usage_metadata.prompt_token_count
response.usage_metadata.candidates_token_count
```

### 1.3 Avantages Migration Gemini

‚úÖ **Co√ªts r√©duits de 75%** (voir `COST_ANALYSIS_OPENAI_VS_GEMINI.md`)
‚úÖ **Context caching** : -75% suppl√©mentaire sur input tokens
‚úÖ **Gemini 2.0 Flash Exp** : GRATUIT pendant preview
‚úÖ **Qualit√© √©quivalente** selon benchmarks Google
‚úÖ **R√©silience multi-provider** : Gemini + OpenAI fallback

---

## 2. Analyse Dimensions Embeddings (768D vs 3072D)

### 2.1 Mod√®les Vertex AI Disponibles

| Mod√®le | Dimensions | Langues | Use Case |
|--------|-----------|---------|----------|
| **gemini-embedding-001** | **3072** | Multilingue + Code | Recommand√© Google (performances pointe) |
| text-embedding-005 | 768 | Anglais + Code | Sp√©cialis√© anglais |
| text-multilingual-embedding-002 | 768 | Multilingue | Optimis√© multilingue |

**Note**: Les anciens mod√®les `text-embedding-gecko@002/003` ont √©t√© remplac√©s par les mod√®les ci-dessus.

**Recommandation Google**: `gemini-embedding-001` (3072D) unifie les mod√®les pr√©c√©dents et offre les meilleures performances.

### 2.2 Comparaison 768D vs 3072D

#### **Stockage Qdrant**

**Calcul pour 1 document (230 slides, ~13,763 chunks)**

| Dimension | Taille/Vecteur | Taille Totale (13,763 chunks) | Facteur |
|-----------|----------------|------------------------------|---------|
| **768D** | 3,072 bytes (768 √ó 4) | **42.3 MB** | 1√ó |
| **1024D (actuel)** | 4,096 bytes (1024 √ó 4) | **56.4 MB** | 1.33√ó |
| **3072D** | 12,288 bytes (3072 √ó 4) | **169.1 MB** | **4√ó** |

**Pour 1,000 documents** :
- 768D : **42.3 GB**
- 1024D : **56.4 GB** (actuel)
- 3072D : **169.1 GB** (‚ö†Ô∏è +200% vs actuel)

**Impact stockage** :
- ‚úÖ 768D : R√©duit stockage de -25% vs actuel
- ‚ö†Ô∏è 3072D : Augmente stockage de +200% vs actuel

#### **Performance Recherche**

**Distance cosine avec HNSW** :

| Dimension | Calculs/recherche | Latence estim√©e | vs Actuel |
|-----------|------------------|----------------|-----------|
| 768D | 768 multiplications | ~80ms | **-20%** ‚úÖ |
| 1024D | 1024 multiplications | ~100ms | Baseline |
| 3072D | 3072 multiplications | ~150ms | **+50%** ‚ö†Ô∏è |

**Note** : Avec index HNSW bien configur√©, l'impact latence est partiellement att√©nu√©.

**Impact performance** :
- ‚úÖ 768D : Recherches l√©g√®rement plus rapides
- ‚ö†Ô∏è 3072D : Recherches plus lentes (+50%)

#### **Qualit√© S√©mantique**

**Dimensions ‚â† Qualit√© automatique**

üìä **√âtudes acad√©miques** ([MTEB benchmarks](https://huggingface.co/spaces/mteb/leaderboard)) :
- Dimensions √©lev√©es (3072) peuvent am√©liorer la **pr√©cision fine** (nuances s√©mantiques)
- **MAIS** : Qualit√© d√©pend surtout du **mod√®le pr√©-entra√Æn√©**, pas juste des dimensions
- 768D bien entra√Æn√© > 3072D mal entra√Æn√©

**Comparaison mod√®les Vertex AI** (selon Google) :
- `gemini-embedding-001` (3072D) : Performances pointe (‚≠ê recommand√©)
- `text-multilingual-embedding-002` (768D) : Optimis√© multilingue (bon compromis)

**Impact qualit√©** :
- ‚úÖ 3072D (`gemini-embedding-001`) : Meilleure pr√©cision th√©orique
- ‚úÖ 768D (`text-multilingual-embedding-002`) : Bon compromis qualit√©/co√ªt

**USP OSMOSE** (cross-lingual) :
- Les 2 mod√®les supportent multilingue
- `gemini-embedding-001` (3072D) : L√©g√®rement meilleur sur nuances
- `text-multilingual-embedding-002` (768D) : D√©j√† excellent pour cross-lingual

#### **Co√ªts Vertex AI**

**Tarifs Vertex AI Embeddings** ([pricing](https://cloud.google.com/vertex-ai/pricing)) :

| Service | Tarif | vs OpenAI |
|---------|-------|-----------|
| Vertex AI Text Embeddings | $0.025 / 1M tokens | **-80.8%** ‚úÖ |
| OpenAI text-embedding-3-large | $0.130 / 1M tokens | Baseline |

**Co√ªt identique** entre 768D et 3072D sur Vertex AI (tarif au token, pas √† la dimension).

**Impact co√ªts** :
- ‚úÖ 768D : **-80.8%** vs OpenAI actuel
- ‚úÖ 3072D : **-80.8%** vs OpenAI actuel (m√™me tarif)

#### **Complexit√© Migration**

**768D** :
- ‚ö†Ô∏è Changement dimension : N√©cessite re-embedding de TOUT le corpus existant
- ‚ö†Ô∏è Incompatibilit√© : Vectors 768D ‚â† 1024D actuels ‚Üí Pas de recherche mixte
- ‚úÖ Migration plus l√©g√®re (moins de stockage)

**3072D** :
- ‚ö†Ô∏è Changement dimension : Re-embedding total requis aussi
- ‚ö†Ô∏è Impact stockage : +200% (besoin de v√©rifier capacit√© infrastructure)
- ‚ö†Ô∏è Impact performance : +50% latence recherche

**Conclusion** : Les deux n√©cessitent migration compl√®te (re-embedding).

---

## 3. Impact sur le Code Existant

### 3.1 Fichiers √† Modifier (Dimensions Embeddings)

**A. Configuration centrale** :

```python
# src/knowbase/semantic/config.py (ligne 139)
class QdrantProtoConfig(BaseModel):
    vector_size: int = 768  # OU 3072 selon choix
```

**B. Embedder wrapper** :

```python
# src/knowbase/semantic/utils/embeddings.py
def get_embedder(config: SemanticConfig):
    """Retourne l'embedder configur√© (Vertex AI)."""
    # Impl√©menter VertexAIEmbedder
    return VertexAIEmbedder(
        model="gemini-embedding-001",  # ou text-multilingual-embedding-002
        dimensions=config.qdrant.vector_size
    )
```

**C. Cloud embeddings** :

```python
# src/knowbase/semantic/utils/cloud_embeddings.py
class VertexAIEmbedder:
    """Embeddings via Vertex AI (Google Cloud)."""

    def __init__(self, model: str, dimensions: int):
        import vertexai
        from vertexai.language_models import TextEmbeddingModel

        # Init Vertex AI
        vertexai.init(project=os.getenv("GCP_PROJECT_ID"))

        self.model = TextEmbeddingModel.from_pretrained(model)
        self.dimensions = dimensions

    def encode(self, texts: List[str]) -> np.ndarray:
        """Encode texts via Vertex AI."""
        embeddings = self.model.get_embeddings(
            texts,
            output_dimensionality=self.dimensions  # Optionnel si r√©duction dims
        )

        return np.array([e.values for e in embeddings], dtype=np.float32)
```

**D. Cr√©ation collections Qdrant** :

Tous les fichiers qui appellent `ensure_qdrant_collection()` ou `create_collection()` vont utiliser automatiquement `config.qdrant.vector_size`.

**Fichiers affect√©s** :
- `src/knowbase/semantic/setup_infrastructure.py` (ligne 212-214)
- `src/knowbase/common/clients/qdrant_client.py` (ligne 47)
- `src/knowbase/common/clients/shared_clients.py` (ligne 95)
- `scripts/reset_proto_kg.py`

**Impact** : Changement transparent si on met √† jour `config.qdrant.vector_size`.

### 3.2 Migration du Corpus Existant

**Proc√©dure compl√®te** :

1. **Purge collections Qdrant** (vectors 1024D incompatibles)
```bash
curl -X DELETE "http://localhost:6333/collections/knowbase"
curl -X DELETE "http://localhost:6333/collections/concepts_proto"
```

2. **Recr√©er collections avec nouvelles dimensions**
```bash
docker exec knowbase-app python scripts/reset_proto_kg.py --full
```
‚Üí Utilise automatiquement `config.qdrant.vector_size` (768 ou 3072)

3. **Re-importer tous les documents**
- Les fichiers `.knowcache.json` contiennent extraction text/concepts
- R√©utilisables pour √©viter appels LLM co√ªteux
- Seuls les embeddings seront r√©g√©n√©r√©s

**Temps estim√©** (1000 documents, 13M chunks) :
- Avec Vertex AI batch : ~10-15 min (vs 15h local)
- Co√ªt : $325 (13M tokens √ó $0.025/1M)

### 3.3 Compatibilit√© Descendante

**Collections existantes** :
- ‚ö†Ô∏è **Incompatible** : Qdrant ne permet PAS de rechercher vectors 768D dans collection 1024D
- ‚ùå Migration = Purge + Re-embedding complet

**Strat√©gie Zero-Downtime** (si critique) :
1. Cr√©er nouvelles collections `knowbase_v2`, `concepts_proto_v2` (nouvelles dims)
2. Re-importer en parall√®le (collections coexistent)
3. Basculer l'API vers v2 quand pr√™t
4. Supprimer anciennes collections

**Dur√©e** : +2-3h pour setup dual, mais service disponible pendant migration.

---

## 4. Recommandations Finales

### üéØ Recommandation #1 : **768D** (`text-multilingual-embedding-002`)

**Pourquoi** :
‚úÖ **Compromis optimal** qualit√©/performance/co√ªt
‚úÖ **Stockage r√©duit** : -25% vs actuel (42 GB vs 56 GB pour 1000 docs)
‚úÖ **Performance** : -20% latence recherche
‚úÖ **Qualit√© cross-lingual** : Excellent (optimis√© multilingue)
‚úÖ **Co√ªts Vertex AI** : -80.8% vs OpenAI
‚úÖ **Infrastructure** : Pas de pression sur stockage/RAM

**Inconv√©nients** :
‚ö†Ô∏è L√©g√®rement moins pr√©cis que 3072D sur nuances fines (marginal)

### üî¨ Option #2 : **3072D** (`gemini-embedding-001`)

**Pourquoi** :
‚úÖ **Qualit√© maximale** : Performances pointe selon Google
‚úÖ **Future-proof** : Mod√®le flagship de Google
‚úÖ **Co√ªts Vertex AI** : Identiques √† 768D (-80.8% vs OpenAI)

**Inconv√©nients** :
‚ö†Ô∏è **Stockage +200%** : 169 GB pour 1000 docs (vs 56 GB actuel)
‚ö†Ô∏è **Performance -50%** : Latence recherche augment√©e
‚ö†Ô∏è **Infrastructure** : Besoin de v√©rifier capacit√© RAM/disque Qdrant

**Quand choisir 3072D** :
- Cas d'usage ultra-pr√©cis (recherche de brevets, analyse juridique fine)
- Infrastructure scalable (K8s avec auto-scaling)
- Corpus < 10,000 documents (impact stockage g√©rable)

### üöÄ Recommandation Finale

**Phase 1.8** : **768D** (`text-multilingual-embedding-002`)

**Raison** :
1. **USP OSMOSE** = Cross-lingual intelligence ‚Üí 768D d√©j√† excellent
2. **Performance** : Recherches plus rapides critiques pour UX
3. **Co√ªts** : -80.8% vs OpenAI suffit largement
4. **Infrastructure** : Pas de pression sur ressources
5. **Scalabilit√©** : 10,000 docs = 420 GB (g√©rable), vs 1.69 TB avec 3072D

**Si besoin futur de 3072D** :
- Re-migration facile (m√™me process)
- Tester sur corpus √©chantillon d'abord (100 docs)
- Comparer qualit√© recherche 768D vs 3072D empiriquement

---

## 5. Plan de Migration

### 5.1 √âtape 1 : Support Gemini (LLM)

**Dur√©e** : 1-2h

```bash
# 1. Cr√©er client Gemini
touch src/knowbase/common/clients/gemini_client.py

# 2. Modifier llm_router.py
# - Ajouter _call_gemini()
# - Ajouter _call_gemini_async()
# - Ajouter d√©tection provider "google"

# 3. Mettre √† jour config/llm_models.yaml
# - Ajouter section providers.google
# - Configurer task_models avec Gemini
# - Ajouter fallback_strategy

# 4. Tester
docker exec knowbase-app pytest tests/common/test_llm_router.py -k gemini
```

### 5.2 √âtape 2 : Migration Embeddings 768D

**Dur√©e** : 2-3h (dont 15 min embeddings batch)

```bash
# 1. Installer SDK Vertex AI
pip install google-cloud-aiplatform

# 2. Configurer GCP credentials
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account-key.json"
export GCP_PROJECT_ID="your-project-id"

# 3. Cr√©er VertexAIEmbedder
# Impl√©menter dans src/knowbase/semantic/utils/cloud_embeddings.py

# 4. Modifier config
# src/knowbase/semantic/config.py : vector_size = 768

# 5. Purge + recr√©ation infrastructure
docker exec knowbase-app python scripts/purge_system.py --yes
docker exec knowbase-app python scripts/reset_proto_kg.py --full

# 6. Re-import corpus
# Utilise cache extraction (.knowcache.json)
# Seuls embeddings r√©g√©n√©r√©s
```

### 5.3 √âtape 3 : Validation

**Tests critiques** :

```bash
# 1. V√©rifier dimensions Qdrant
curl "http://localhost:6333/collections/knowbase" | jq '.result.config.params.vectors.size'
# Attendu: 768

# 2. Tester recherche s√©mantique
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "SAP S/4HANA Cloud authentication", "top_k": 5}'

# 3. Comparer qualit√© vs baseline OpenAI
# Mesurer recall@5, recall@10 sur √©chantillon test

# 4. Mesurer performance
# Latence recherche moyenne < 100ms
```

### 5.4 Rollback Plan

**Si probl√®me d√©tect√©** :

```bash
# 1. Revenir config 1024D
# src/knowbase/semantic/config.py : vector_size = 1024

# 2. Purge + recr√©ation
docker exec knowbase-app python scripts/purge_system.py --yes
docker exec knowbase-app python scripts/reset_proto_kg.py --full

# 3. Re-import avec OpenAI embeddings
# Modifier cloud_embeddings.py pour r√©utiliser OpenAI

# Temps: ~1h
```

---

## 6. Checklist de D√©cision

### ‚úÖ Migration Gemini (LLM) : **OUI**

- [x] √âconomie 75% co√ªts LLM
- [x] Context caching (-75% suppl√©mentaire)
- [x] Gemini 2.0 Flash Exp GRATUIT
- [x] Fallback OpenAI pr√©serv√© (r√©silience)
- [x] Modification code mod√©r√©e (llm_router + client)

**Verdict** : **GO** - ROI √©vident, risque faible avec fallbacks

### ‚úÖ Migration Embeddings : **OUI (768D)**

- [x] √âconomie 80.8% co√ªts embeddings
- [x] Performance +20% (latence r√©duite)
- [x] Stockage -25% (optimisation infrastructure)
- [x] Qualit√© cross-lingual excellente
- [x] Scalabilit√© 10,000 docs sans pression

**Verdict** : **GO 768D** - Compromis optimal

### ‚ö†Ô∏è Alternative 3072D : **TESTER PLUS TARD**

- [ ] Qualit√© marginalement sup√©rieure (√† valider empiriquement)
- [ ] Stockage +200% (besoin infra scalable)
- [ ] Performance -50% (acceptable si qualit√© justifie)

**Verdict** : **WAIT** - Tester sur √©chantillon 100 docs d'abord, comparer qualit√©

---

## 7. Estimations Co√ªts Migration

### 7.1 Co√ªts One-Time (Migration)

**Re-embedding corpus existant** (estimation 1,000 docs, 13M chunks) :

| Provider | Tokens | Tarif | Co√ªt |
|----------|--------|-------|------|
| OpenAI text-embedding-3-large | 5.5M | $0.130/1M | $715 |
| Vertex AI (768D ou 3072D) | 5.5M | $0.025/1M | **$138** |

**√âconomie migration** : -$577 (-80.8%)

### 7.2 Co√ªts R√©currents (Par Document)

**Sc√©nario OSMOSE Pure** (Vision + Extraction + Embeddings) :

| Composant | OpenAI | Gemini + Vertex | √âconomie |
|-----------|--------|-----------------|----------|
| Vision Summary | $22.78 | $5.70 | -$17.08 |
| Concept Extraction | $0.30 | $0.08 | -$0.22 |
| Embeddings | $0.72 | $0.14 | -$0.58 |
| **TOTAL/DOC** | **$23.80** | **$5.92** | **-$17.88 (-75.1%)** |

**Pour 5,000 docs/an** :
- OpenAI : $119,000
- Gemini + Vertex AI : **$29,600**
- **√âconomie annuelle** : **-$89,400 (-75.1%)**

### 7.3 ROI Migration

**Investissement** :
- D√©veloppement : 3-4h dev (n√©gligeable)
- Migration corpus : $138 (one-time)

**Retour** :
- √âconomie d√®s le 1er document post-migration
- Break-even : 8 documents ($138 / $17.88)
- ROI 1 an (5000 docs) : **64,700%**

**Verdict** : ROI imm√©diat et massif.

---

## üìö Ressources

- [Google Vertex AI Embeddings Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings)
- [Gemini API Pricing](https://ai.google.dev/pricing)
- [Context Caching Gemini](https://ai.google.dev/gemini-api/docs/caching)
- [MTEB Embeddings Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [Qdrant Vector Storage Optimization](https://qdrant.tech/documentation/guides/optimize/)

---

**Prochaine √©tape** : Validation choix dimensions (768D recommand√©) ‚Üí Impl√©mentation migration
