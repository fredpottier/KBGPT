# OSMOSE - Strat√©gie GPU Acceleration & Multi-Worker

**Date:** 2025-11-15
**Version:** 1.0
**Status:** ‚úÖ Solution Impl√©ment√©e
**Phase:** Phase 1 - Optimisation Performance

---

## üìã Table des Mati√®res

1. [Contexte & Probl√®me](#contexte--probl√®me)
2. [Analyse Technique](#analyse-technique)
3. [Solution Impl√©ment√©e](#solution-impl√©ment√©e)
4. [Strat√©gies Multi-Worker Production](#strat√©gies-multi-worker-production)
5. [Recommandations](#recommandations)
6. [Plan d'Action](#plan-daction)

---

## üéØ Contexte & Probl√®me

### Objectif Performance

**Cible:** R√©duire le temps d'ingestion d'un document de **85 minutes ‚Üí 15-20 minutes**

**√âquipement:**
- GPU: NVIDIA RTX 5070 Ti
- Environnement: Docker Desktop Windows + WSL2
- Configuration: 1 worker RQ pour traitement documents

### Probl√®me Rencontr√©

**Erreur CUDA Multiprocessing:**
```
RuntimeError: Cannot re-initialize CUDA in forked subprocess.
To use CUDA with multiprocessing, you must use the 'spawn' start method
```

**Manifestation:**
- Le worker RQ d√©marre correctement
- Les mod√®les d'embeddings tentent de s'initialiser sur GPU
- ‚ùå **ERREUR** lors de l'ex√©cution du job (subprocess fork√©)
- Fallback sur CPU ‚Üí performance d√©grad√©e (85 min au lieu de 15-20 min)

---

## üî¨ Analyse Technique

### Cause Racine

Le probl√®me est une **incompatibilit√© architecturale** entre RQ Worker et CUDA :

#### 1. Architecture RQ Worker

```python
# RQ Worker utilise os.fork() pour ex√©cuter les jobs
class Worker:
    def execute_job(self, job):
        pid = os.fork()  # ‚Üê Fork le processus
        if pid == 0:
            # Processus fils ex√©cute le job
            job.perform()
```

#### 2. Limitation CUDA

**CUDA ne supporte PAS fork()** :
- Lors d'un `fork()`, le processus fils h√©rite de l'√©tat m√©moire du parent
- Si le parent a initialis√© CUDA, le fils essaie de r√©utiliser cet √©tat
- ‚ùå **CUDA refuse** : les contextes GPU ne peuvent pas √™tre partag√©s via fork

#### 3. Pourquoi set_start_method('spawn') ne fonctionnait pas

**Tentative initiale** (lignes 3-9 de worker.py) :
```python
import multiprocessing
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass
```

**Pourquoi √ßa √©chouait** :
- `set_start_method()` configure le module `multiprocessing` Python
- RQ n'utilise PAS `multiprocessing`, mais `os.fork()` directement
- Le param√©trage n'avait donc aucun effet sur RQ

### S√©quence d'Erreur

```
1. Worker RQ d√©marre (processus principal)
   ‚îî‚îÄ> warm_clients() charge les mod√®les
       ‚îî‚îÄ> get_sentence_transformer() initialise SentenceTransformer
           ‚îî‚îÄ> Auto-d√©tecte CUDA et l'initialise ‚úÖ

2. Job arrive dans la queue Redis

3. RQ Worker fork() un subprocess
   ‚îî‚îÄ> Le subprocess h√©rite de l'√©tat CUDA du parent

4. Job tente d'utiliser les embeddings
   ‚îî‚îÄ> MultilingualEmbedder.__init__() d√©tecte CUDA
       ‚îî‚îÄ> Essaie d'initialiser SentenceTransformer sur CUDA
           ‚îî‚îÄ> ‚ùå RuntimeError: Cannot re-initialize CUDA in forked subprocess
```

---

## ‚úÖ Solution Impl√©ment√©e

### SimpleWorker : La Solution RQ Native

RQ fournit une classe `SimpleWorker` qui **n'utilise PAS fork()** :

**Diff√©rence cl√©** :
- `Worker` : Fork un subprocess pour chaque job (incompatible CUDA)
- `SimpleWorker` : Ex√©cute les jobs **dans le m√™me processus** (compatible CUDA)

### Modifications Apport√©es

**Fichier:** `src/knowbase/ingestion/queue/worker.py`

**Avant** :
```python
from __future__ import annotations

# CRITICAL: Force 'spawn' method BEFORE any torch/CUDA imports
import multiprocessing
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

import logging
import os
import debugpy
from rq import Worker  # ‚Üê Worker standard (utilise fork)

def warm_clients() -> None:
    """Preload shared heavy clients so all jobs reuse the same instances."""
    get_openai_client()
    get_qdrant_client()
    get_sentence_transformer()

def run_worker(*, queue_name: str | None = None, with_scheduler: bool = True) -> None:
    warm_clients()
    queue = get_queue(queue_name)

    worker = Worker(  # ‚Üê Utilise fork()
        [queue.name],
        connection=get_redis_connection(),
        job_monitoring_interval=30,
    )

    worker.work(
        with_scheduler=with_scheduler,
        logging_level=logging.INFO,
        max_jobs=max_jobs,
    )
```

**Apr√®s** :
```python
from __future__ import annotations

import logging
import os
import debugpy
from rq import SimpleWorker  # ‚Üê SimpleWorker (pas de fork)

def warm_clients() -> None:
    """Preload shared heavy clients so all jobs reuse the same instances.

    Using SimpleWorker (no fork), we can safely warm all clients including GPU models.
    """
    get_openai_client()
    get_qdrant_client()
    get_sentence_transformer()  # ‚úÖ Safe avec SimpleWorker

def run_worker(*, queue_name: str | None = None, with_scheduler: bool = True) -> None:
    warm_clients()
    queue = get_queue(queue_name)

    # IMPORTANT: Use SimpleWorker instead of Worker to avoid fork() with CUDA
    # SimpleWorker runs jobs in the same process (no fork), making it safe for GPU
    worker = SimpleWorker(  # ‚Üê Pas de fork
        [queue.name],
        connection=get_redis_connection(),
        job_monitoring_interval=30,
    )

    worker.work(
        with_scheduler=with_scheduler,
        logging_level=logging.INFO,
        max_jobs=max_jobs,
    )
```

**Changements** :
1. ‚úÖ `Worker` ‚Üí `SimpleWorker` (ligne 7)
2. ‚úÖ Suppression du code `multiprocessing.set_start_method()` (plus n√©cessaire)
3. ‚úÖ Commentaires explicatifs ajout√©s

### Avantages de SimpleWorker

| Crit√®re | Worker (fork) | SimpleWorker (same process) |
|---------|--------------|----------------------------|
| **Compatible CUDA** | ‚ùå Non | ‚úÖ Oui |
| **Isolation jobs** | ‚úÖ Forte (subprocess) | ‚ö†Ô∏è Moyenne (m√™me processus) |
| **Overhead startup** | ‚ö†Ô∏è Fork √† chaque job | ‚úÖ Aucun overhead |
| **M√©moire** | ‚ö†Ô∏è Dupliqu√©e | ‚úÖ Partag√©e |
| **Performance GPU** | ‚ùå Impossible | ‚úÖ Optimale |

### Comportement avec DEV_MODE

**Configuration actuelle** (`docker-compose.yml`) :
```yaml
environment:
  DEV_MODE: "true"  # Auto-reload apr√®s chaque job
```

**Impact** (`worker.py` lignes 41-42) :
```python
is_dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
max_jobs = 1 if is_dev_mode else 10  # Recharge apr√®s 1 job en dev
```

**R√©sultat** :
- En DEV : Worker traite 1 job ‚Üí se termine ‚Üí red√©marre proprement
- En PROD : Worker traite 10 jobs ‚Üí se termine ‚Üí red√©marre (√©vite fuites m√©moire)

‚úÖ **Parfait pour SimpleWorker** : Le worker se recharge r√©guli√®rement donc pas d'accumulation d'√©tat

---

## üèóÔ∏è Strat√©gies Multi-Worker Production

### Contexte

SimpleWorker ex√©cute les jobs **s√©quentiellement** (1 job √† la fois par worker).

**Question** : Comment scaler pour traiter plusieurs documents en parall√®le ?

### Option 1 : Multi-Containers SimpleWorker (‚≠ê RECOMMAND√â)

**Architecture** : 1 container = 1 SimpleWorker = 1 GPU d√©di√©

#### Configuration Docker Compose

```yaml
# docker-compose.prod.yml
version: '3.8'

x-worker-common: &worker-common
  build:
    context: .
    dockerfile: ./app/Dockerfile
  image: sap-kb-worker:latest
  env_file: .env
  environment:
    REDIS_URL: redis://redis:6379/0
    DEV_MODE: "false"  # Production mode
    HF_HOME: /data/models
    KNOWBASE_DATA_DIR: /data
  volumes:
    - ./data:/data
    - ./src:/app/src
    - ./config:/app/config
  networks:
    - knowbase_net
  working_dir: /app
  command: python -m knowbase.ingestion.queue

services:
  # ========================================
  # Worker 1 - GPU 0
  # ========================================
  ingestion-worker-1:
    <<: *worker-common
    container_name: osmose-worker-gpu-1
    stop_grace_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']  # GPU physique 0
              capabilities: [gpu]
    environment:
      CUDA_VISIBLE_DEVICES: "0"
      WORKER_NAME: "gpu-worker-1"

  # ========================================
  # Worker 2 - GPU 1
  # ========================================
  ingestion-worker-2:
    <<: *worker-common
    container_name: osmose-worker-gpu-2
    stop_grace_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']  # GPU physique 1
              capabilities: [gpu]
    environment:
      CUDA_VISIBLE_DEVICES: "0"  # Mapp√© comme device 0 dans le container
      WORKER_NAME: "gpu-worker-2"

  # ========================================
  # Worker 3 - CPU Fallback (optionnel)
  # ========================================
  ingestion-worker-cpu:
    <<: *worker-common
    container_name: osmose-worker-cpu
    stop_grace_period: 30s
    # Pas de configuration GPU ‚Üí utilisera CPU automatiquement
    environment:
      CUDA_VISIBLE_DEVICES: ""  # Force CPU
      WORKER_NAME: "cpu-worker"

networks:
  knowbase_net:
    name: knowbase_network
    driver: bridge
```

#### D√©ploiement

```bash
# Production avec 2 GPUs + 1 CPU fallback
docker-compose -f docker-compose.prod.yml up -d

# V√©rifier les workers actifs
docker-compose -f docker-compose.prod.yml ps

# Logs d'un worker sp√©cifique
docker logs osmose-worker-gpu-1 -f

# Scaling dynamique (workers CPU uniquement)
docker-compose -f docker-compose.prod.yml up -d --scale ingestion-worker-cpu=3
```

#### Avantages ‚úÖ

1. **Simplicit√©** : Aucun changement de code n√©cessaire
2. **Isolation GPU** : Chaque worker a son propre GPU d√©di√©
3. **Scalabilit√© lin√©aire** :
   - 1 GPU ‚Üí 1 worker ‚Üí 1 job concurrent
   - 2 GPUs ‚Üí 2 workers ‚Üí 2 jobs concurrents
   - N GPUs ‚Üí N workers ‚Üí N jobs concurrents
4. **Failover** : Si un worker crash, les autres continuent
5. **Monitoring** : Logs s√©par√©s par container
6. **Hybrid** : M√©lange GPU + CPU workers possible

#### Inconv√©nients ‚ö†Ô∏è

1. **M√©moire** : Chaque worker charge ses propres mod√®les
   - `multilingual-e5-large` : ~2.5 GB par worker
   - Solution : Acceptable si GPU a ‚â•8GB VRAM
2. **Overhead** : N containers au lieu de 1
   - Impact : N√©gligeable avec Docker (containers l√©gers)

#### Estimation Ressources

**Par Worker GPU** :
- VRAM : ~3-4 GB (mod√®le embeddings + contexte CUDA)
- RAM : ~4-6 GB (mod√®les Python + cache)
- CPU : 2-4 cores (extraction texte, Vision API)

**Exemple Configuration** :
- **Machine 1** : 1√ó RTX 5070 Ti (16GB) ‚Üí 2 workers GPU + 1 worker CPU
- **Machine 2** : 2√ó RTX 4090 (48GB) ‚Üí 4 workers GPU + 2 workers CPU

---

### Option 2 : Celery avec Pool=Solo (Alternative)

**Principe** : Remplacer RQ par Celery qui supporte nativement les workers sans fork

#### Migration vers Celery

**1. Installation**
```bash
pip install celery[redis]
```

**2. Configuration Celery**
```python
# config/celery_config.py
from celery import Celery

app = Celery(
    'osmose',
    broker='redis://redis:6379/0',
    backend='redis://redis:6379/1'
)

app.conf.update(
    task_serializer='pickle',
    accept_content=['pickle', 'json'],
    result_serializer='pickle',
    timezone='Europe/Paris',
    enable_utc=True,

    # IMPORTANT: Pool solo (pas de fork)
    worker_pool='solo',  # ‚Üê √âquivalent SimpleWorker
    worker_concurrency=1,
)
```

**3. D√©finition Tasks**
```python
# src/knowbase/ingestion/celery_tasks.py
from config.celery_config import app
from knowbase.ingestion.pipelines.pptx_pipeline import PPTXPipeline

@app.task(bind=True)
def ingest_pptx_task(self, file_path: str, **kwargs):
    """Task Celery pour ingestion PPTX (√©quivalent ingest_pptx_job RQ)."""

    # Progress callback avec Celery
    def update_progress(progress: int, message: str):
        self.update_state(
            state='PROGRESS',
            meta={'current': progress, 'message': message}
        )

    pipeline = PPTXPipeline()
    result = pipeline.process_pptx(
        file_path,
        progress_callback=update_progress,
        **kwargs
    )

    return result
```

**4. Lancement Worker**
```bash
# Development
celery -A config.celery_config worker --pool=solo --loglevel=info

# Production avec 3 workers
celery -A config.celery_config worker --pool=solo --concurrency=1 --hostname=worker1@%h
celery -A config.celery_config worker --pool=solo --concurrency=1 --hostname=worker2@%h
celery -A config.celery_config worker --pool=solo --concurrency=1 --hostname=worker3@%h
```

**5. Enqueue Tasks**
```python
# Remplacer les appels RQ
# AVANT (RQ)
from knowbase.ingestion.queue.jobs import ingest_pptx_job
job = queue.enqueue(ingest_pptx_job, file_path, ...)

# APR√àS (Celery)
from knowbase.ingestion.celery_tasks import ingest_pptx_task
task = ingest_pptx_task.delay(file_path, ...)
```

#### Avantages ‚úÖ

1. **Production-grade** : Utilis√© par millions d'applications
2. **Monitoring avanc√©** : Flower UI (dashboard web)
3. **Task chaining** : Workflows complexes (ingestion ‚Üí OCR ‚Üí embedding ‚Üí indexing)
4. **Retry logic** : Gestion sophistiqu√©e des erreurs
5. **Distributed** : Workers sur plusieurs machines facilement
6. **Pool options** : `solo`, `threads`, `gevent` selon besoins

#### Inconv√©nients ‚ö†Ô∏è

1. **Migration compl√®te** : Remplacer tout le code RQ (2-3 jours)
2. **Complexit√©** : Courbe d'apprentissage Celery
3. **Overhead** : Plus lourd que RQ (mais features++)

#### Quand Migrer vers Celery ?

**Signaux pour migrer** :
- ‚úÖ Besoin de workers distribu√©s (plusieurs machines)
- ‚úÖ Workflows complexes avec d√©pendances entre tasks
- ‚úÖ Monitoring avanc√© requis (dashboard, m√©triques)
- ‚úÖ Plus de 5-10 workers concurrents
- ‚úÖ Gestion fine des priorit√©s et retry

**Pas n√©cessaire si** :
- ‚úÖ ‚â§ 5 workers sur m√™me machine
- ‚úÖ Jobs simples et ind√©pendants
- ‚úÖ RQ + SimpleWorker fonctionne bien

---

### Option 3 : Custom Worker avec subprocess.spawn (‚ùå Non Recommand√©)

**Principe** : Cr√©er une classe Worker personnalis√©e qui override le m√©canisme de fork

**Pourquoi ne pas faire √ßa** :
- ‚ùå Complexit√© √©lev√©e (maintenance long terme)
- ‚ùå Risque de bugs subtils (s√©rialisation, IPC)
- ‚ùå Doit suivre les mises √† jour RQ
- ‚úÖ **SimpleWorker fait d√©j√† le job**

---

## üéØ Recommandations

### Phase 1-2 : D√©veloppement & MVP (Actuel)

**‚úÖ Solution Actuelle : SimpleWorker (1 worker)**

**Configuration** : `docker-compose.yml`
```yaml
ingestion-worker:
  image: sap-kb-worker:latest
  container_name: knowbase-worker
  environment:
    DEV_MODE: "true"  # Auto-reload apr√®s chaque job
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

**Avantages** :
- ‚úÖ Fonctionne MAINTENANT (pas de refactoring)
- ‚úÖ Compatible GPU (RTX 5070 Ti utilis√©)
- ‚úÖ Simple √† d√©bugger

**Limitations acceptables** :
- ‚ö†Ô∏è 1 document √† la fois (s√©quentiel)
- ‚ö†Ô∏è OK pour d√©veloppement et d√©monstrations

---

### Phase 3 : Pr√©-Production (Semaines 20-24)

**‚úÖ Solution : Multi-Containers SimpleWorker**

**Configuration** : Cr√©er `docker-compose.prod.yml`

**Scalabilit√© cible** :
- **2 workers GPU** : Traitement de 2 documents simultan√©s
- **1 worker CPU** : Fallback si GPUs occup√©s

**D√©ploiement** :
```bash
# Lancement production
docker-compose -f docker-compose.infra.yml \
               -f docker-compose.yml \
               -f docker-compose.prod.yml \
               -f docker-compose.monitoring.yml \
               up -d
```

**Monitoring** :
- Grafana : Dashboards per-worker
- Prometheus : M√©triques GPU (nvidia-smi)
- Loki : Logs agr√©g√©s

---

### Phase 4 : Production & Scale (Semaines 25+)

**√âvaluation Migration Celery**

**Crit√®res de d√©cision** :

| Crit√®re | Rester RQ+SimpleWorker | Migrer Celery |
|---------|----------------------|---------------|
| **Nb workers** | ‚â§ 5 workers | > 5 workers |
| **Distribution** | M√™me machine | Plusieurs machines |
| **Workflows** | Jobs ind√©pendants | Tasks avec d√©pendances |
| **Monitoring** | Grafana suffit | Besoin Flower UI |
| **Effort migration** | 0 jours | 2-3 jours |

**Recommandation** :
- ‚úÖ **Si ‚â§ 5 workers** : Garder SimpleWorker (KISS principle)
- ‚ö†Ô∏è **Si > 5 workers** : √âvaluer Celery vs scale horizontal (+ machines)

---

## üìù Plan d'Action

### ‚úÖ Fait (2025-11-15)

1. ‚úÖ Analyse du probl√®me CUDA multiprocessing
2. ‚úÖ Identification solution : SimpleWorker
3. ‚úÖ Impl√©mentation dans `worker.py`
4. ‚úÖ Red√©marrage worker sans erreurs
5. ‚úÖ Documentation technique cr√©√©e

### üîÑ En Cours

1. ‚è≥ **Test GPU avec import r√©el**
   - Action : Lancer import d'un document
   - Validation : Observer logs CUDA initialization
   - Cible : Confirmer temps < 20 min (vs 85 min CPU)

### üìÖ Court Terme (Phase 1 - Semaines 11-14)

1. **Valider performance GPU** (Semaine 11)
   - Import de 5 documents test
   - Mesurer temps r√©els
   - Comparer CPU vs GPU

2. **Optimiser configuration GPU** (Semaine 12)
   - Ajuster `batch_size` pour VRAM disponible
   - Tester diff√©rentes configurations `max_jobs`
   - Documenter settings optimaux

3. **Monitoring GPU** (Semaine 13)
   - Ajouter m√©triques GPU √† Prometheus
   - Dashboard Grafana : VRAM, utilisation, temp√©rature
   - Alertes si GPU non utilis√©

### üìÖ Moyen Terme (Phase 2-3 - Semaines 15-24)

1. **Cr√©er docker-compose.prod.yml** (Semaine 20)
   - Configuration 2-3 workers
   - Tests de charge
   - Documentation d√©ploiement

2. **Tests scalabilit√©** (Semaine 21)
   - Import concurrent de 10 documents
   - Mesurer throughput r√©el
   - Identifier bottlenecks

3. **Optimisation m√©moire** (Semaine 22)
   - Profiling VRAM par worker
   - Ajuster `max_jobs` si n√©cessaire
   - Tests stabilit√© 24h

### üìÖ Long Terme (Phase 4 - Semaines 25+)

1. **√âvaluation Celery** (Semaine 25)
   - POC Celery sur branche `feat/celery-migration`
   - Comparaison RQ vs Celery
   - D√©cision GO/NO-GO

2. **Production readiness** (Semaine 26+)
   - CI/CD pour d√©ploiement workers
   - Runbooks op√©rationnels
   - Formation √©quipe ops

---

## üîß Maintenance & Troubleshooting

### V√©rifier que GPU est utilis√©

```bash
# Dans le container worker
docker exec knowbase-worker python -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'Device: {torch.cuda.get_device_name(0)}')
"
```

**Output attendu** :
```
CUDA available: True
Device: NVIDIA GeForce RTX 5070 Ti
```

### V√©rifier initialisation SentenceTransformer

```bash
# Logs worker au d√©marrage
docker logs knowbase-worker | grep OSMOSE

# Devrait afficher
[OSMOSE] Loading embeddings model: intfloat/multilingual-e5-large...
[OSMOSE] ‚úÖ Embeddings model loaded: intfloat/multilingual-e5-large (1024D, device: cuda (GPU: NVIDIA GeForce RTX 5070 Ti))
```

### Monitoring VRAM en temps r√©el

```bash
# Sur l'h√¥te Windows avec GPU
nvidia-smi -l 1

# Observer "GPU-Util" et "Memory-Usage" pendant ingestion
```

### Si GPU non utilis√© (fallback CPU)

**Sympt√¥mes** :
- Logs montrent `device: cpu`
- Temps d'ingestion > 60 min

**Diagnostic** :
```bash
# 1. V√©rifier GPU visible par Docker
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi

# 2. V√©rifier configuration docker-compose
docker-compose config | grep -A 5 "ingestion-worker"

# 3. V√©rifier CUDA_VISIBLE_DEVICES
docker exec knowbase-worker env | grep CUDA
```

**Solutions** :
1. Red√©marrer Docker Desktop (bug Windows)
2. V√©rifier `docker-compose.yml` lignes 79-85 (deploy.resources.reservations)
3. Mettre √† jour NVIDIA Container Toolkit

---

## üìä M√©triques Cibles

### Performance GPU vs CPU

| M√©trique | CPU (baseline) | GPU (cible) | Am√©lioration |
|----------|----------------|-------------|--------------|
| **Temps ingestion** (230 slides) | 85 min | 15-20 min | **~4.5x** |
| **Embeddings batch** (128 texts) | ~5 sec | ~0.5 sec | **10x** |
| **Topic segmentation** | ~45 min | ~5 min | **9x** |
| **Throughput** | 0.7 docs/h | 3-4 docs/h | **5x** |

### Scalabilit√© Multi-Worker

| Configuration | Throughput Th√©orique | Utilisation VRAM |
|---------------|---------------------|------------------|
| 1 worker GPU | 3-4 docs/h | 3-4 GB |
| 2 workers GPU | 6-8 docs/h | 6-8 GB |
| 3 workers GPU | 9-12 docs/h | 9-12 GB |

**Note** : RTX 5070 Ti (16 GB VRAM) ‚Üí Max 3-4 workers GPU recommand√©s

---

## üîó R√©f√©rences

### Documentation Projet

- **Architecture OSMOSE** : `doc/OSMOSE_ARCHITECTURE_TECHNIQUE.md`
- **Phase 1 Semantic Core** : `doc/phases/PHASE1_SEMANTIC_CORE.md`
- **Roadmap** : `doc/OSMOSE_ROADMAP_INTEGREE.md`

### Documentation Externe

- **RQ SimpleWorker** : https://python-rq.org/docs/workers/#simpleworker
- **CUDA Multiprocessing** : https://pytorch.org/docs/stable/notes/multiprocessing.html
- **Celery** : https://docs.celeryproject.org/en/stable/
- **Docker GPU** : https://docs.docker.com/config/containers/resource_constraints/#gpu

### Issues & Discussions

- **RQ + CUDA Fork Issue** : https://github.com/rq/rq/issues/1220
- **PyTorch Multiprocessing** : https://github.com/pytorch/pytorch/issues/3492

---

## ‚úçÔ∏è Changelog

| Date | Version | Changements |
|------|---------|-------------|
| 2025-11-15 | 1.0 | Document initial - Solution SimpleWorker impl√©ment√©e |

---

**Maintenu par** : √âquipe OSMOSE
**Derni√®re r√©vision** : 2025-11-15
