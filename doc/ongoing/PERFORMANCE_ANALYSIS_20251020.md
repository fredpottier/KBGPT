# Analyse Performance - Import Document 250 Slides

**Date** : 2025-10-20
**Document** : RISE_with_SAP_Cloud_ERP_Private.pptx (250 slides)
**Dur√©e totale** : 1h 09min 44s (4184 secondes)
**Status** : Phase 2 en √©chec (NoneType error)

---

## üìä R√©sum√© Ex√©cutif

### Dur√©e par Phase

| Phase | Dur√©e | % du Total | Status |
|-------|-------|------------|--------|
| **Phase 1 - Extraction** | 16 min 18s | 23.4% | ‚úÖ OK |
| **Phase 1 - Canonicalisation** | 53 min 08s | 76.2% | ‚ö†Ô∏è TR√àS LENT |
| **Phase 2 - Relations** | 3s | 0.1% | ‚ùå √âCHEC |
| **Indexing Qdrant** | N/A | 0% | ‚ùå NON ATTEINT |

### Goulot d'√âtranglement Principal

**üî¥ CANONICALISATION = 76% du temps total**

---

## üìã Chronologie D√©taill√©e

### Phase 1 - Extraction (16 min 18s)

```
13:40:18 ‚Üí D√©but FSM + Extraction
13:56:36 ‚Üí Fin Extraction
```

**R√©sultats** :
- 79 segments cr√©√©s
- 556 concepts candidats extraits
- Temps moyen : **12.4s/segment**

**D√©tails op√©rations** :
1. PDF conversion via LibreOffice (PPTX ‚Üí PDF)
2. Vision API calls (GPT-4o Vision) pour chaque segment
3. Text extraction + chunking intelligent

**√âvaluation** : ‚úÖ **Performance acceptable**
- 12.4s/segment est raisonnable pour 250 slides
- Inclut PDF conversion + Vision API + chunking

---

### Phase 1 - Canonicalisation (53 min 08s) ‚ö†Ô∏è

```
13:56:36 ‚Üí D√©but Gate Check + Canonicalisation
14:49:44 ‚Üí Fin Canonicalisation
```

**R√©sultats** :
- 556 concepts candidats trait√©s
- 447 concepts canoniques cr√©√©s
- 2266 relations proto-KG cr√©√©es
- Temps moyen : **5.7s/concept**

**Op√©rations internes** :
1. **EntityNormalizerNeo4j** : Recherche dans ontologie (queries Cypher)
2. **LLMCanonicalizer** : Appels LLM s√©quentiels pour concepts non trouv√©s
3. **Circuit Breaker** : Gestion erreurs JSON parsing
4. **Gate Check** : Validation + promotion concepts
5. **Neo4j Persistence** : Cr√©ation CanonicalConcept + relations PROMOTED_TO

**M√©triques observ√©es** :
- **209 changements d'√©tat Circuit Breaker** (OPEN/HALF_OPEN/CLOSED)
- **3887 JSON truncation fixes** appliqu√©s avec succ√®s
- **10337 title case fallbacks** utilis√©s (logs dupliqu√©s inclus)

**√âvaluation** : üî¥ **GOULOT D'√âTRANGLEMENT CRITIQUE**

---

### Phase 2 - Extraction Relations (3s) ‚ùå

```
14:49:59 ‚Üí D√©but EXTRACT_RELATIONS
14:50:00 ‚Üí Initialisation composants
14:50:02 ‚Üí ERREUR : 'NoneType' object has no attribute 'lower'
```

**R√©sultats** :
- ‚ùå √âchec imm√©diat (3 secondes)
- 0 relations typ√©es cr√©√©es (USES, REQUIRES, etc.)
- Document incomplet dans Neo4j

**Impact** :
- Relations s√©mantiques absentes
- Indexing Qdrant jamais atteint
- Recherche d√©grad√©e (pas de graph traversal)

**√âvaluation** : üî¥ **BLOQUANT - Emp√™che finalisation document**

---

## üîç Analyse D√©taill√©e du Goulot

### Probl√®me : Canonicalisation Trop Lente (53 min)

#### Causes Identifi√©es

**1. Circuit Breaker Instable (209 changements d'√©tat)**

Le circuit breaker s'ouvre/ferme fr√©quemment :
```
CLOSED ‚Üí OPEN (apr√®s 5 √©checs cons√©cutifs)
OPEN ‚Üí HALF_OPEN (apr√®s 60s timeout)
HALF_OPEN ‚Üí CLOSED (si 1 succ√®s)
HALF_OPEN ‚Üí OPEN (si 1 √©chec)
```

**Impact** :
- Chaque ouverture = 60s de d√©lai avant retry
- 209 transitions ‚âà **30-40 minutes perdues en timeouts**

**Cause racine** : JSON truncation par le LLM (malgr√© max_tokens=400)

---

**2. JSON Truncation Massive (3887 fixes appliqu√©s)**

Le LLM (gpt-4o-mini) retourne du JSON tronqu√© :

```json
{
  "canonical_name": "Content Owner",
  "confidence": 0.85,
  "reasoning": "The term 'Content Owner' is commonly used in various contexts, including project management and content management, but doe
```

**Fix appliqu√©** : `_parse_json_robust()` compl√®te le JSON
- Ferme quotes ouvertes
- Ajoute `}` manquants
- Parse le JSON compl√©t√©

**Impact** :
- Fix fonctionne (3887 JSON r√©par√©s)
- Mais le retry prend du temps (~1-2s/concept suppl√©mentaire)
- **Temps perdu** : ~1-2 heures cumul√©es en parsing retries

---

**3. Appels LLMCanonicalizer S√©quentiels**

Actuellement :
```python
for concept in concepts:
    canonical_name = llm_canonicalizer.canonicalize(concept)
    # 1 appel LLM par concept ‚Üí 556 appels s√©quentiels
```

**Impact** :
- 556 appels √ó 5.7s/appel = 53 minutes
- Pas de parall√©lisation
- Pas de batch processing

**Potentiel d'optimisation** : **TR√àS √âLEV√â (70-80% gain)**

---

**4. Neo4j Queries Non Optimis√©es**

EntityNormalizerNeo4j fait :
```cypher
MATCH (ont:OntologyEntity)-[:HAS_ALIAS]->(alias:OntologyAlias {
    normalized: $normalized,
    tenant_id: $tenant_id
})
WHERE ont.status <> 'auto_learned_pending'
RETURN ont.canonical_name, ont.entity_type
LIMIT 1
```

**Probl√®mes** :
- 1 query par concept (556 queries)
- Pas de cache en m√©moire
- Index global sur `normalized` mais pas de cache applicatif

**Potentiel d'optimisation** : MOYEN (5-10% gain)

---

## üöÄ Plan d'Optimisation

### Priorit√© 0 : Fix Phase 2 NoneType Error (BLOQUANT)

**Probl√®me** : Phase 2 √©choue imm√©diatement avec `'NoneType' object has no attribute 'lower'`

**Impact** :
- Document incomplet
- Pas de relations s√©mantiques
- Pas d'indexing Qdrant
- Recherche d√©grad√©e

**Action** :
1. Identifier ligne exacte du crash
2. Ajouter null checks
3. Tester avec document actuel

**ROI** : **CRITIQUE - D√©blocage complet de la pipeline**

---

### Priorit√© 1 : Batch LLMCanonicalizer (GAIN 70%)

**Objectif** : R√©duire 53 min ‚Üí 10-15 min

**Impl√©mentation** :

#### Option A : Batch Processing (Recommand√©)

Grouper 20 concepts par appel LLM :

```python
# Au lieu de :
for concept in concepts:
    result = llm.complete_canonicalization([{
        "role": "user",
        "content": f"Canonicalize: {concept}"
    }])

# Faire :
batch_size = 20
for i in range(0, len(concepts), batch_size):
    batch = concepts[i:i+batch_size]
    result = llm.complete_canonicalization([{
        "role": "user",
        "content": f"Canonicalize these {len(batch)} concepts:\n{json.dumps(batch)}"
    }])
    # Parser le JSON avec 20 r√©sultats
```

**Sch√©ma JSON batch** :
```json
{
  "canonicalizations": [
    {
      "raw_name": "aws",
      "canonical_name": "Amazon Web Services (AWS)",
      "confidence": 0.95,
      "reasoning": "..."
    },
    {
      "raw_name": "sap cloud",
      "canonical_name": "SAP Cloud Platform",
      "confidence": 0.90,
      "reasoning": "..."
    }
  ]
}
```

**Gain estim√©** :
- 556 concepts / 20 par batch = **28 appels LLM** (au lieu de 556)
- 28 √ó 5s = **140s = 2.3 minutes** (au lieu de 53 minutes)
- **R√©duction : 95% du temps de canonicalisation**

**Risques** :
- JSON plus complexe ‚Üí Plus de risques de truncation
- N√©cessite modifier sch√©ma JSON + parsing

**Mitigation** :
- Utiliser `response_format={"type": "json_object"}` explicite
- Augmenter max_tokens √† 2000-3000 pour batch de 20
- Garder fallback s√©quentiel si batch √©choue

---

#### Option B : Parall√©lisation (Alternative)

Parall√©liser les appels LLM :

```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [
        executor.submit(llm_canonicalizer.canonicalize, concept)
        for concept in concepts
    ]
    results = [f.result() for f in futures]
```

**Gain estim√©** :
- 556 concepts / 10 workers = **56 batches parall√®les**
- 56 √ó 5s = **280s = 4.7 minutes**
- **R√©duction : 91% du temps**

**Risques** :
- Rate limiting OpenAI (500 req/min)
- Co√ªt API augment√© (m√™me nombre d'appels)
- Circuit breaker complexe √† g√©rer en //

---

### Priorit√© 2 : Optimiser Circuit Breaker (GAIN 20-30%)

**Probl√®me** : 209 transitions OPEN/HALF_OPEN perdent 30-40 min

**Actions** :

#### 2.1 R√©duire Timeout R√©cup√©ration

```python
# Actuellement
recovery_timeout = 60  # secondes

# Propos√©
recovery_timeout = 30  # secondes
```

**Gain estim√©** : 15-20 minutes

---

#### 2.2 Augmenter Threshold √âchecs

```python
# Actuellement
failure_threshold = 5  # √©checs cons√©cutifs

# Propos√©
failure_threshold = 10  # √©checs cons√©cutifs
```

**Gain estim√©** : Moins d'ouvertures ‚Üí 5-10 minutes

---

#### 2.3 Am√©liorer JSON Parsing

**Probl√®me** : 3887 JSON truncation fixes ‚Üí retries multiples

**Actions** :
1. Ajouter `response_format={"type": "json_object"}` explicite dans llm_router
2. Augmenter max_tokens √† 500 (actuellement 400)
3. Simplifier sch√©ma JSON (enlever `reasoning` field si n√©cessaire)

```python
# Dans llm_router.py
def complete_canonicalization(
    messages: List[Dict[str, Any]],
    temperature: float = 0.0,
    max_tokens: int = 500,  # Augment√© de 400 √† 500
    response_format: dict = {"type": "json_object"}  # AJOUT√â
) -> str:
```

**Gain estim√©** : 10-15 minutes

---

### Priorit√© 3 : Cache Neo4j (GAIN 5-10%)

**Impl√©mentation** :

```python
from functools import lru_cache

class EntityNormalizerNeo4j:
    def __init__(self, driver):
        self.driver = driver
        self._cache = {}  # Cache en m√©moire

    def normalize_entity_name(
        self,
        raw_name: str,
        entity_type_hint: Optional[str] = None,
        tenant_id: str = "default",
        include_pending: bool = False
    ):
        # Check cache
        cache_key = f"{tenant_id}:{raw_name.lower()}:{entity_type_hint}"
        if cache_key in self._cache:
            return self._cache[cache_key]

        # Query Neo4j
        result = self._query_neo4j(...)

        # Store in cache
        self._cache[cache_key] = result
        return result
```

**Gain estim√©** : 2-5 minutes (sur 556 concepts, beaucoup de duplicates)

---

## üìä Estimations Gain Total

### Sc√©nario Actuel (Baseline)

| Phase | Dur√©e Actuelle |
|-------|----------------|
| Extraction | 16 min |
| Canonicalisation | 53 min |
| Phase 2 (si fonctionnait) | ~5 min (estim√©) |
| Indexing | ~2 min (estim√©) |
| **TOTAL** | **~76 minutes** |

---

### Sc√©nario Optimis√© (Tous les Fixes)

| Phase | Dur√©e Optimis√©e | Gain |
|-------|-----------------|------|
| Extraction | 16 min | - |
| Canonicalisation | **5 min** | -48 min (-91%) |
| Phase 2 | 5 min | D√©bloqu√© |
| Indexing | 2 min | D√©bloqu√© |
| **TOTAL** | **28 minutes** | **-48 min (-63%)** |

---

### D√©tail Gains Canonicalisation

| Optimisation | Gain | Dur√©e R√©sultante |
|--------------|------|------------------|
| Baseline | - | 53 min |
| **+ Batch LLM (20 concepts)** | -50 min | **3 min** |
| + Circuit breaker tuning | -15 min | 38 min |
| + JSON parsing am√©lior√© | -10 min | 43 min |
| + Cache Neo4j | -3 min | 50 min |

**MEILLEURE OPTION : Batch LLM = 95% de gain √† lui seul**

---

## üéØ Recommandations Finales

### Actions Imm√©diates (Cette Semaine)

#### 1. Fixer Phase 2 NoneType Error (Priorit√© 0) ‚ö†Ô∏è

**Action** :
```bash
# Trouver ligne exacte du crash
docker-compose logs ingestion-worker 2>&1 | grep "NoneType.*lower" -A 5 -B 5
```

**D√©lai** : 1-2 heures
**ROI** : CRITIQUE (d√©blocage complet)

---

#### 2. Impl√©menter Batch LLMCanonicalizer (Priorit√© 1) üöÄ

**Action** :
1. Cr√©er `batch_canonicalize()` dans `llm_canonicalizer.py`
2. Modifier sch√©ma JSON pour supporter batch
3. Mettre √† jour `_parse_json_robust()` pour batch
4. Tester avec 556 concepts

**D√©lai** : 4-6 heures
**ROI** : **TR√àS √âLEV√â (53 min ‚Üí 3 min = -50 min)**

---

#### 3. Mesurer Nouveau Temps Total

Apr√®s fixes 1+2 :
- Tester import m√™me document
- Comparer dur√©e totale
- V√©rifier qualit√© r√©sultats (concepts + relations)

**R√©sultat attendu** : **28 minutes** (au lieu de 76 min)

---

### Actions Court Terme (Semaine Prochaine)

#### 4. Optimiser Circuit Breaker (Priorit√© 2)

- R√©duire timeout : 60s ‚Üí 30s
- Augmenter threshold : 5 ‚Üí 10
- Ajouter `response_format` explicite

**Gain suppl√©mentaire** : 5-10 minutes

---

#### 5. Impl√©menter Cache Neo4j (Priorit√© 3)

- LRU cache en m√©moire
- Invalidation sur updates

**Gain suppl√©mentaire** : 2-5 minutes

---

## üìà Objectif Cible

### Temps Id√©al pour 250 Slides

| Phase | Temps Cible |
|-------|-------------|
| Extraction | 15 min |
| Canonicalisation | **< 5 min** |
| Phase 2 Relations | 5 min |
| Indexing | 2 min |
| **TOTAL** | **< 30 minutes** |

**Avec batch LLM : OBJECTIF ATTEIGNABLE ‚úÖ**

---

## üîß Commandes Diagnostic

### V√©rifier Temps Import Futur

```bash
# Monitorer timestamps
docker-compose logs ingestion-worker -f | grep -E "Starting|completed|ERROR"

# Calculer dur√©e Phase 1
docker-compose logs ingestion-worker 2>&1 | grep -E "Starting extraction|Pattern mining starting" | tail -2

# Calculer dur√©e Canonicalisation
docker-compose logs ingestion-worker 2>&1 | grep -E "Gate check starting|Starting persistence" | tail -2
```

### V√©rifier Circuit Breaker

```bash
# Compter transitions
docker-compose logs ingestion-worker 2>&1 | grep "\[CircuitBreaker\]" | grep -E "OPEN|HALF_OPEN|CLOSED" | wc -l

# Voir √©tat actuel
docker-compose logs ingestion-worker 2>&1 | grep "\[CircuitBreaker\]" | tail -10
```

### V√©rifier JSON Truncation

```bash
# Compter fixes appliqu√©s
docker-compose logs ingestion-worker 2>&1 | grep "Fixed truncated JSON" | wc -l

# Voir exemples
docker-compose logs ingestion-worker 2>&1 | grep "Fixed truncated JSON" | head -10
```

---

## üìù Notes

### Pourquoi Batch LLM est la Meilleure Option ?

1. **Gain maximal (95%)** : 53 min ‚Üí 3 min
2. **Simple √† impl√©menter** : ~6h de dev
3. **Pas de risque rate limiting** : 28 appels au lieu de 556
4. **Co√ªt API r√©duit** : -95% de calls
5. **Scalable** : Fonctionne pour 1000+ concepts

### Pourquoi Pas Parall√©lisation ?

1. **Gain inf√©rieur** : 53 min ‚Üí 5 min (vs 3 min avec batch)
2. **Plus complexe** : Gestion //  + circuit breaker
3. **M√™me co√ªt API** : 556 calls (pas de r√©duction)
4. **Risque rate limiting** : OpenAI 500 req/min

---

**Cr√©√© par** : Claude Code
**Pour** : Analyse performance import document 250 slides
**Prochaine √âtape** : Impl√©menter Batch LLMCanonicalizer
