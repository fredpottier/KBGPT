# Optimisation Performance - Traitement Mono-Document

**Date:** 2025-10-24
**Objectif:** Acc√©l√©rer consid√©rablement le traitement d'un **seul document** via parall√©lisation interne

---

## üéØ Probl√®me Identifi√©

**Situation actuelle:** Le traitement d'un document PPTX de 50 slides prend ~120-180 secondes

**Cause:** Traitement **100% s√©quentiel** des segments/chunks :
```python
# Actuel dans extractor/orchestrator.py (ligne 164)
for idx, segment in enumerate(state.segments):  # ‚ùå S√âQUENTIEL !
    # Extraction LLM (~5-10s par segment)
    prepass_result = await prepass_analyzer.execute(...)
    extraction_result = await extraction_agent.execute(...)
```

**Impact:** Pour un document avec 10 segments ‚Üí 10 √ó 10s = **100 secondes** d'attente !

---

## üöÄ Solution: Parall√©lisation Interne

### Gain Attendu

| M√©trique | Actuel (S√©quentiel) | Optimis√© (Parall√®le) | Am√©lioration |
|----------|---------------------|----------------------|--------------|
| 10 segments √ó 10s | 100s | 15-20s | **5-7x plus rapide** |
| Document 50 slides | 150-180s | 30-40s | **4-5x plus rapide** |
| Utilisation CPU | 10-20% | 70-90% | Meilleur ROI |

---

## üìä Analyse du Pipeline Actuel

### Flux de Traitement (S√©quentiel)

```
Document PPTX
    ‚Üì
Segmentation (rapide, ~2s)
    ‚Üì
Pour chaque segment (S√âQUENTIEL ‚ùå):
    ‚Üì
    Prepass Analysis (~3-5s)  ‚Üê Appel LLM
    ‚Üì
    Extraction (~5-10s)       ‚Üê Appel LLM
    ‚Üì
    Save to Proto-KG (~1s)    ‚Üê I/O Neo4j
    ‚Üì
Total: N segments √ó 10s = 100-150s
```

### Goulots d'√âtranglement

1. **Extractor Orchestrator** (`src/knowbase/agents/extractor/orchestrator.py:164`)
   ```python
   for idx, segment in enumerate(state.segments):  # ‚ùå S√âQUENTIEL
       prepass_result = await prepass_analyzer.execute(...)
       extraction_result = await extraction_agent.execute(...)
   ```

2. **Miner Relations** (`src/knowbase/agents/miner/miner.py:270`)
   ```python
   for topic_id, segment_concepts in segments.items():  # ‚ùå S√âQUENTIEL
       # Cr√©ation relations entre concepts
   ```

3. **Chunking + Embedding** (`src/knowbase/agents/supervisor/supervisor.py:459`)
   ```python
   for i, chunk in enumerate(chunks):  # ‚ùå S√âQUENTIEL
       points.append({"embedding": chunk.embedding, ...})
   ```

---

## üõ†Ô∏è Optimisations √† Impl√©menter

### Niveau 1: Parall√©liser Extraction par Segment ‚≠ê‚≠ê‚≠ê

**Impact:** **5-7x plus rapide** pour l'extraction

#### Modification: `src/knowbase/agents/extractor/orchestrator.py`

**Avant (ligne 164):**
```python
for idx, segment in enumerate(state.segments):
    logger.debug(f"[EXTRACTOR] Processing segment {idx+1}/{len(state.segments)}")

    # Prepass Analysis
    prepass_input = PrepassAnalyzerInput(...)
    prepass_result = await prepass_analyzer.execute(prepass_input)

    # Extraction
    extraction_input = ExtractionInput(...)
    extraction_result = await extraction_agent.execute(extraction_input)

    # Save to Proto-KG
    self._save_to_proto_kg(extraction_result, segment_id)
```

**Apr√®s (parall√©lis√© avec `asyncio.gather`):**
```python
import asyncio
from typing import List, Tuple

async def _process_segment(
    self,
    idx: int,
    segment: dict,
    state: AgentState,
    prepass_analyzer,
    extraction_agent
) -> Tuple[int, dict]:
    """Traite un segment en parall√®le."""
    logger.debug(f"[EXTRACTOR] Processing segment {idx+1}")

    # Prepass Analysis
    prepass_input = PrepassAnalyzerInput(
        segment_text=segment.get("text", ""),
        segment_id=segment.get("segment_id"),
        topic_label=segment.get("topic_label", "unknown"),
        tenant_id=state.tenant_id
    )
    prepass_result = await prepass_analyzer.execute(prepass_input)

    # Extraction
    extraction_input = ExtractionInput(
        segment_text=segment.get("text", ""),
        segment_id=segment.get("segment_id"),
        prepass_context=prepass_result.model_dump(),
        tenant_id=state.tenant_id
    )
    extraction_result = await extraction_agent.execute(extraction_input)

    return idx, extraction_result


# Dans execute() - remplacer la boucle for par:
logger.info(f"[EXTRACTOR] üöÄ Processing {len(state.segments)} segments IN PARALLEL")

# Cr√©er toutes les t√¢ches d'extraction
extraction_tasks = [
    self._process_segment(idx, segment, state, prepass_analyzer, extraction_agent)
    for idx, segment in enumerate(state.segments)
]

# Ex√©cuter TOUTES les extractions en parall√®le
segment_results = await asyncio.gather(*extraction_tasks)

# Sauvegarder tous les r√©sultats (peut aussi √™tre parall√©lis√©)
logger.info(f"[EXTRACTOR] ‚úÖ {len(segment_results)} segments extracted, saving to Proto-KG")
for idx, extraction_result in segment_results:
    segment_id = state.segments[idx].get("segment_id")
    self._save_to_proto_kg(extraction_result, segment_id, state)
```

**Gain:** Si vous avez 10 segments de 10s chacun :
- Avant: 10 √ó 10s = **100s**
- Apr√®s: max(10s) + overhead = **15s** (avec rate limiting LLM)

---

### Niveau 2: Parall√©liser Embeddings Qdrant ‚≠ê‚≠ê

**Impact:** **3-4x plus rapide** pour l'indexation

#### Modification: `src/knowbase/agents/supervisor/supervisor.py`

**Avant (ligne 459):**
```python
points = []
for i, chunk in enumerate(chunks):
    points.append({
        "id": chunk.get("chunk_id"),
        "vector": chunk.get("embedding", []),
        "payload": {...}
    })
```

**Apr√®s (batch parall√®le):**
```python
from concurrent.futures import ThreadPoolExecutor
import asyncio

def _create_point(chunk, state):
    """Cr√©e un point Qdrant (peut √™tre CPU-intensif)."""
    return {
        "id": chunk.get("chunk_id"),
        "vector": chunk.get("embedding", []),
        "payload": {
            "tenant_id": state.tenant_id,
            "document_id": state.document_id,
            "text": chunk.get("text", ""),
            # ... autres champs
        }
    }

# Parall√©liser la cr√©ation des points avec ThreadPoolExecutor
loop = asyncio.get_event_loop()
with ThreadPoolExecutor(max_workers=8) as executor:
    points = await asyncio.gather(*[
        loop.run_in_executor(executor, _create_point, chunk, state)
        for chunk in chunks
    ])

logger.info(f"[SUPERVISOR] ‚úÖ {len(points)} points created in parallel")

# Uploader par batches
batch_size = 100
for i in range(0, len(points), batch_size):
    batch = points[i:i+batch_size]
    qdrant_client.upsert(collection_name=collection, points=batch)
```

**Gain:** 100 chunks
- Avant: 100 √ó 0.1s = **10s**
- Apr√®s: 100 / 8 cores = **1.5s**

---

### Niveau 3: Parall√©liser Requ√™tes Neo4j ‚≠ê

**Impact:** **2-3x plus rapide** pour les sauvegardes

#### Modification: `src/knowbase/agents/extractor/orchestrator.py`

**Probl√®me:** Sauvegardes Neo4j s√©quentielles dans `_save_to_proto_kg()`

**Solution:** Utiliser transactions batch Neo4j

```python
def _save_all_to_proto_kg_batch(
    self,
    extraction_results: List[dict],
    state: AgentState
) -> None:
    """Sauvegarde tous les concepts en une seule transaction batch."""

    # Pr√©parer tous les concepts
    all_concepts = []
    for result in extraction_results:
        concepts = result.get("concepts", [])
        all_concepts.extend(concepts)

    if not all_concepts:
        return

    # Transaction batch unique avec UNWIND
    query = """
    UNWIND $concepts AS concept
    MERGE (c:ProtoConcept {
        canonical_name: concept.canonical_name,
        tenant_id: $tenant_id
    })
    SET c.concept_type = concept.concept_type,
        c.surface_form = concept.surface_form,
        c.definition = concept.definition,
        c.confidence = concept.confidence,
        c.document_id = $document_id,
        c.segment_id = concept.segment_id,
        c.updated_at = datetime()
    """

    with self.neo4j_client.session() as session:
        session.run(
            query,
            concepts=all_concepts,
            tenant_id=state.tenant_id,
            document_id=state.document_id
        )

    logger.info(f"[EXTRACTOR] ‚úÖ {len(all_concepts)} concepts saved in SINGLE batch transaction")
```

**Gain:** 100 concepts
- Avant: 100 √ó 0.05s = **5s**
- Apr√®s: 1 transaction = **0.3s**

---

## üéõÔ∏è Configuration Infrastructure

### Variables d'Environnement Optimis√©es

**Fichier:** `.env.production`

```bash
# =====================================================
# PERFORMANCE - TRAITEMENT MONO-DOCUMENT
# =====================================================

# LLM Rate Limits (augmenter pour parall√©lisation)
OPENAI_MAX_RPM=500           # Requ√™tes par minute (tier 1: 500, tier 5: 10000)
ANTHROPIC_MAX_RPM=100        # Requ√™tes par minute

# Parall√©lisation Extraction
MAX_PARALLEL_SEGMENTS=10     # Nombre de segments trait√©s en parall√®le (NOUVEAU)
                              # Limit√© par rate limits LLM

# Neo4j Connection Pool
NEO4J_MAX_CONNECTION_POOL_SIZE=50   # Connexions simultan√©es
NEO4J_MAX_TRANSACTION_RETRY_TIME=30 # Timeout transactions

# Qdrant Batch Processing
QDRANT_BATCH_SIZE=100        # Taille des batches upload
QDRANT_UPLOAD_PARALLELISM=4  # Uploads parall√®les (NOUVEAU)

# ThreadPoolExecutor
MAX_WORKER_THREADS=8         # Threads I/O (embeddings, parsing)
```

### Instance EC2 Recommand√©e

Pour maximiser la parall√©lisation **interne** d'un document :

**Optimal:** `c5.4xlarge`
- 16 vCPU (n√©cessaire pour parall√©liser 10+ segments)
- 32 GB RAM
- R√©seau: 10 Gbps
- **Co√ªt:** ~$0.68/heure

**Pourquoi plus de vCPU ?**
- 10 segments en parall√®le = 10 appels LLM simultan√©s
- Chaque appel LLM = 1 thread d'attente + parsing JSON
- Plus de vCPU = meilleure gestion concurrence asyncio

---

## üìà Gains Cumul√©s

### Sc√©nario: Document PPTX 50 Slides

**Pipeline actuel (s√©quentiel):**
```
Segmentation:        5s
Extraction (10 seg): 100s   ‚Üê GOULOT
Mining:              10s
Gatekeeper:          15s
Chunking:            10s
Embedding + Upload:  20s    ‚Üê GOULOT
----------------------------
TOTAL:               160s
```

**Pipeline optimis√© (parall√®le):**
```
Segmentation:        5s
Extraction (10 seg): 15s    ‚Üê 7x plus rapide (parall√®le)
Mining:              3s     ‚Üê 3x plus rapide (batch Neo4j)
Gatekeeper:          15s
Chunking:            5s
Embedding + Upload:  5s     ‚Üê 4x plus rapide (ThreadPool)
----------------------------
TOTAL:               48s    ‚Üê 3.3x AM√âLIORATION GLOBALE
```

**Am√©lioration:** **160s ‚Üí 48s** = **Gain de 112 secondes (70%)** üöÄ

---

## üîß Guide d'Impl√©mentation

### √âtape 1: Ajouter Parall√©lisation Extraction

**Fichier:** `src/knowbase/agents/extractor/orchestrator.py`

1. Ajouter m√©thode `_process_segment()` (voir code Niveau 1)
2. Remplacer boucle `for` par `asyncio.gather()`
3. Ajouter variable env `MAX_PARALLEL_SEGMENTS`

**Test:**
```bash
# V√©rifier parall√©lisation dans logs
docker-compose logs -f ingestion-worker | grep "Processing segment"

# Devrait afficher tous les segments presque simultan√©ment
# Au lieu de : segment 1 ‚Üí segment 2 ‚Üí segment 3...
# Voir : segment 1, 2, 3... (tous ensemble)
```

### √âtape 2: Ajouter ThreadPoolExecutor pour Embeddings

**Fichier:** `src/knowbase/agents/supervisor/supervisor.py`

1. Ajouter `ThreadPoolExecutor` pour cr√©ation points Qdrant
2. Utiliser `loop.run_in_executor()` pour I/O
3. Configurer `max_workers=8`

**Test:**
```python
import time

start = time.time()
# Votre code d'embedding...
duration = time.time() - start

logger.info(f"Embedding {len(chunks)} chunks took {duration:.2f}s")
# Avant: ~20s pour 100 chunks
# Apr√®s: ~5s pour 100 chunks
```

### √âtape 3: Optimiser Transactions Neo4j

**Fichier:** `src/knowbase/agents/extractor/orchestrator.py`

1. Remplacer `_save_to_proto_kg()` par `_save_all_to_proto_kg_batch()`
2. Utiliser `UNWIND` pour batch insert
3. Une seule transaction pour tous les concepts

**Test:**
```cypher
// V√©rifier temps d'insertion
PROFILE
UNWIND $concepts AS concept
MERGE (c:ProtoConcept {canonical_name: concept.name, tenant_id: 'default'})
SET c += concept.properties
```

### √âtape 4: Configurer Variables d'Environnement

**Fichier:** `.env.production`

```bash
# Ajuster selon vos rate limits LLM
MAX_PARALLEL_SEGMENTS=10
OPENAI_MAX_RPM=500
ANTHROPIC_MAX_RPM=100

# Optimiser I/O
MAX_WORKER_THREADS=8
NEO4J_MAX_CONNECTION_POOL_SIZE=50
QDRANT_BATCH_SIZE=100
```

### √âtape 5: D√©ployer et Tester

```powershell
# 1. Rebuild images (car modifications code)
docker-compose build app ingestion-worker

# 2. Push vers ECR
.\scripts\aws\build-and-push-ecr.ps1

# 3. D√©truire stack
.\scripts\aws\destroy-cloudformation.ps1 -StackName "knowbase-test"

# 4. Red√©ployer avec instance boost√©e
.\scripts\aws\deploy-cloudformation.ps1 `
    -StackName "knowbase-perf" `
    -InstanceType "c5.4xlarge" `
    -KeyPairName "my-key" `
    -KeyPath ".\my-key.pem"
```

### √âtape 6: Mesurer les Performances

**Script de test:**
```bash
#!/bin/bash
# test-single-doc-perf.sh

DOC_PATH="test-50-slides.pptx"
EC2_IP="<IP_EC2>"

echo "Testing single document performance..."
start_time=$(date +%s)

curl -X POST http://$EC2_IP:8000/ingest/pptx \
  -F "file=@$DOC_PATH" \
  -w "\nHTTP Status: %{http_code}\nTotal Time: %{time_total}s\n"

end_time=$(date +%s)
duration=$((end_time - start_time))

echo "Total duration: ${duration}s"
```

**M√©triques √† surveiller:**
```bash
# 1. Logs extraction parall√®le
ssh ubuntu@<IP> "docker-compose logs ingestion-worker | grep 'Processing segment' | tail -20"

# 2. Utilisation CPU (devrait √™tre √©lev√©e pendant extraction)
ssh ubuntu@<IP> "docker stats --no-stream | grep knowbase-worker"

# 3. Nombre de requ√™tes LLM simultan√©es (Redis)
ssh ubuntu@<IP> "docker exec knowbase-redis redis-cli INFO stats | grep instantaneous_ops"
```

---

## ‚ö†Ô∏è Limites et Pr√©cautions

### 1. Rate Limits LLM

**Probl√®me:** OpenAI Tier 1 = 500 RPM (requ√™tes par minute)

**Impact sur parall√©lisation:**
- 10 segments en parall√®le = 20 requ√™tes (prepass + extraction)
- Si 1 requ√™te = 10s ‚Üí 20 requ√™tes en parall√®le OK
- Si trop de requ√™tes simultan√©es ‚Üí erreur 429 (rate limit)

**Solution:**
```python
import asyncio
from asyncio import Semaphore

# Limiter nombre de requ√™tes LLM simultan√©es
MAX_CONCURRENT_LLM_CALLS = 5  # Ajuster selon tier
llm_semaphore = Semaphore(MAX_CONCURRENT_LLM_CALLS)

async def _process_segment_with_limit(self, segment, ...):
    async with llm_semaphore:
        # Seulement 5 appels LLM simultan√©s max
        result = await self._process_segment(segment, ...)
    return result
```

### 2. M√©moire RAM

**Probl√®me:** 10 segments en parall√®le = 10√ó la m√©moire

**Recommandation:**
- c5.4xlarge (32 GB RAM) : 10-15 segments parall√®les OK
- t3.2xlarge (32 GB RAM) : 5-8 segments max

**Monitoring:**
```bash
# Surveiller utilisation m√©moire
watch -n 1 'docker stats --no-stream | grep knowbase-worker'
```

### 3. Co√ªts LLM

**Attention:** Parall√©lisation = plus de requ√™tes/minute

**Impact co√ªts:** Si vous avez 100 documents/jour :
- Avant: Traitement √©tal√© sur 3-4 heures
- Apr√®s: Traitement concentr√© sur 1 heure ‚Üí plus de RPM

**V√©rifier quotas:**
- OpenAI: https://platform.openai.com/account/limits
- Anthropic: https://console.anthropic.com/settings/limits

---

## üéØ R√©sum√© Configuration Recommand√©e

### Pour c5.4xlarge (16 vCPU, 32 GB RAM)

**docker-compose.ecr.yml:**
```yaml
ingestion-worker:
  # PAS de replicas (1 seul worker pour traiter 1 doc √† la fois)
  deploy:
    resources:
      limits:
        cpus: '12.0'  # Allouer plus de CPU pour parall√©lisation interne
        memory: 16G   # Plus de RAM pour segments en parall√®le
```

**.env.production:**
```bash
# Parall√©lisation INTERNE d'un document
MAX_PARALLEL_SEGMENTS=10
MAX_WORKER_THREADS=8

# Rate limits
OPENAI_MAX_RPM=500
ANTHROPIC_MAX_RPM=100

# Neo4j & Qdrant optimis√©s
NEO4J_MAX_CONNECTION_POOL_SIZE=50
QDRANT_BATCH_SIZE=100
```

**Modifications code:**
- ‚úÖ `extractor/orchestrator.py` : `asyncio.gather()` pour segments
- ‚úÖ `supervisor/supervisor.py` : `ThreadPoolExecutor` pour embeddings
- ‚úÖ `extractor/orchestrator.py` : Batch Neo4j avec `UNWIND`

**Gain attendu:** **3-4x plus rapide** pour un document unique üöÄ

---

## üìö R√©f√©rences

- [asyncio.gather() documentation](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather)
- [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)
- [Neo4j Batch Operations](https://neo4j.com/docs/cypher-manual/current/clauses/unwind/)
- [OpenAI Rate Limits](https://platform.openai.com/docs/guides/rate-limits)

---

**Auteur:** Claude Code
**Version:** 1.0
**Prochaine √©tape:** Impl√©menter parall√©lisation extraction (Niveau 1) pour gain imm√©diat de 5-7x
