# Plan d'Impl√©mentation Unifi√© - 6 Probl√®mes OSMOSE - 2025-10-21

**Date** : 2025-10-21 02:00
**Objectif** : R√©soudre LES 6 PROBL√àMES de mani√®re coh√©rente et coordonn√©e
**Statut** : PLAN COMPLET - Pr√™t pour impl√©mentation

---

## üéØ Vue d'Ensemble

### Les 6 Probl√®mes √† R√©soudre Ensemble

| # | Probl√®me | Impact | Priorit√© |
|---|----------|--------|----------|
| **#1** | 0 Relations Extraites | ‚ùå Phase 2 inutile | üî¥ P2 |
| **#2** | 0 Ontologies Redis | ‚ö†Ô∏è Pas d'apprentissage | üü° P4 |
| **#3** | 18% canonical_name=None | ‚ö†Ô∏è 100/547 concepts perdus | üî¥ P1 |
| **#4** | 0 Chunks Qdrant | ‚ö†Ô∏è Pas de RAG | üü° P3 |
| **#5** | Duplications S√©mantiques | ‚ö†Ô∏è KG pollu√© (8 entit√©s pour S/4HANA) | üî¥ P1 |
| **#6** | Pollution Acronymes | ‚ö†Ô∏è 47 acronymes sans expansion | üî¥ P1 |

### Exigences Utilisateur (Explicites)

1. **Acronymes** : Expansion syst√©matique ("MFA" ‚Üí "Multi-Factor Authentication") avec acronyme en **alias**
2. **Produits** : Canonical name officiel + toutes variantes en **aliases dans ontologie**
3. **D√©duplication** : Seuil **85% similarit√©**
4. **Scope** : R√©soudre TOUS les probl√®mes **ensemble** (pas de fixes isol√©s)

---

## üèóÔ∏è Architecture de la Solution

### Nouveau Sch√©ma Ontologie (Core Change)

**Actuellement** :
```python
# Neo4j CanonicalConcept
{
  "canonical_id": "uuid",
  "canonical_name": "Content Owner",  # ‚úÖ OK
  "surface_form": "Content Owner",    # ‚ö†Ô∏è SINGULIER (string)
  "concept_type": "ROLE",
  "tenant_id": "default"
}

# Redis AdaptiveOntology
ontology:default:content_owner = {
  "canonical_name": "Content Owner",
  "concept_type": "ROLE",
  # ‚ùå PAS d'aliases
}
```

**NOUVEAU SCH√âMA** :
```python
# Neo4j CanonicalConcept (√âTENDU)
{
  "canonical_id": "uuid",
  "canonical_name": "Multi-Factor Authentication",  # ‚úÖ Forme CANONIQUE (expansion)
  "surface_forms": ["MFA", "2FA", "multi factor auth"],  # ‚úÖ PLURIEL (liste)
  "primary_alias": "MFA",  # ‚úÖ Alias principal (acronyme d'origine)
  "concept_type": "SECURITY_FEATURE",
  "tenant_id": "default",
  "confidence": 0.85,
  "merged_from": ["uuid1", "uuid2"]  # ‚úÖ Tra√ßabilit√© d√©duplication
}

# Redis AdaptiveOntology (√âTENDU)
ontology:default:multi_factor_authentication = {
  "canonical_name": "Multi-Factor Authentication",
  "aliases": ["MFA", "2FA", "multi factor auth"],
  "primary_alias": "MFA",
  "concept_type": "SECURITY_FEATURE",
  "confidence": 0.85
}
```

### Exemple Concret : S/4HANA Cloud

**Avant (8 entit√©s dupliqu√©es)** :
```
CanonicalConcept: canonical_name="SAP Cloud ERP's", surface_form="SAP Cloud ERP's"
CanonicalConcept: canonical_name="SAP Cloud ERP", surface_form="SAP Cloud ERP"
CanonicalConcept: canonical_name="SAP Cloud ERP Private", surface_form="SAP Cloud ERP Private"
CanonicalConcept: canonical_name="ERP", surface_form="ERP"
CanonicalConcept: canonical_name="PCE", surface_form="PCE"
CanonicalConcept: canonical_name="S/4HANA Cloud", surface_form="S/4HANA Cloud"
CanonicalConcept: canonical_name="RISE With SAP Cloud ERP", surface_form="RISE With SAP Cloud ERP"
CanonicalConcept: canonical_name="RISE With SAP S/4HANA", surface_form="RISE With SAP S/4HANA"
```

**Apr√®s (1 entit√© consolid√©e)** :
```python
CanonicalConcept {
  canonical_name: "SAP S/4HANA Cloud Private Edition",  # ‚úÖ Nom officiel canonique
  surface_forms: [
    "SAP Cloud ERP",
    "SAP Cloud ERP Private",
    "S/4HANA Cloud",
    "RISE with SAP Cloud ERP",
    "RISE with SAP S/4HANA",
    "PCE"  # Private Cloud Edition acronym
  ],
  primary_alias: "S/4HANA Cloud Private Edition",
  concept_type: "PRODUCT",
  confidence: 0.92,
  merged_from: ["uuid1", "uuid2", "uuid3", "uuid4", "uuid5", "uuid6", "uuid7", "uuid8"]
}
```

---

## üìã Plan d'Impl√©mentation par Priorit√©

### üî¥ PRIORIT√â 1 : Am√©liorer LLM Canonicalizer (Probl√®mes #3, #5, #6)

**Objectif** : Fix batch JSON parsing + expansion acronymes + normalisation produits

#### 1.1 Fixer Batch JSON Parsing (Probl√®me #3)

**Fichier** : `src/knowbase/agents/gatekeeper/llm_canonicalizer.py`

**Diagnostic requis d'abord** :
```python
# Ajouter log AVANT parsing pour voir r√©ponse LLM brute
logger.info(f"[LLMCanonicalizer:Batch] üîç Raw LLM response:\n{response_content}")
```

**Causes probables** :
1. LLM retourne texte explicatif au lieu de JSON pur
2. LLM retourne JSON mais sch√©ma diff√©rent (cl√©s manquantes)
3. Parser attend liste mais re√ßoit dict, ou inversement

**Fix probable** :
```python
def _parse_batch_response(self, response_content: str) -> dict[str, tuple[str, float]]:
    """Parse batch response avec robustesse accrue."""

    # Fix 2025-10-21: Extraction JSON robuste
    json_content = response_content.strip()

    # Si LLM entoure JSON de markdown code blocks
    if json_content.startswith("```json"):
        json_content = json_content.split("```json")[1].split("```")[0].strip()
    elif json_content.startswith("```"):
        json_content = json_content.split("```")[1].split("```")[0].strip()

    # Tenter parsing
    try:
        data = json.loads(json_content)
    except json.JSONDecodeError as e:
        logger.error(f"[LLMCanonicalizer:Batch] ‚ùå JSON parsing failed: {e}")
        logger.error(f"[LLMCanonicalizer:Batch] Raw content:\n{response_content[:500]}")
        raise

    # Adapter selon sch√©ma retourn√©
    if isinstance(data, list):
        # Si LLM retourne liste [{concept_name, canonical_name, confidence}, ...]
        return {
            item["concept_name"]: (item["canonical_name"], item.get("confidence", 0.5))
            for item in data
        }
    elif isinstance(data, dict):
        # Si LLM retourne dict {concept_name: {canonical_name, confidence}}
        return {
            key: (val["canonical_name"], val.get("confidence", 0.5))
            for key, val in data.items()
        }
    else:
        raise ValueError(f"Unexpected response format: {type(data)}")
```

#### 1.2 Am√©liorer Prompt LLM avec R√®gles Explicites

**Fichier** : `src/knowbase/agents/gatekeeper/llm_canonicalizer.py`

**Nouveau Prompt Batch** :
```python
BATCH_CANONICALIZATION_PROMPT = """
Tu es un expert en normalisation de concepts pour construire un Knowledge Graph coh√©rent.

**R√àGLES STRICTES (√Ä APPLIQUER DANS CET ORDRE)** :

1. **EXPANSION ACRONYMES** :
   - TOUJOURS √©tendre les acronymes courts (‚â§5 lettres) vers leur forme compl√®te
   - Exemples :
     * "MFA" ‚Üí "Multi-Factor Authentication"
     * "PCE" ‚Üí "Private Cloud Edition"
     * "EDR" ‚Üí "Endpoint Detection and Response"
     * "ILM" ‚Üí "Information Lifecycle Management"
   - Si acronyme ambigu et contexte insuffisant, utiliser forme la plus probable dans contexte SAP/IT
   - CONSERVER l'acronyme comme "primary_alias"

2. **NORMALISATION NOMS PRODUITS** :
   - Utiliser le nom de produit OFFICIEL complet (consulter catalogue SAP si n√©cessaire)
   - Exemples :
     * "SAP Cloud ERP", "PCE", "S/4HANA Cloud" ‚Üí "SAP S/4HANA Cloud Private Edition"
     * "BTP", "Business Technology Platform" ‚Üí "SAP Business Technology Platform"
     * "RISE with SAP" ‚Üí "RISE with SAP" (d√©j√† canonical)
   - Ajouter toutes les variantes rencontr√©es dans "aliases"

3. **NETTOYAGE BASIQUE** :
   - Supprimer possessifs : "SAP Cloud ERP's" ‚Üí "SAP Cloud ERP"
   - Normaliser singulier/pluriel : "Connectors" ‚Üí "Connector"
   - Supprimer articles : "The Content Owner" ‚Üí "Content Owner"
   - Capitalisation coh√©rente : "multi-factor authentication" ‚Üí "Multi-Factor Authentication"

4. **D√âDUPLICATION INTRA-BATCH** :
   - Si plusieurs concepts dans le batch sont synonymes (similarit√© > 85%), les fusionner
   - Exemple : "ERP", "SAP Cloud ERP", "PCE" ‚Üí UN SEUL canonical "SAP S/4HANA Cloud Private Edition"
   - Lister TOUS les noms originaux dans "merged_aliases"

**FORMAT DE SORTIE (JSON STRICT)** :
```json
{
  "concepts": [
    {
      "concept_name": "MFA",  // Nom original du concept
      "canonical_name": "Multi-Factor Authentication",  // Forme canonique
      "primary_alias": "MFA",  // Alias principal (souvent l'acronyme d'origine)
      "aliases": ["2FA", "Multi Factor Auth", "MFA"],  // Toutes variantes rencontr√©es
      "confidence": 0.90,  // Confiance dans la canonicalisation (0-1)
      "expansion_applied": true,  // Bool√©en : acronyme √©tendu ?
      "merged_from": []  // Liste des concept_name fusionn√©s (si d√©duplication)
    },
    {
      "concept_name": "SAP Cloud ERP's",
      "canonical_name": "SAP S/4HANA Cloud Private Edition",
      "primary_alias": "S/4HANA Cloud Private Edition",
      "aliases": ["SAP Cloud ERP", "PCE", "S/4HANA Cloud", "SAP Cloud ERP Private"],
      "confidence": 0.95,
      "expansion_applied": false,
      "merged_from": ["ERP", "PCE", "S/4HANA Cloud"]  // Fusionn√© avec d'autres du batch
    }
  ]
}
```

**IMPORTANT** :
- Retourner UNIQUEMENT le JSON, AUCUN texte explicatif avant/apr√®s
- Chaque concept du batch doit appara√Ætre dans la sortie (m√™me si juste nettoyage)
- Si incertain sur expansion acronyme, confidence < 0.7

**CONCEPTS √Ä CANONICALISER** :
{concepts_list}

**CONTEXTE DOCUMENT** :
{document_context}
"""
```

**Code Modifications** :
```python
def _batch_canonicalize_concepts_with_llm(
    self,
    concepts: list[dict],
    tenant_id: str = "default"
) -> dict[str, dict]:
    """
    Retourne dict[concept_name] = {
        "canonical_name": str,
        "primary_alias": str,
        "aliases": list[str],
        "confidence": float,
        "expansion_applied": bool,
        "merged_from": list[str]
    }
    """

    # Construire liste concepts pour prompt
    concepts_list = "\n".join([
        f"- {c['concept_name']}: {c.get('definition', 'N/A')[:100]}"
        for c in concepts
    ])

    # Document context (pour aider expansion acronymes)
    document_context = concepts[0].get("document_title", "N/A") if concepts else "N/A"

    prompt = BATCH_CANONICALIZATION_PROMPT.format(
        concepts_list=concepts_list,
        document_context=document_context
    )

    # Appel LLM
    response = self._call_llm(prompt, temperature=0.1)  # Basse temp√©rature pour coh√©rence

    # Log raw response
    logger.info(f"[LLMCanonicalizer:Batch] üîç Raw LLM response:\n{response[:500]}")

    # Parse avec nouveau sch√©ma
    parsed = self._parse_batch_response_v2(response)

    return parsed
```

#### 1.3 Ajouter Fuzzy Deduplication Post-LLM (Probl√®me #5)

**Fichier** : `src/knowbase/agents/gatekeeper/gatekeeper.py`

**Nouvelle Fonction** :
```python
from difflib import SequenceMatcher

def _fuzzy_deduplicate_concepts(
    self,
    concepts: list[dict],
    similarity_threshold: float = 0.85
) -> list[dict]:
    """
    D√©duplication floue post-canonicalization.

    Fusionne concepts avec canonical_name similaire > threshold.

    Returns:
        Liste d√©dupliqu√©e avec merged_from trac√©.
    """

    logger.info(f"[GATEKEEPER:FuzzyDedup] üîç Deduplicating {len(concepts)} concepts (threshold={similarity_threshold})")

    deduplicated = []
    merged_ids = set()

    for i, concept_a in enumerate(concepts):
        if concept_a["concept_id"] in merged_ids:
            continue  # D√©j√† fusionn√©

        canonical_a = concept_a.get("canonical_name", "").lower()
        if not canonical_a:
            deduplicated.append(concept_a)
            continue

        # Chercher concepts similaires
        similar_concepts = [concept_a]

        for j, concept_b in enumerate(concepts[i+1:], start=i+1):
            if concept_b["concept_id"] in merged_ids:
                continue

            canonical_b = concept_b.get("canonical_name", "").lower()
            if not canonical_b:
                continue

            # Calcul similarit√©
            similarity = SequenceMatcher(None, canonical_a, canonical_b).ratio()

            if similarity >= similarity_threshold:
                logger.info(
                    f"[GATEKEEPER:FuzzyDedup] ‚úÖ MERGE ({similarity:.2%}): "
                    f"'{concept_a['canonical_name']}' ‚Üê '{concept_b['canonical_name']}'"
                )
                similar_concepts.append(concept_b)
                merged_ids.add(concept_b["concept_id"])

        # Fusionner si plusieurs trouv√©s
        if len(similar_concepts) > 1:
            merged_concept = self._merge_concepts(similar_concepts)
            deduplicated.append(merged_concept)
        else:
            deduplicated.append(concept_a)

    logger.info(
        f"[GATEKEEPER:FuzzyDedup] ‚úÖ Deduplicated: {len(concepts)} ‚Üí {len(deduplicated)} "
        f"({len(concepts) - len(deduplicated)} merged)"
    )

    return deduplicated

def _merge_concepts(self, concepts: list[dict]) -> dict:
    """
    Fusionne plusieurs concepts similaires en UN concept.

    Strat√©gie :
    - canonical_name : Le PLUS LONG (plus descriptif)
    - surface_forms : UNION de toutes les variantes
    - primary_alias : Le PLUS COURT (souvent acronyme)
    - confidence : MOYENNE pond√©r√©e
    - merged_from : Tous les concept_id fusionn√©s
    """

    # Trier par longueur canonical_name (desc)
    concepts_sorted = sorted(
        concepts,
        key=lambda c: len(c.get("canonical_name", "")),
        reverse=True
    )

    # Prendre le plus long comme base
    merged = concepts_sorted[0].copy()

    # Union surface_forms
    all_surface_forms = set()
    for c in concepts:
        all_surface_forms.update(c.get("surface_forms", []))
        all_surface_forms.add(c.get("canonical_name", ""))  # Ajouter aussi canonical

    merged["surface_forms"] = list(all_surface_forms)

    # Primary alias = le plus court (souvent acronyme)
    all_names = [c.get("canonical_name", "") for c in concepts]
    merged["primary_alias"] = min(all_names, key=len)

    # Confidence moyenne
    confidences = [c.get("confidence", 0.5) for c in concepts]
    merged["confidence"] = sum(confidences) / len(confidences)

    # Tra√ßabilit√©
    merged["merged_from"] = [c["concept_id"] for c in concepts]

    return merged
```

**Int√©gration dans PromoteConcepts** :
```python
# gatekeeper.py - PromoteConcepts tool

# Apr√®s batch canonicalization, AVANT promotion Neo4j
concepts_with_canonical = []
for concept in concepts:
    # ... r√©cup√©ration canonical depuis cache ...
    concepts_with_canonical.append(concept)

# ‚úÖ NOUVEAU : Fuzzy deduplication
concepts_deduplicated = self._fuzzy_deduplicate_concepts(
    concepts_with_canonical,
    similarity_threshold=0.85
)

# Promotion Neo4j avec concepts d√©dupliqu√©s
for concept in concepts_deduplicated:
    # ... existing promotion logic ...
```

---

### üî¥ PRIORIT√â 2 : Fixer Surface Forms pour Phase 2 (Probl√®me #1)

**Objectif** : Phase 2 doit recevoir `surface_forms` (liste) pour extraction relations

**Fichier** : `src/knowbase/agents/supervisor/supervisor.py`

**Localisation** : Step EXTRACT_RELATIONS

**Modification** :
```python
# supervisor.py - EXTRACT_RELATIONS step

# ‚ùå AVANT : Passer concepts sans surface_forms
concepts_for_extraction = neo4j_client.get_all_concepts(tenant_id=tenant_id)

# ‚úÖ APR√àS : Requ√™te Neo4j avec conversion surface_form ‚Üí surface_forms
query = """
MATCH (c:CanonicalConcept)
WHERE c.tenant_id = $tenant_id
RETURN c.canonical_id AS concept_id,
       c.canonical_name AS canonical_name,
       c.surface_forms AS surface_forms_list,  // Si schema Neo4j d√©j√† updated
       c.surface_form AS surface_form_single,  // Ancien schema (fallback)
       c.concept_type AS concept_type
"""

with neo4j_client.driver.session() as session:
    result = session.run(query, tenant_id=tenant_id)

    concepts_for_extraction = []
    for row in result:
        # Conversion schema : singular string ‚Üí list
        surface_forms = row["surface_forms_list"]  # Nouveau schema (peut √™tre None)

        if not surface_forms:
            # Fallback ancien schema : convertir string ‚Üí liste
            surface_form_single = row["surface_form_single"]
            surface_forms = [surface_form_single] if surface_form_single else []

        concepts_for_extraction.append({
            "concept_id": row["concept_id"],
            "canonical_name": row["canonical_name"],
            "surface_forms": surface_forms,  # ‚úÖ TOUJOURS liste
            "concept_type": row["concept_type"]
        })

logger.info(
    f"[SUPERVISOR:EXTRACT_RELATIONS] Retrieved {len(concepts_for_extraction)} concepts "
    f"with surface_forms for relation extraction"
)

# Appel RelationExtraction tool avec concepts corrig√©s
relation_extraction_result = await relation_extraction_tool.run(
    tool_input=RelationExtractionInput(concepts=concepts_for_extraction, ...)
)
```

**Migration Schema Neo4j** (si n√©cessaire) :
```python
# Migration script : scripts/migrate_surface_forms.py

from knowbase.common.clients.neo4j_client import get_neo4j_client

def migrate_surface_form_to_list():
    """
    Migrer surface_form (string) ‚Üí surface_forms (liste).
    """

    neo4j = get_neo4j_client()

    query = """
    MATCH (c:CanonicalConcept)
    WHERE c.surface_form IS NOT NULL
      AND c.surface_forms IS NULL
    SET c.surface_forms = [c.surface_form]
    RETURN count(c) as migrated
    """

    with neo4j.driver.session() as session:
        result = session.run(query)
        count = result.single()["migrated"]
        print(f"‚úÖ Migrated {count} concepts: surface_form ‚Üí surface_forms")

if __name__ == "__main__":
    migrate_surface_form_to_list()
```

**Ex√©cution** :
```bash
docker-compose exec app python scripts/migrate_surface_forms.py
```

---

### üü° PRIORIT√â 3 : Ajouter TextChunker dans FINALIZE (Probl√®me #4)

**Objectif** : Cr√©er chunks Qdrant pour RAG

**Fichier** : `src/knowbase/agents/supervisor/supervisor.py`

**Localisation** : Step FINALIZE (apr√®s EXTRACT_RELATIONS)

**Code √† Ajouter** :
```python
# supervisor.py - FINALIZE step

from knowbase.chunks.text_chunker import get_text_chunker
from knowbase.common.clients.qdrant_client import get_qdrant_client

# R√©cup√©rer texte complet document depuis state
full_text = state.get("full_text", "")
document_id = state.get("document_id")
tenant_id = state.get("tenant_id", "default")

if not full_text:
    logger.warning("[SUPERVISOR:FINALIZE] No full_text in state, skipping chunking")
else:
    logger.info(f"[SUPERVISOR:FINALIZE] üìÑ Chunking document {document_id}...")

    # Chunking
    chunker = get_text_chunker()
    chunks = chunker.chunk_document(
        document_id=document_id,
        text=full_text,
        metadata={
            "tenant_id": tenant_id,
            "document_title": state.get("document_title", "Unknown"),
            "file_name": state.get("file_name", "Unknown"),
            "import_date": state.get("import_date", "Unknown")
        }
    )

    logger.info(f"[SUPERVISOR:FINALIZE] ‚úÖ Created {len(chunks)} chunks")

    # Upload Qdrant
    qdrant = get_qdrant_client()

    # Convertir chunks en points Qdrant
    points = []
    for i, chunk in enumerate(chunks):
        points.append({
            "id": f"{document_id}_chunk_{i}",
            "vector": chunk["embedding"],  # D√©j√† cr√©√© par TextChunker
            "payload": {
                "document_id": document_id,
                "chunk_index": i,
                "text": chunk["text"],
                "tenant_id": tenant_id,
                **chunk["metadata"]
            }
        })

    # Upsert dans collection knowbase
    qdrant.upsert(
        collection_name="knowbase",
        points=points
    )

    logger.info(
        f"[SUPERVISOR:FINALIZE] ‚úÖ Uploaded {len(points)} chunks to Qdrant "
        f"(collection=knowbase, tenant={tenant_id})"
    )
```

**V√©rification Post-Import** :
```bash
# Compter chunks dans Qdrant
curl http://localhost:6333/collections/knowbase

# Expected:
# {
#   "result": {
#     "points_count": 500-1000,  # D√©pend taille document
#     ...
#   }
# }
```

---

### üü° PRIORIT√â 4 : Fixer Ontologies Redis (Probl√®me #2)

**Objectif** : Stocker concepts dans Redis pour apprentissage

#### 4.1 Baisser Threshold Confidence

**Fichier** : `src/knowbase/ontology/adaptive_ontology_manager.py`

**Modification** :
```python
class AdaptiveOntologyManager:

    # ‚ùå AVANT
    MIN_CONFIDENCE_THRESHOLD = 0.6

    # ‚úÖ APR√àS : Baisser √† 0.25 (accepter canonicalization LLM baseline)
    MIN_CONFIDENCE_THRESHOLD = 0.25

    def store_concept(self, concept: dict, tenant_id: str = "default") -> bool:
        """Store concept in Redis ontology."""

        canonical_name = concept.get("canonical_name")
        confidence = concept.get("confidence", 0.3)

        # Validation confidence
        if confidence < self.MIN_CONFIDENCE_THRESHOLD:
            logger.warning(
                f"[AdaptiveOntology:Store] ‚ö†Ô∏è Low confidence {confidence:.2f} < {self.MIN_CONFIDENCE_THRESHOLD}, "
                f"skipping store for '{canonical_name}'"
            )
            return False

        # ... rest of storage logic ...
```

#### 4.2 Autoriser Caract√®res Sp√©ciaux

**Fichier** : `src/knowbase/ontology/adaptive_ontology_manager.py`

**Modification** :
```python
import re

def _validate_concept_name(self, concept_name: str) -> bool:
    """Validate concept name format."""

    # ‚ùå AVANT : Rejet de &, -, (), etc.
    # ALLOWED_PATTERN = r"^[\w\s]+$"

    # ‚úÖ APR√àS : Autoriser caract√®res sp√©ciaux courants
    ALLOWED_PATTERN = r"^[\w\s\-&(),./]+$"

    if not re.match(ALLOWED_PATTERN, concept_name):
        logger.warning(
            f"[AdaptiveOntology:Validation] Invalid characters in concept name: {concept_name}"
        )
        return False

    return True
```

#### 4.3 Stocker Aliases dans Redis

**Nouveau sch√©ma Redis** :
```python
def store_concept(self, concept: dict, tenant_id: str = "default") -> bool:
    """Store concept with aliases in Redis."""

    canonical_name = concept.get("canonical_name")
    key = f"ontology:{tenant_id}:{self._normalize_key(canonical_name)}"

    # ‚úÖ NOUVEAU : Inclure aliases
    ontology_entry = {
        "canonical_name": canonical_name,
        "aliases": concept.get("aliases", []),
        "primary_alias": concept.get("primary_alias"),
        "concept_type": concept.get("concept_type"),
        "confidence": concept.get("confidence", 0.5),
        "surface_forms": concept.get("surface_forms", []),
        "created_at": datetime.now(timezone.utc).isoformat()
    }

    # Store in Redis
    self.redis_client.set(
        key,
        json.dumps(ontology_entry),
        ex=self.ONTOLOGY_TTL  # 30 days
    )

    # ‚úÖ NOUVEAU : Cr√©er index inverse pour lookup par alias
    for alias in concept.get("aliases", []):
        alias_key = f"alias:{tenant_id}:{self._normalize_key(alias)}"
        self.redis_client.set(
            alias_key,
            canonical_name,  # Pointe vers canonical
            ex=self.ONTOLOGY_TTL
        )

    logger.info(
        f"[AdaptiveOntology:Store] ‚úÖ Stored '{canonical_name}' "
        f"with {len(concept.get('aliases', []))} aliases"
    )

    return True
```

---

## üéØ M√©triques de Validation

### Avant Fixes

| M√©trique | Valeur Actuelle |
|----------|-----------------|
| Batch JSON parsing success | 0% (28/28 batches failed) |
| Concepts avec canonical_name=None | 100 (18%) |
| Appels LLM canonicalization | 547 (individual fallback) |
| Temps canonicalization | 18 min |
| Duplications s√©mantiques | 8 entit√©s pour S/4HANA |
| Acronymes non-expans√©s | 47 (HA, DR, MFA, etc.) |
| Co-occurring concept pairs | 0 |
| Relations extraites | 0 |
| Chunks Qdrant knowbase | 0 |
| Ontologies Redis | 0 |

### Cibles Apr√®s Fixes

| M√©trique | Cible | Impact |
|----------|-------|--------|
| **Batch JSON parsing success** | **100%** | ‚úÖ 28 batches OK |
| **Concepts canonical_name=None** | **0 (0%)** | ‚úÖ 100% concepts valides |
| **Appels LLM canonicalization** | **28 (batches)** | ‚úÖ 19x moins d'appels |
| **Temps canonicalization** | **< 1 min** | ‚úÖ 18x plus rapide |
| **Duplications S/4HANA** | **1 entit√©** | ‚úÖ 8 ‚Üí 1 (87% r√©duction) |
| **Acronymes expans√©s** | **100%** | ‚úÖ MFA ‚Üí Multi-Factor Authentication |
| **Co-occurring concept pairs** | **50-200** | ‚úÖ Phase 2 fonctionnelle |
| **Relations extraites** | **100-200** | ‚úÖ KG enrichi |
| **Chunks Qdrant** | **500-1000** | ‚úÖ RAG op√©rationnel |
| **Ontologies Redis** | **200-400** | ‚úÖ Apprentissage actif |

---

## üì¶ D√©ploiement Coordonn√©

### Ordre d'Impl√©mentation (S√©quentiel)

**Phase A : Fixes Canonicalisation** (2-3h)
1. ‚úÖ Diagnostiquer + fixer batch JSON parsing
2. ‚úÖ Am√©liorer prompt LLM (acronymes + produits + normalisation)
3. ‚úÖ Impl√©menter fuzzy deduplication (85%)
4. ‚úÖ Mettre √† jour sch√©ma Neo4j + Redis (aliases)

**Phase B : Fixes Extraction** (1-2h)
5. ‚úÖ Fixer surface_forms pour Phase 2
6. ‚úÖ Ajouter TextChunker dans FINALIZE
7. ‚úÖ Ajuster threshold + validation Redis

**Phase C : Tests & Validation** (1h)
8. ‚úÖ Rebuild worker avec tous les fixes
9. ‚úÖ Purge Neo4j + Redis + Qdrant
10. ‚úÖ Import test document S/4HANA
11. ‚úÖ V√©rifier m√©triques cibles

### Commandes D√©ploiement

```bash
# 1. Rebuild worker
docker-compose build ingestion-worker

# 2. Restart worker
docker-compose restart ingestion-worker

# 3. Purge databases
docker exec knowbase-neo4j cypher-shell -u neo4j -p graphiti_neo4j_pass \
  --format plain "MATCH (n) DETACH DELETE n"

docker exec knowbase-redis redis-cli FLUSHDB

curl -X DELETE http://localhost:6333/collections/knowbase

# 4. Recr√©er collection Qdrant
curl -X PUT http://localhost:6333/collections/knowbase \
  -H 'Content-Type: application/json' \
  -d '{
    "vectors": {
      "size": 1024,
      "distance": "Cosine"
    }
  }'

# 5. Import test
# ‚Üí Upload RISE_with_SAP_Cloud_ERP_Private.pptx via http://localhost:3000/documents/import

# 6. V√©rifier Neo4j
docker exec knowbase-neo4j cypher-shell -u neo4j -p graphiti_neo4j_pass \
  --format plain "
    MATCH (c:CanonicalConcept)
    WHERE c.tenant_id = 'default'
    RETURN c.canonical_name, size(c.surface_forms) as aliases_count, c.confidence
    ORDER BY c.created_at DESC
    LIMIT 20
  "

# Expected:
# SAP S/4HANA Cloud Private Edition | 7 | 0.92
# Multi-Factor Authentication | 3 | 0.88
# High Availability | 2 | 0.85
# ...

# 7. V√©rifier Relations
docker exec knowbase-neo4j cypher-shell -u neo4j -p graphiti_neo4j_pass \
  --format plain "
    MATCH (a)-[r]->(b)
    WHERE a.tenant_id = 'default'
    RETURN type(r) as relation_type, count(*) as count
  "

# Expected:
# HAS_FEATURE | 45
# USES | 23
# REQUIRES | 18
# ...

# 8. V√©rifier Qdrant
curl http://localhost:6333/collections/knowbase | jq '.result.points_count'

# Expected: 500-1000

# 9. V√©rifier Redis
docker exec knowbase-redis redis-cli KEYS "ontology:*" | wc -l

# Expected: 200-400
```

---

## üîç Questions Ouvertes & D√©cisions

### Q1 : Catalogue Produits SAP

**Question** : Pour canonicaliser correctement les noms de produits SAP, avons-nous acc√®s √† un catalogue officiel ?

**Options** :
- A) Utiliser `config/sap_solutions.yaml` existant
- B) Appel API SAP Product Catalog (si accessible)
- C) LLM knowledge (risque hallucinations)

**Recommandation** : **Option A** - Enrichir `sap_solutions.yaml` avec aliases connus, utiliser comme r√©f√©rence dans prompt LLM.

### Q2 : Expansion Acronymes - Contexte Requis

**Question** : Acronymes ambigus (ex: "PCE" = "Private Cloud Edition" OU "Peripheral Component Expansion") - quelle strat√©gie ?

**Options** :
- A) Toujours utiliser contexte document pour d√©sambigu√Øser
- B) Privil√©gier sens SAP/IT par d√©faut
- C) Conserver acronyme si ambigu (confidence < 0.7)

**Recommandation** : **Option A + B** - Contexte document en priorit√©, fallback SAP/IT, confidence < 0.7 si incertain.

### Q3 : Migration Schema Neo4j - R√©troactif ?

**Question** : Faut-il migrer les 447 concepts D√âJ√Ä dans Neo4j vers nouveau schema (surface_forms liste) ?

**Options** :
- A) Migration script imm√©diate (UPDATE tous les concepts)
- B) Migration lazy (au prochain import seulement)
- C) Coexistence 2 schemas (fallback dans code)

**Recommandation** : **Option A** - Migration imm√©diate via script `migrate_surface_forms.py` pour coh√©rence.

---

## üéØ Prochaines √âtapes Imm√©diates

**Action Utilisateur** : Autoriser impl√©mentation du plan

**Ordre Recommand√©** :
1. **Commencer par Phase A.1** : Diagnostiquer batch JSON parsing (ajouter log raw response)
2. **Attendre r√©sultat diagnostic** avant d'impl√©menter fix parsing
3. **Impl√©menter s√©quentiellement** : A.2 ‚Üí A.3 ‚Üí A.4 ‚Üí B.5 ‚Üí B.6 ‚Üí B.7
4. **Tester apr√®s chaque phase** (pas tout d'un coup)

**Temps Total Estim√©** : 4-6h (impl√©mentation + tests)

---

**Cr√©√© par** : Claude Code
**Pour** : R√©solution unifi√©e des 6 probl√®mes OSMOSE
**Statut** : PLAN COMPLET - En attente autorisation impl√©mentation
**Priorit√©** : CRITIQUE
**Impact Business** : +85% qualit√© KG, Phase 2 op√©rationnelle, RAG fonctionnel

