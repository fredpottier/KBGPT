# üöÄ Sp√©cifications Architecture Zero-Config + Self-Learning

**Projet:** KnowWhere (OSMOSE)
**Version:** 2.0 - Architecture Autonome
**Date:** 2025-10-17
**Objectif:** √âliminer toute configuration initiale tout en maintenant qualit√© production

---

## üìã Table des Mati√®res

1. [Vision et Principes](#1-vision-et-principes)
2. [Architecture Actuelle vs Cible](#2-architecture-actuelle-vs-cible)
3. [Composants Techniques](#3-composants-techniques)
4. [Sp√©cifications D√©taill√©es](#4-sp√©cifications-d√©taill√©es)
5. [Plan de Migration](#5-plan-de-migration)
6. [M√©triques de Succ√®s](#6-m√©triques-de-succ√®s)
7. [Annexes Techniques](#7-annexes-techniques)

---

## 1Ô∏è‚É£ Vision et Principes

### üéØ Vision Produit

> **"Une solution qui comprend VOTRE m√©tier sans configuration - Plus vous l'utilisez, plus elle devient pr√©cise"**

**Diff√©renciation march√©:**
- Microsoft Copilot / Google Gemini : G√©n√©ralistes, z√©ro m√©moire m√©tier
- **KnowWhere** : Sp√©cialis√© documentaire + Ontologie adaptive qui s'enrichit

### üé® Principes de Design

#### 1. **Zero-Config by Default**
```
Installation ‚Üí Upload Document ‚Üí Extraction Imm√©diate
     ‚Üì              ‚Üì                    ‚Üì
  0 minutes    1 action          R√©sultats en 30s
```

**Pas de:**
- ‚ùå Formulaires configuration
- ‚ùå Catalogues √† remplir
- ‚ùå Domaines √† d√©finir
- ‚ùå Ontologies √† importer

#### 2. **Self-Improving with Usage**
```
Documents 1-10   ‚Üí Qualit√© 80-85% (LLM pur)
Documents 50+    ‚Üí Qualit√© 85-90% (Clustering √©mergent)
Documents 200+   ‚Üí Qualit√© 93-95% (Ontologie riche)
Documents 1000+  ‚Üí Qualit√© 95-98% (Expert-level)
```

**M√©canisme:** Apprentissage continu via clustering s√©mantique + feedback loop

#### 3. **Expert-Tuneable if Desired**
```
Utilisateur standard ‚Üí Utilise tel quel (80-90% qualit√© suffit)
Utilisateur avanc√©  ‚Üí Review clusters + corrections (95-98%)
```

**Optionnel, pas obligatoire.**

---

## 2Ô∏è‚É£ Architecture Actuelle vs Cible

### üìä Matrice de Transformation

| Composant | Architecture Actuelle (SAP-First) | Architecture Cible (Zero-Config) | Gain |
|-----------|----------------------------------|----------------------------------|------|
| **Catalogues Solutions** | üî¥ Hard-coded `sap_solutions.yaml` (41 solutions) | ‚úÖ LLM canonical names + Adaptive ontology | -100% config |
| **Prompts LLM** | üî¥ "Use SAP canonical name" | ‚úÖ "Use vendor official name" | Domain-agnostic |
| **Cat√©gories** | üî¥ 7 cat√©gories fixes (`erp`, `analytics`, ...) | ‚úÖ Auto-inf√©rence LLM/clustering | -100% config |
| **Domain Classification** | üü° Liste fixe (`finance`, `pharma`, `consulting`) | ‚úÖ Auto-d√©tection + extensible | Adaptative |
| **Normalisation** | üü° Fuzzy match vs catalogue statique | ‚úÖ Clustering s√©mantique adaptive | Self-learning |
| **ConceptType** | ‚úÖ D√©j√† g√©n√©rique (ENTITY, PRACTICE, etc.) | ‚úÖ Inchang√© | Perfect |
| **Extraction NER** | ‚úÖ spaCy multilingue g√©n√©rique | ‚úÖ Inchang√© | Perfect |
| **Architecture Agents** | ‚úÖ Logique domain-agnostic | ‚úÖ Inchang√© | Perfect |

### üìà √âvolution Qualit√© dans le Temps

```
Qualit√©
  100% ‚î§                                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄExpert Tuned (optionnel)
   95% ‚î§                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   90% ‚î§              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§            ‚îÇ
   85% ‚î§       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§          ‚îÇ            ‚îÇ
   80% ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îÇ          ‚îÇ            ‚îÇ
   75% ‚î§       ‚îÇ      ‚îÇ          ‚îÇ            ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Temps
       Day 1  Week 2  Week 4    Month 2    Month 6

       [Zero-Config] [Self-Learning] [Convergence] [Plateau]
```

**L√©gende:**
- **Day 1 (80%)**: LLM extraction pure, z√©ro configuration
- **Week 2-4 (85-90%)**: Clustering s√©mantique commence, normalisation s'am√©liore
- **Month 2 (93-95%)**: Ontologie adaptive mature, variantes bien g√©r√©es
- **Month 6+ (95-98%)**: Plateau qualit√© (avec reviews experts optionnelles)

---

## 3Ô∏è‚É£ Composants Techniques

### Architecture en Couches

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 1: LLM Extraction (Zero-Config Core)                     ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îÇ  ‚Ä¢ GPT-4o / Claude Sonnet 3.5                                   ‚îÇ
‚îÇ  ‚Ä¢ Prompts domain-agnostic ("Extract vendor official names")    ‚îÇ
‚îÇ  ‚Ä¢ Connaissances internes LLM (SAP, Moderna, Bloomberg, etc.)   ‚îÇ
‚îÇ  ‚Ä¢ Qualit√© baseline: 80-85%                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 2: Auto Domain Detection (Transparent)                   ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îÇ  ‚Ä¢ Keyword density analysis (0 cost, rapide)                    ‚îÇ
‚îÇ  ‚Ä¢ NER distribution analysis (ORG types, products)              ‚îÇ
‚îÇ  ‚Ä¢ LLM zero-shot classification (si ambigu√Øt√©)                  ‚îÇ
‚îÇ  ‚Ä¢ Output: (domain, confidence) ‚Üí "pharmaceutical", 0.92        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 3: Adaptive Ontology (Self-Learning)                     ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îÇ  ‚Ä¢ Semantic clustering (embeddings cosine similarity)           ‚îÇ
‚îÇ  ‚Ä¢ Cluster management (cr√©ation, fusion, split)                 ‚îÇ
‚îÇ  ‚Ä¢ LLM canonical names (pour nouveaux concepts)                 ‚îÇ
‚îÇ  ‚Ä¢ Feedback loop (corrections humaines ‚Üí am√©lioration)          ‚îÇ
‚îÇ  ‚Ä¢ Qualit√© √©volutive: 85% ‚Üí 90% ‚Üí 95%                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 4: Expert Tuning (Optional)                              ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îÇ  ‚Ä¢ Admin UI: Review clusters auto-d√©tect√©s                      ‚îÇ
‚îÇ  ‚Ä¢ Import ontologie custom (YAML/CSV/Excel)                     ‚îÇ
‚îÇ  ‚Ä¢ R√®gles m√©tier manuelles (cas edge complexes)                 ‚îÇ
‚îÇ  ‚Ä¢ Qualit√© max: 95-98%                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 4Ô∏è‚É£ Sp√©cifications D√©taill√©es

### 4.1 AutoDomainDetector

**Responsabilit√©:** D√©tecter automatiquement le domaine m√©tier d'un document sans configuration pr√©alable, avec apprentissage continu.

#### üéõÔ∏è Configuration (.env)

```bash
# Mode d√©tection domaine
# - "self_learning" (d√©faut, Option C) : Apprentissage pur, zero config, universel
# - "bootstrap" (Option C+) : Signatures minimales + apprentissage (tests/dev rapide)
DOMAIN_DETECTION_MODE=self_learning

# Seuil similarit√© cluster matching (default: 0.75)
DOMAIN_CLUSTER_SIMILARITY_THRESHOLD=0.75

# Nombre minimum de documents avant cluster matching (default: 5)
DOMAIN_BOOTSTRAP_MIN_DOCS=5
```

**Recommandations** :
- **Prod / Client** : `DOMAIN_DETECTION_MODE=self_learning` (universel, adaptatif)
- **Dev / Tests** : `DOMAIN_DETECTION_MODE=bootstrap` (bootstrap rapide avec 5 domaines)

---

#### Interface

```python
from typing import Tuple, Optional, List
from dataclasses import dataclass
import numpy as np

@dataclass
class DomainDetectionResult:
    """R√©sultat d√©tection domaine"""
    domain: str                    # Ex: "retail", "pharmaceutical", "energy"
    confidence: float              # 0.0 - 1.0
    method: str                    # "cluster_match", "llm_bootstrap", "keyword_bootstrap"
    is_new_domain: bool           # True si nouveau domaine d√©couvert
    cluster_id: Optional[str]     # ID cluster Neo4j (si existe)
    signals: Dict[str, float]     # Scores d√©taill√©s par domaine/cluster
    execution_time_ms: float

class AutoDomainDetector:
    """
    D√©tecte le domaine m√©tier d'un document via Self-Learning.

    üåü Option C (self_learning) - D√©faut Prod:
    - Z√©ro signature hard-cod√©e
    - Apprentissage pur via clustering s√©mantique
    - Universel (retail, energy, legal, etc.)
    - Co√ªt d√©croissant (95% gratuit apr√®s 200 docs)

    ‚ö° Option C+ (bootstrap) - Tests/Dev:
    - 5 signatures minimales (pharma, finance, tech, manufacturing, consulting)
    - Acc√©l√®re bootstrap phase (docs 1-10)
    - Switch automatique vers self-learning apr√®s MIN_DOCS

    Workflow (Mode self_learning):
    1. G√©n√©rer embedding document (1024D)
    2. Chercher match dans clusters existants (Neo4j)
    3. Si match > threshold ‚Üí Domaine d√©tect√© (gratuit, 5ms)
    4. Si pas de match ‚Üí LLM classifie + cr√©e cluster
    5. Enrichir cluster avec keywords/entities

    Workflow (Mode bootstrap):
    1. Keyword density sur signatures (rapide, gratuit)
    2. Si confidence < 0.70 ‚Üí LLM classification
    3. Parall√®lement : apprentissage clusters en arri√®re-plan
    4. Apr√®s MIN_DOCS ‚Üí switch auto vers clusters
    """

    def __init__(
        self,
        llm_router: LLMRouter,
        neo4j_client: Neo4jClient,
        embeddings_model,  # SentenceTransformer("multilingual-e5-large")
        config: Optional[Dict[str, Any]] = None
    ):
        """
        Initialise le d√©tecteur.

        Args:
            llm_router: Router LLM pour classification zero-shot
            neo4j_client: Client Neo4j pour storage clusters
            embeddings_model: Mod√®le embeddings (1024D)
            config: Configuration optionnelle
        """
        self.llm_router = llm_router
        self.neo4j_client = neo4j_client
        self.embeddings_model = embeddings_model
        self.config = config or {}

        # Mode d√©tection (via .env)
        self.mode = os.getenv("DOMAIN_DETECTION_MODE", "self_learning")
        self.cluster_threshold = float(os.getenv("DOMAIN_CLUSTER_SIMILARITY_THRESHOLD", "0.75"))
        self.bootstrap_min_docs = int(os.getenv("DOMAIN_BOOTSTRAP_MIN_DOCS", "5"))

        # Signatures bootstrap (seulement si mode=bootstrap)
        self.bootstrap_signatures = self._load_bootstrap_signatures() if self.mode == "bootstrap" else {}

        logger.info(
            f"[AutoDomainDetector] Initialized with mode={self.mode}, "
            f"cluster_threshold={self.cluster_threshold}, bootstrap_min_docs={self.bootstrap_min_docs}"
        )

    def detect(
        self,
        document_text: str,
        document_id: str,
        tenant_id: str = "default"
    ) -> DomainDetectionResult:
        """
        D√©tecte le domaine d'un document (mode auto selon config).

        Args:
            document_text: Texte complet du document
            document_id: ID document pour storage cluster
            tenant_id: ID tenant pour isolation

        Returns:
            DomainDetectionResult avec domaine d√©tect√©

        Workflow d√©pend du mode (.env):
        - self_learning: Cluster matching ‚Üí LLM bootstrap si besoin
        - bootstrap: Keyword signatures ‚Üí LLM si besoin ‚Üí apprentissage parall√®le
        """
        import time
        start_time = time.time()

        if self.mode == "self_learning":
            return self._detect_self_learning(document_text, document_id, tenant_id, start_time)
        elif self.mode == "bootstrap":
            return self._detect_bootstrap(document_text, document_id, tenant_id, start_time)
        else:
            raise ValueError(f"Invalid DOMAIN_DETECTION_MODE: {self.mode}")

    def _detect_self_learning(
        self,
        document_text: str,
        document_id: str,
        tenant_id: str,
        start_time: float
    ) -> DomainDetectionResult:
        """
        D√©tection pure Self-Learning (Option C).

        Workflow:
        1. G√©n√©rer embedding document (1024D)
        2. Chercher clusters existants dans Neo4j
        3. Si match > threshold ‚Üí Return domaine (gratuit, ~5ms)
        4. Si pas de match ‚Üí LLM classifie + cr√©e cluster
        5. Enrichir cluster avec document
        """
        # √âtape 1: G√©n√©rer embedding document
        doc_embedding = self.embeddings_model.encode(document_text)

        # √âtape 2: Chercher clusters existants
        existing_clusters = self._get_domain_clusters(tenant_id)

        if existing_clusters:
            # Calculer similarit√© avec chaque cluster
            best_match = self._find_best_cluster_match(doc_embedding, existing_clusters)

            if best_match and best_match.similarity >= self.cluster_threshold:
                # Match trouv√© ! Pas besoin LLM
                self._enrich_cluster(
                    cluster_id=best_match.cluster_id,
                    document_id=document_id,
                    document_text=document_text,
                    document_embedding=doc_embedding,
                    tenant_id=tenant_id
                )

                execution_time = (time.time() - start_time) * 1000

                logger.info(
                    f"[DomainDetector:SelfLearning] Matched cluster '{best_match.domain_name}' "
                    f"(similarity={best_match.similarity:.3f}, time={execution_time:.1f}ms)"
                )

                return DomainDetectionResult(
                    domain=best_match.domain_name,
                    confidence=best_match.similarity,
                    method="cluster_match",
                    is_new_domain=False,
                    cluster_id=best_match.cluster_id,
                    signals={"cluster_similarity": best_match.similarity},
                    execution_time_ms=execution_time
                )

        # √âtape 3: Pas de match ‚Üí LLM bootstrap
        llm_result = self._llm_classify_domain(document_text[:3000])

        # √âtape 4: Cr√©er ou attacher √† cluster
        cluster_id = self._create_or_attach_cluster(
            domain_name=llm_result.domain,
            document_id=document_id,
            document_text=document_text,
            document_embedding=doc_embedding,
            tenant_id=tenant_id
        )

        execution_time = (time.time() - start_time) * 1000

        logger.info(
            f"[DomainDetector:SelfLearning] Bootstrapped new domain '{llm_result.domain}' "
            f"via LLM (confidence={llm_result.confidence:.3f}, time={execution_time:.1f}ms)"
        )

        return DomainDetectionResult(
            domain=llm_result.domain,
            confidence=llm_result.confidence,
            method="llm_bootstrap",
            is_new_domain=True,
            cluster_id=cluster_id,
            signals={"llm_confidence": llm_result.confidence},
            execution_time_ms=execution_time
        )

    def _detect_bootstrap(
        self,
        document_text: str,
        document_id: str,
        tenant_id: str,
        start_time: float
    ) -> DomainDetectionResult:
        """
        D√©tection Bootstrap (Option C+) avec signatures minimales.

        Workflow:
        1. V√©rifier nombre documents ‚Üí Si >= MIN_DOCS, switch vers self_learning
        2. Sinon: Keyword density sur signatures
        3. Si confidence < 0.70 ‚Üí LLM classification
        4. Parall√®lement: apprendre clusters en arri√®re-plan
        """
        # Check si on doit switcher vers self_learning
        doc_count = self._get_tenant_document_count(tenant_id)

        if doc_count >= self.bootstrap_min_docs:
            # Assez de docs ‚Üí Passer en self_learning auto
            logger.info(
                f"[DomainDetector:Bootstrap] Switching to self_learning mode "
                f"({doc_count} >= {self.bootstrap_min_docs} docs)"
            )
            return self._detect_self_learning(document_text, document_id, tenant_id, start_time)

        # √âtape 1: Keyword density sur signatures
        keyword_scores = self._compute_keyword_scores_bootstrap(document_text)
        top_domain = max(keyword_scores, key=keyword_scores.get) if keyword_scores else None

        if top_domain and keyword_scores[top_domain] >= 0.70:
            # Confidence suffisante
            # Apprendre cluster en parall√®le (non-bloquant)
            self._learn_cluster_async(document_text, document_id, top_domain, tenant_id)

            execution_time = (time.time() - start_time) * 1000

            return DomainDetectionResult(
                domain=top_domain,
                confidence=keyword_scores[top_domain],
                method="keyword_bootstrap",
                is_new_domain=False,
                cluster_id=None,
                signals=keyword_scores,
                execution_time_ms=execution_time
            )

        # √âtape 2: LLM fallback
        llm_result = self._llm_classify_domain(document_text[:3000])

        # Apprendre cluster en parall√®le
        self._learn_cluster_async(document_text, document_id, llm_result.domain, tenant_id)

        execution_time = (time.time() - start_time) * 1000

        return DomainDetectionResult(
            domain=llm_result.domain,
            confidence=llm_result.confidence,
            method="llm_bootstrap",
            is_new_domain=True,
            cluster_id=None,
            signals={"llm_confidence": llm_result.confidence},
            execution_time_ms=execution_time
        )

    def _load_bootstrap_signatures(self) -> Dict[str, Dict]:
        """
        Charge signatures bootstrap (MODE bootstrap uniquement).

        Signatures MINIMALES pour 5 domaines courants.
        Utilis√© seulement en mode C+ (bootstrap) pour acc√©l√©rer les 5 premiers docs.

        Format:
        {
            "pharmaceutical": {
                "keywords": ["FDA", "GMP", "clinical trial", ...],
                "weight": 1.0
            },
            ...
        }

        Note: En mode self_learning (C), cette m√©thode n'est PAS appel√©e.
        """
        # Signatures minimales (5 domaines courants)
        default_signatures = {
            "pharmaceutical": {
                "keywords": [
                    "FDA", "GMP", "clinical trial", "drug", "molecule",
                    "biologics", "vaccine", "pharma", "pharmaceutical",
                    "patient", "dosage", "efficacy", "adverse event",
                    "regulatory", "EMA", "ICH", "21 CFR", "GxP"
                ],
                "org_patterns": [
                    "pharma", "biotech", "laboratories", "therapeutics",
                    "biopharma", "life sciences"
                ],
                "weight": 1.0
            },
            "finance": {
                "keywords": [
                    "trading", "Basel", "MiFID", "derivative", "portfolio",
                    "hedge fund", "investment", "capital markets", "equity",
                    "bond", "swap", "option", "futures", "risk management",
                    "compliance", "KYC", "AML", "Dodd-Frank"
                ],
                "org_patterns": [
                    "bank", "capital", "securities", "trading", "investment",
                    "asset management", "financial services"
                ],
                "weight": 1.0
            },
            "technology": {
                "keywords": [
                    "software", "cloud", "API", "microservices", "DevOps",
                    "kubernetes", "SaaS", "platform", "infrastructure",
                    "database", "architecture", "deployment", "CI/CD",
                    "container", "serverless", "agile", "sprint"
                ],
                "org_patterns": [
                    "tech", "software", "systems", "solutions", "digital",
                    "technology", "computing"
                ],
                "weight": 1.0
            },
            "manufacturing": {
                "keywords": [
                    "production", "assembly", "quality control", "ISO 9001",
                    "Six Sigma", "lean manufacturing", "supply chain",
                    "inventory", "MES", "PLM", "CAD", "CAM", "SCADA",
                    "OEE", "throughput", "yield"
                ],
                "org_patterns": [
                    "manufacturing", "industries", "production", "factory",
                    "industrial", "engineering"
                ],
                "weight": 1.0
            },
            "consulting": {
                "keywords": [
                    "strategy", "transformation", "roadmap", "framework",
                    "best practices", "business model", "value proposition",
                    "digital transformation", "change management",
                    "organizational", "governance", "maturity"
                ],
                "org_patterns": [
                    "consulting", "advisory", "partners", "strategy",
                    "management consulting"
                ],
                "weight": 1.0
            }
        }

        # Merge avec config custom si fournie
        custom_signatures = self.config.get("domain_signatures", {})
        return {**default_signatures, **custom_signatures}

    def _compute_keyword_scores(self, text: str) -> Dict[str, float]:
        """
        Calcule scores domaines via keyword density.

        Algorithm:
        1. Normaliser texte (lowercase, tokenize)
        2. Pour chaque domaine, compter matches keywords
        3. Score = matches_count / total_keywords * weight
        4. Normaliser scores (sum = 1.0)
        """
        text_lower = text.lower()
        scores = {}

        for domain, signature in self.domain_signatures.items():
            keywords = signature["keywords"]
            weight = signature.get("weight", 1.0)

            # Compter matches
            matches = sum(1 for kw in keywords if kw.lower() in text_lower)

            # Score brut
            if len(keywords) > 0:
                raw_score = (matches / len(keywords)) * weight
            else:
                raw_score = 0.0

            scores[domain] = raw_score

        # Normaliser (sum = 1.0)
        total = sum(scores.values())
        if total > 0:
            scores = {d: s / total for d, s in scores.items()}

        return scores

    def _compute_ner_scores(self, text: str) -> Dict[str, float]:
        """
        Calcule scores domaines via NER distribution.

        Algorithm:
        1. Extraire toutes entities (ORG, PRODUCT, etc.)
        2. Pour chaque domaine, matcher entities vs org_patterns
        3. Score = matched_orgs / total_orgs * weight
        """
        # Extraire entities
        entities = self.ner_manager.extract_entities(text, language="en")
        org_entities = [e for e in entities if e["label"] == "ORG"]

        if not org_entities:
            return {d: 0.0 for d in self.domain_signatures.keys()}

        scores = {}
        for domain, signature in self.domain_signatures.items():
            org_patterns = signature["org_patterns"]
            weight = signature.get("weight", 1.0)

            # Compter matches
            matches = 0
            for entity in org_entities:
                entity_text = entity["text"].lower()
                if any(pattern.lower() in entity_text for pattern in org_patterns):
                    matches += 1

            # Score brut
            raw_score = (matches / len(org_entities)) * weight
            scores[domain] = raw_score

        # Normaliser
        total = sum(scores.values())
        if total > 0:
            scores = {d: s / total for d, s in scores.items()}

        return scores

    def _combine_scores(
        self,
        keyword_scores: Dict[str, float],
        ner_scores: Dict[str, float],
        keyword_weight: float = 0.6,
        ner_weight: float = 0.4
    ) -> Dict[str, float]:
        """Combine scores avec pond√©ration"""
        combined = {}
        for domain in keyword_scores.keys():
            combined[domain] = (
                keyword_scores[domain] * keyword_weight +
                ner_scores[domain] * ner_weight
            )
        return combined

    def _llm_classify(self, text: str) -> Tuple[str, float]:
        """
        Classification LLM zero-shot (arbitrage final).

        Utilis√© seulement si keyword + NER ambigus.
        """
        domains_list = ", ".join(self.domain_signatures.keys())

        prompt = f"""
Analyze this document excerpt and classify it into ONE domain:

Available domains: {domains_list}, general

Document excerpt:
{text}

Rules:
- Choose the MOST SPECIFIC domain that fits
- Use "general" only if no specific domain matches well
- Return format: "domain: <name>, confidence: <0.0-1.0>"

Classification:
"""

        from knowbase.common.llm_router import TaskType

        response = self.llm_router.complete(
            task_type=TaskType.CLASSIFICATION,
            messages=[{"role": "user", "content": prompt}]
        )

        # Parse response: "domain: pharmaceutical, confidence: 0.92"
        import re
        match = re.search(r"domain:\s*(\w+),\s*confidence:\s*([\d.]+)", response.lower())
        if match:
            domain = match.group(1)
            confidence = float(match.group(2))
            return (domain, confidence)

        # Fallback parsing
        return ("general", 0.5)

    def learn_domain(
        self,
        domain_name: str,
        keywords: List[str],
        org_patterns: List[str]
    ):
        """
        Apprendre un nouveau domaine dynamiquement.

        Use case: Admin ajoute domaine custom (ex: "aerospace", "retail")
        """
        self.domain_signatures[domain_name] = {
            "keywords": keywords,
            "org_patterns": org_patterns,
            "weight": 1.0
        }

        logger.info(
            f"[AutoDomainDetector] Learned new domain: {domain_name} "
            f"({len(keywords)} keywords, {len(org_patterns)} patterns)"
        )
```

#### Fichier: `src/knowbase/semantic/domain_detector.py`

---

### 4.2 AdaptiveOntology

**Responsabilit√©:** Ontologie qui se construit automatiquement par clustering s√©mantique et s'am√©liore avec l'usage.

#### Interface

```python
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass, field
import numpy as np
from datetime import datetime

@dataclass
class ConceptCluster:
    """Cluster de concepts similaires (variantes d'un m√™me concept)"""
    cluster_id: str
    canonical_name: str              # Nom canonique (choisi par LLM)
    variants: List[str]               # Variantes d√©tect√©es
    centroid: np.ndarray             # Embedding moyen du cluster
    mention_count: int = 0           # Nombre total de mentions
    confidence: float = 1.0          # Confidence globale du cluster
    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)

    # Metadata
    concept_type: Optional[str] = None  # ENTITY, PRACTICE, etc.
    domain: Optional[str] = None        # pharmaceutical, finance, etc.
    source_documents: List[str] = field(default_factory=list)

@dataclass
class NormalizationResult:
    """R√©sultat normalisation d'un nom brut"""
    canonical_name: str
    confidence: float
    method: str                      # "cluster_match", "llm_new", "manual"
    cluster_id: Optional[str] = None
    execution_time_ms: float = 0.0

class AdaptiveOntology:
    """
    Ontologie qui se construit automatiquement par clustering s√©mantique.

    Principes:
    1. D√©marrage vierge (pas de catalogue pr√©-rempli)
    2. Clustering au fil des extractions (embeddings cosine similarity)
    3. LLM g√©n√®re canonical names pour nouveaux clusters
    4. Feedback loop: Corrections humaines ‚Üí Am√©lioration clusters
    5. Convergence progressive vers ontologie riche

    Avantages:
    - Z√©ro configuration initiale
    - S'adapte automatiquement au vocabulaire m√©tier client
    - Multi-tenant naturel (1 ontologie par tenant)
    - Self-improving avec usage

    M√©triques attendues:
    - 0 documents: 85% qualit√© (LLM pur, pas de clusters)
    - 50 documents: 90% qualit√© (clusters √©mergents)
    - 200 documents: 95% qualit√© (ontologie mature)
    """

    def __init__(
        self,
        tenant_id: str,
        neo4j_client: Optional[Any] = None,
        embedder: Optional[Any] = None,
        llm_router: Optional[LLMRouter] = None
    ):
        """
        Initialise ontologie adaptive pour un tenant.

        Args:
            tenant_id: ID tenant (isolation multi-tenant)
            neo4j_client: Client Neo4j pour persistence
            embedder: Mod√®le embeddings (sentence-transformers)
            llm_router: Router LLM pour canonical names
        """
        self.tenant_id = tenant_id
        self.neo4j_client = neo4j_client
        self.embedder = embedder or get_embedder()
        self.llm_router = llm_router or get_llm_router()

        # Charger clusters existants depuis Neo4j (si tenant existant)
        self.clusters: Dict[str, ConceptCluster] = self._load_clusters()

        # Seuils
        self.similarity_threshold = 0.85  # Cosine similarity pour match cluster
        self.merge_threshold = 0.92       # Seuil pour fusionner clusters

        logger.info(
            f"[AdaptiveOntology] Initialized for tenant={tenant_id} "
            f"with {len(self.clusters)} existing clusters"
        )

    def normalize(
        self,
        raw_name: str,
        context: str = "",
        concept_type: Optional[str] = None
    ) -> NormalizationResult:
        """
        Normalise un nom brut vers nom canonique.

        Workflow:
        1. Chercher dans clusters existants (embedding cosine similarity)
        2. Si match trouv√© (>0.85) ‚Üí Retourner canonical name du cluster
        3. Sinon ‚Üí LLM g√©n√®re canonical name + cr√©er nouveau cluster
        4. Update cluster (crowdsourcing implicite)

        Args:
            raw_name: Nom brut extrait du document
            context: Contexte d'extraction (optionnel, am√©liore pr√©cision LLM)
            concept_type: Type concept (ENTITY, PRACTICE, etc.)

        Returns:
            NormalizationResult avec canonical_name et confidence
        """
        import time
        start_time = time.time()

        # √âtape 1: G√©n√©rer embedding du nom brut
        raw_embedding = self.embedder.encode([raw_name])[0]

        # √âtape 2: Chercher dans clusters existants
        best_match = self._find_best_cluster_match(
            raw_embedding,
            raw_name,
            concept_type
        )

        if best_match:
            cluster_id, similarity = best_match
            cluster = self.clusters[cluster_id]

            # Match trouv√© !
            logger.info(
                f"[AdaptiveOntology] Matched '{raw_name}' ‚Üí '{cluster.canonical_name}' "
                f"(cluster={cluster_id[:8]}, similarity={similarity:.3f})"
            )

            # Mise √† jour cluster (crowdsourcing implicite)
            self._update_cluster(cluster_id, raw_name, raw_embedding)

            execution_time = (time.time() - start_time) * 1000
            return NormalizationResult(
                canonical_name=cluster.canonical_name,
                confidence=similarity,
                method="cluster_match",
                cluster_id=cluster_id,
                execution_time_ms=execution_time
            )

        # √âtape 3: Pas de match ‚Üí Cr√©er nouveau cluster
        canonical_name = self._llm_canonical_name(raw_name, context)
        new_cluster_id = self._create_cluster(
            canonical_name=canonical_name,
            raw_name=raw_name,
            embedding=raw_embedding,
            concept_type=concept_type
        )

        logger.info(
            f"[AdaptiveOntology] Created new cluster '{canonical_name}' "
            f"(cluster_id={new_cluster_id[:8]}, raw='{raw_name}')"
        )

        execution_time = (time.time() - start_time) * 1000
        return NormalizationResult(
            canonical_name=canonical_name,
            confidence=0.95,  # High confidence (LLM-generated)
            method="llm_new_cluster",
            cluster_id=new_cluster_id,
            execution_time_ms=execution_time
        )

    def _find_best_cluster_match(
        self,
        raw_embedding: np.ndarray,
        raw_name: str,
        concept_type: Optional[str] = None
    ) -> Optional[Tuple[str, float]]:
        """
        Trouve meilleur cluster match via cosine similarity.

        Returns:
            (cluster_id, similarity) si match trouv√© (>threshold)
            None sinon
        """
        from sklearn.metrics.pairwise import cosine_similarity

        best_cluster_id = None
        best_similarity = 0.0

        for cluster_id, cluster in self.clusters.items():
            # Filtrer par type si fourni (optionnel)
            if concept_type and cluster.concept_type != concept_type:
                continue

            # Calculer similarit√© avec centroid du cluster
            similarity = cosine_similarity(
                [raw_embedding],
                [cluster.centroid]
            )[0][0]

            if similarity > best_similarity:
                best_similarity = similarity
                best_cluster_id = cluster_id

        # Retourner seulement si au-dessus du seuil
        if best_similarity >= self.similarity_threshold:
            return (best_cluster_id, best_similarity)

        return None

    def _update_cluster(
        self,
        cluster_id: str,
        new_variant: str,
        new_embedding: np.ndarray
    ):
        """
        Met √† jour cluster avec nouvelle variante (crowdsourcing implicite).

        Actions:
        1. Ajouter variante √† la liste (si pas d√©j√† pr√©sente)
        2. Recalculer centroid (moyenne mobile)
        3. Incr√©menter mention_count
        4. Persister dans Neo4j
        """
        cluster = self.clusters[cluster_id]

        # Ajouter variante (d√©dupliqu√©e)
        if new_variant.lower() not in [v.lower() for v in cluster.variants]:
            cluster.variants.append(new_variant)

        # Recalculer centroid (moyenne mobile)
        # Formula: new_centroid = (old_centroid * n + new_embedding) / (n + 1)
        n = cluster.mention_count
        cluster.centroid = (cluster.centroid * n + new_embedding) / (n + 1)

        # Update metadata
        cluster.mention_count += 1
        cluster.updated_at = datetime.utcnow()

        # Persister (async, non-bloquant)
        if self.neo4j_client:
            self._persist_cluster(cluster)

    def _create_cluster(
        self,
        canonical_name: str,
        raw_name: str,
        embedding: np.ndarray,
        concept_type: Optional[str] = None
    ) -> str:
        """
        Cr√©e nouveau cluster.

        Returns:
            cluster_id (UUID)
        """
        import uuid

        cluster_id = str(uuid.uuid4())

        cluster = ConceptCluster(
            cluster_id=cluster_id,
            canonical_name=canonical_name,
            variants=[raw_name],
            centroid=embedding,
            mention_count=1,
            confidence=0.95,
            concept_type=concept_type,
            source_documents=[]
        )

        # Stocker en m√©moire
        self.clusters[cluster_id] = cluster

        # Persister dans Neo4j
        if self.neo4j_client:
            self._persist_cluster(cluster)

        return cluster_id

    def _llm_canonical_name(self, raw_name: str, context: str) -> str:
        """
        Demander √† LLM le nom canonique officiel.

        Prompt: G√©n√®re nom canonique pour entity/concept d√©tect√©.
        """
        prompt = f"""
Given the entity/concept name "{raw_name}" extracted from this context:

Context: "{context[:300]}"

Return the official, canonical name for this entity/concept.

Rules:
- Use the full, official product/company/concept name
- NOT abbreviations or acronyms (unless that IS the official name)
- As published by the vendor/organization/standards body
- Preserve proper capitalization and formatting

Examples:
- "S4 PCE" ‚Üí "SAP S/4HANA Cloud, Private Edition"
- "BBG Terminal" ‚Üí "Bloomberg Terminal"
- "mRNA-1273" ‚Üí "Moderna mRNA-1273 Platform"
- "GMP" ‚Üí "Good Manufacturing Practice" (if standard/practice)
- "FDA" ‚Üí "FDA" (acronym IS the official name for entity)

Important: Return ONLY the canonical name, no explanation.

Canonical name:
"""

        from knowbase.common.llm_router import TaskType

        canonical = self.llm_router.complete(
            task_type=TaskType.ENTITY_NORMALIZATION,
            messages=[{"role": "user", "content": prompt}]
        ).strip()

        # Cleanup response
        canonical = canonical.strip('"').strip("'").strip()

        return canonical

    def _load_clusters(self) -> Dict[str, ConceptCluster]:
        """
        Charge clusters existants depuis Neo4j (si tenant existant).

        Query Neo4j:
        MATCH (c:AdaptiveCluster {tenant_id: $tenant_id})
        RETURN c
        """
        if not self.neo4j_client:
            return {}

        try:
            # Query Neo4j pour charger clusters
            query = """
            MATCH (c:AdaptiveCluster {tenant_id: $tenant_id})
            RETURN c.cluster_id AS cluster_id,
                   c.canonical_name AS canonical_name,
                   c.variants AS variants,
                   c.centroid AS centroid,
                   c.mention_count AS mention_count,
                   c.confidence AS confidence,
                   c.concept_type AS concept_type,
                   c.created_at AS created_at,
                   c.updated_at AS updated_at
            """

            results = self.neo4j_client.execute_query(
                query,
                {"tenant_id": self.tenant_id}
            )

            clusters = {}
            for record in results:
                cluster_id = record["cluster_id"]

                # Reconstruire cluster
                cluster = ConceptCluster(
                    cluster_id=cluster_id,
                    canonical_name=record["canonical_name"],
                    variants=record["variants"],
                    centroid=np.array(record["centroid"]),
                    mention_count=record["mention_count"],
                    confidence=record["confidence"],
                    concept_type=record.get("concept_type"),
                    created_at=record["created_at"],
                    updated_at=record["updated_at"]
                )

                clusters[cluster_id] = cluster

            logger.info(
                f"[AdaptiveOntology] Loaded {len(clusters)} clusters for tenant={self.tenant_id}"
            )

            return clusters

        except Exception as e:
            logger.error(f"[AdaptiveOntology] Failed to load clusters: {e}")
            return {}

    def _persist_cluster(self, cluster: ConceptCluster):
        """
        Persiste cluster dans Neo4j (MERGE pour upsert).
        """
        if not self.neo4j_client:
            return

        try:
            query = """
            MERGE (c:AdaptiveCluster {cluster_id: $cluster_id, tenant_id: $tenant_id})
            SET c.canonical_name = $canonical_name,
                c.variants = $variants,
                c.centroid = $centroid,
                c.mention_count = $mention_count,
                c.confidence = $confidence,
                c.concept_type = $concept_type,
                c.updated_at = datetime()
            """

            self.neo4j_client.execute_query(
                query,
                {
                    "cluster_id": cluster.cluster_id,
                    "tenant_id": self.tenant_id,
                    "canonical_name": cluster.canonical_name,
                    "variants": cluster.variants,
                    "centroid": cluster.centroid.tolist(),
                    "mention_count": cluster.mention_count,
                    "confidence": cluster.confidence,
                    "concept_type": cluster.concept_type
                }
            )

        except Exception as e:
            logger.error(f"[AdaptiveOntology] Failed to persist cluster: {e}")

    def learn_from_correction(
        self,
        raw_name: str,
        corrected_canonical: str,
        cluster_id: Optional[str] = None
    ):
        """
        Apprendre d'une correction humaine (feedback loop).

        Use cases:
        1. Admin corrige normalisation incorrecte
        2. Admin fusionne deux clusters
        3. Admin split un cluster

        Args:
            raw_name: Nom brut qui a √©t√© mal normalis√©
            corrected_canonical: Nom canonique correct (fourni par humain)
            cluster_id: ID cluster √† corriger (optionnel, retrouv√© si None)
        """
        # Trouver cluster concern√©
        if not cluster_id:
            # Retrouver cluster via raw_name
            for cid, cluster in self.clusters.items():
                if raw_name.lower() in [v.lower() for v in cluster.variants]:
                    cluster_id = cid
                    break

        if not cluster_id:
            logger.warning(
                f"[AdaptiveOntology] Cannot learn from correction: "
                f"cluster not found for '{raw_name}'"
            )
            return

        cluster = self.clusters[cluster_id]

        # Mettre √† jour canonical_name
        old_canonical = cluster.canonical_name
        cluster.canonical_name = corrected_canonical
        cluster.updated_at = datetime.utcnow()

        # Persister
        self._persist_cluster(cluster)

        logger.info(
            f"[AdaptiveOntology] Learned from correction: "
            f"'{old_canonical}' ‚Üí '{corrected_canonical}' "
            f"(cluster={cluster_id[:8]})"
        )

    def merge_clusters(self, cluster_id1: str, cluster_id2: str):
        """
        Fusionner deux clusters (d√©tect√©s comme similaires).

        Use case: Admin d√©tecte que deux clusters repr√©sentent m√™me concept
        """
        cluster1 = self.clusters[cluster_id1]
        cluster2 = self.clusters[cluster_id2]

        # Fusionner variantes
        cluster1.variants.extend(cluster2.variants)

        # Recalculer centroid (moyenne pond√©r√©e)
        n1 = cluster1.mention_count
        n2 = cluster2.mention_count
        cluster1.centroid = (
            cluster1.centroid * n1 + cluster2.centroid * n2
        ) / (n1 + n2)

        # Mettre √† jour counts
        cluster1.mention_count += cluster2.mention_count
        cluster1.updated_at = datetime.utcnow()

        # Supprimer cluster2
        del self.clusters[cluster_id2]

        # Persister
        self._persist_cluster(cluster1)
        self._delete_cluster(cluster_id2)

        logger.info(
            f"[AdaptiveOntology] Merged clusters: "
            f"{cluster_id1[:8]} ‚Üê {cluster_id2[:8]} "
            f"(canonical='{cluster1.canonical_name}')"
        )

    def split_cluster(
        self,
        cluster_id: str,
        variants_group1: List[str],
        variants_group2: List[str],
        canonical1: str,
        canonical2: str
    ):
        """
        Split cluster en deux (d√©tect√© qu'un cluster m√©lange concepts diff√©rents).

        Use case: Admin d√©tecte qu'un cluster contient concepts distincts
        """
        original_cluster = self.clusters[cluster_id]

        # Cr√©er cluster1
        embedding1 = self.embedder.encode([canonical1])[0]
        cluster_id1 = self._create_cluster(
            canonical_name=canonical1,
            raw_name=variants_group1[0],
            embedding=embedding1,
            concept_type=original_cluster.concept_type
        )
        cluster1 = self.clusters[cluster_id1]
        cluster1.variants = variants_group1

        # Cr√©er cluster2
        embedding2 = self.embedder.encode([canonical2])[0]
        cluster_id2 = self._create_cluster(
            canonical_name=canonical2,
            raw_name=variants_group2[0],
            embedding=embedding2,
            concept_type=original_cluster.concept_type
        )
        cluster2 = self.clusters[cluster_id2]
        cluster2.variants = variants_group2

        # Supprimer cluster original
        del self.clusters[cluster_id]
        self._delete_cluster(cluster_id)

        logger.info(
            f"[AdaptiveOntology] Split cluster {cluster_id[:8]} ‚Üí "
            f"{cluster_id1[:8]} ('{canonical1}') + "
            f"{cluster_id2[:8]} ('{canonical2}')"
        )

    def _delete_cluster(self, cluster_id: str):
        """Supprime cluster de Neo4j"""
        if not self.neo4j_client:
            return

        try:
            query = """
            MATCH (c:AdaptiveCluster {cluster_id: $cluster_id, tenant_id: $tenant_id})
            DELETE c
            """

            self.neo4j_client.execute_query(
                query,
                {"cluster_id": cluster_id, "tenant_id": self.tenant_id}
            )

        except Exception as e:
            logger.error(f"[AdaptiveOntology] Failed to delete cluster: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """
        Retourne statistiques ontologie (pour dashboard admin).
        """
        if not self.clusters:
            return {
                "cluster_count": 0,
                "total_variants": 0,
                "avg_variants_per_cluster": 0.0,
                "total_mentions": 0,
                "avg_confidence": 0.0
            }

        total_variants = sum(len(c.variants) for c in self.clusters.values())
        total_mentions = sum(c.mention_count for c in self.clusters.values())
        avg_confidence = np.mean([c.confidence for c in self.clusters.values()])

        return {
            "cluster_count": len(self.clusters),
            "total_variants": total_variants,
            "avg_variants_per_cluster": total_variants / len(self.clusters),
            "total_mentions": total_mentions,
            "avg_confidence": float(avg_confidence),
            "most_mentioned_clusters": self._get_top_clusters(10)
        }

    def _get_top_clusters(self, n: int = 10) -> List[Dict]:
        """Top N clusters par mention_count"""
        sorted_clusters = sorted(
            self.clusters.values(),
            key=lambda c: c.mention_count,
            reverse=True
        )[:n]

        return [
            {
                "cluster_id": c.cluster_id,
                "canonical_name": c.canonical_name,
                "mention_count": c.mention_count,
                "variants_count": len(c.variants)
            }
            for c in sorted_clusters
        ]
```

#### Fichier: `src/knowbase/semantic/adaptive_ontology.py`

---

### 4.3 Int√©gration dans Pipeline

**Modification des composants existants pour utiliser architecture Zero-Config.**

#### 4.3.1 Gatekeeper Delegate

```python
# src/knowbase/agents/gatekeeper/gatekeeper.py

class GatekeeperDelegate(BaseAgent):
    """
    Gatekeeper avec support AdaptiveOntology (Phase 2.0).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(AgentRole.GATEKEEPER, config)

        # Mode configuration
        self.mode = config.get("mode", "zero_config")  # "zero_config" | "custom"

        # Lazy-init EntityNormalizer
        self._entity_normalizer = None

        # Lazy-init AdaptiveOntology
        self._adaptive_ontology = None

    def _get_adaptive_ontology(self, tenant_id: str) -> AdaptiveOntology:
        """
        Lazy-init AdaptiveOntology (singleton par tenant).
        """
        if self._adaptive_ontology is None:
            from ...semantic.adaptive_ontology import AdaptiveOntology

            self._adaptive_ontology = AdaptiveOntology(
                tenant_id=tenant_id,
                neo4j_client=self.neo4j_client,
                llm_router=get_llm_router()
            )

            logger.info(
                f"[GATEKEEPER] AdaptiveOntology initialized "
                f"(tenant={tenant_id}, clusters={len(self._adaptive_ontology.clusters)})"
            )

        return self._adaptive_ontology

    def _promote_concepts_tool(self, tool_input: PromoteConceptsInput) -> ToolOutput:
        """
        Tool PromoteConcepts avec normalisation adaptive.
        """
        concepts = tool_input.concepts
        promoted_count = 0
        failed_count = 0

        for concept in concepts:
            concept_name = concept.get("name", "")
            concept_type = concept.get("type", "Unknown")
            tenant_id = concept.get("tenant_id", "default")

            # NORMALISATION ADAPTIVE (remplace EntityNormalizer statique)
            if self.mode == "zero_config":
                # Mode Zero-Config: AdaptiveOntology
                adaptive_ontology = self._get_adaptive_ontology(tenant_id)

                norm_result = adaptive_ontology.normalize(
                    raw_name=concept_name,
                    context=concept.get("context", ""),
                    concept_type=concept_type
                )

                canonical_name = norm_result.canonical_name
                confidence = norm_result.confidence

                logger.info(
                    f"[GATEKEEPER:Adaptive] Normalized '{concept_name}' ‚Üí '{canonical_name}' "
                    f"(method={norm_result.method}, confidence={confidence:.2f})"
                )
            else:
                # Mode Custom: Utiliser EntityNormalizer si fourni
                if self.entity_normalizer:
                    # Legacy normalization (ontologie statique)
                    entity_id, canonical_name, _, is_cataloged = \
                        self.entity_normalizer.normalize_entity_name(
                            raw_name=concept_name,
                            entity_type_hint=concept_type,
                            tenant_id=tenant_id
                        )
                else:
                    # Fallback: Pas de normalisation
                    canonical_name = concept_name
                    confidence = 0.75

            # Promotion vers Neo4j Published-KG
            # ... (reste du code inchang√©)
```

#### 4.3.2 OSMOSE Agentique

```python
# src/knowbase/ingestion/osmose_agentique.py

class OsmoseAgentiqueService:
    """
    Service orchestration OSMOSE avec auto-d√©tection domaine (Phase 2.0).
    """

    def __init__(self):
        # ... (init existant)

        # Lazy-init AutoDomainDetector
        self._domain_detector = None

    def _get_domain_detector(self) -> AutoDomainDetector:
        """Lazy-init AutoDomainDetector"""
        if self._domain_detector is None:
            from ..semantic.domain_detector import AutoDomainDetector

            self._domain_detector = AutoDomainDetector(
                llm_router=get_llm_router()
            )

            logger.info("[OSMOSE] AutoDomainDetector initialized")

        return self._domain_detector

    async def process_document(
        self,
        document_id: str,
        document_title: str,
        document_path: Path,
        text_content: str,
        tenant: str = "default"
    ) -> Dict[str, Any]:
        """
        Process document avec auto-d√©tection domaine.
        """
        # √âtape 0: Auto-d√©tection domaine (transparent)
        domain_detector = self._get_domain_detector()
        domain_result = domain_detector.detect(text_content)

        logger.info(
            f"[OSMOSE] Auto-detected domain: {domain_result.domain} "
            f"(confidence={domain_result.confidence:.2f}, method={domain_result.method})"
        )

        # Reste du traitement inchang√©
        # ... (SupervisorAgent FSM, etc.)
```

---

## 5Ô∏è‚É£ Plan de Migration

### Phase 1: Foundation (Semaine 1) - 5 jours

**Objectif:** Impl√©menter composants core Zero-Config sans casser code existant.

#### Jour 1-2: AutoDomainDetector
- [ ] Cr√©er `src/knowbase/semantic/domain_detector.py`
- [ ] Impl√©menter signatures domaines par d√©faut
- [ ] Tests unitaires (5 domaines √ó 3 tests = 15 tests)
- [ ] Documentation API

#### Jour 3-4: AdaptiveOntology
- [ ] Cr√©er `src/knowbase/semantic/adaptive_ontology.py`
- [ ] Impl√©menter clustering s√©mantique
- [ ] Persistence Neo4j (node `AdaptiveCluster`)
- [ ] Tests unitaires (10 tests)

#### Jour 5: Int√©gration Gatekeeper
- [ ] Modifier `GatekeeperDelegate` pour support mode `zero_config`
- [ ] Backward compatibility (mode `custom` garde ancien comportement)
- [ ] Tests int√©gration (3 tests)

**Deliverables:**
- ‚úÖ 2 nouveaux modules Python (domain_detector, adaptive_ontology)
- ‚úÖ 28 tests passants
- ‚úÖ Backward compatible (ancien code fonctionne toujours)

---

### Phase 2: Prompts & Config (Semaine 2) - 3 jours

**Objectif:** G√©n√©raliser prompts et config pour domain-agnostic.

#### Jour 1: Prompts LLM
- [ ] Modifier `config/prompts.yaml` (supprimer 7 r√©f√©rences "SAP")
- [ ] Templates g√©n√©riques: "Use vendor official name" (pas "SAP name")
- [ ] Tests regression (valider extraction toujours OK)

#### Jour 2: Config Dynamique
- [ ] Cr√©er `src/knowbase/config/zero_config.py`
- [ ] Wrapper mode s√©lection: `zero_config` vs `custom`
- [ ] Environment variable: `KNOWBASE_MODE=zero_config` (default)

#### Jour 3: Documentation
- [ ] Mettre √† jour README avec mode Zero-Config
- [ ] Guide migration existant ‚Üí nouveau mode
- [ ] Changelog

**Deliverables:**
- ‚úÖ Prompts domain-agnostic
- ‚úÖ Config mode s√©lectionnable
- ‚úÖ Documentation √† jour

---

### Phase 3: UI Admin (Semaine 3) - 5 jours

**Objectif:** Interface admin pour review clusters (optionnel mais utile).

#### Jour 1-2: Backend API
- [ ] Route GET `/api/ontology/adaptive/clusters` (liste clusters)
- [ ] Route POST `/api/ontology/adaptive/correct` (correction humaine)
- [ ] Route POST `/api/ontology/adaptive/merge` (fusionner clusters)
- [ ] Route POST `/api/ontology/adaptive/split` (split cluster)

#### Jour 3-4: Frontend UI
- [ ] Page `frontend/src/app/admin/ontology/page.tsx`
- [ ] Composant `<ClusterReview>` (review clusters auto-d√©tect√©s)
- [ ] Composant `<OntologyImport>` (import YAML/CSV optionnel)
- [ ] Composant `<QualityMetrics>` (dashboard stats)

#### Jour 5: Tests E2E
- [ ] Playwright tests (workflow complet)
- [ ] Validation UX

**Deliverables:**
- ‚úÖ 4 routes API nouvelles
- ‚úÖ Interface admin fonctionnelle
- ‚úÖ Tests E2E passants

---

### Phase 4: Testing & Validation (Semaine 4) - 5 jours

**Objectif:** Validation qualit√© sur datasets multi-domaines.

#### Jour 1-2: Datasets Pr√©paration
- [ ] Dataset SAP (50 docs existants)
- [ ] Dataset Pharma (20 docs publics: FDA, EMA)
- [ ] Dataset Finance (20 docs publics: Basel, MiFID)
- [ ] Ground truth annotations (canonical names attendus)

#### Jour 3-4: Tests Qualit√©
- [ ] Mesure pr√©cision normalisation (SAP, Pharma, Finance)
- [ ] Courbes √©volution qualit√© (0, 50, 100, 200 docs)
- [ ] Comparaison Zero-Config vs Custom
- [ ] M√©triques latence/co√ªt LLM

#### Jour 5: Ajustements
- [ ] Tuning seuils (similarity_threshold, etc.)
- [ ] Optimisations performances
- [ ] Documentation r√©sultats

**Deliverables:**
- ‚úÖ Rapport qualit√© (3 domaines test√©s)
- ‚úÖ M√©triques publi√©es
- ‚úÖ Validation succ√®s

---

### Timeline Global

```
Semaine 1: Foundation (AutoDomainDetector + AdaptiveOntology)
    ‚îú‚îÄ Jour 1-2: domain_detector.py
    ‚îú‚îÄ Jour 3-4: adaptive_ontology.py
    ‚îî‚îÄ Jour 5:   Int√©gration Gatekeeper

Semaine 2: Prompts & Config
    ‚îú‚îÄ Jour 1: Prompts domain-agnostic
    ‚îú‚îÄ Jour 2: Config mode s√©lectionnable
    ‚îî‚îÄ Jour 3: Documentation

Semaine 3: UI Admin
    ‚îú‚îÄ Jour 1-2: Backend API (4 routes)
    ‚îú‚îÄ Jour 3-4: Frontend UI (3 composants)
    ‚îî‚îÄ Jour 5:   Tests E2E

Semaine 4: Testing & Validation
    ‚îú‚îÄ Jour 1-2: Datasets pr√©paration
    ‚îú‚îÄ Jour 3-4: Tests qualit√© multi-domaines
    ‚îî‚îÄ Jour 5:   Ajustements & rapport

TOTAL: 18 jours d√©veloppement + 2 jours validation = 4 semaines
```

---

## 5Ô∏è‚É£ Comparaison Option C vs C+

### üìä Tableau Comparatif

| Aspect | **Option C (self_learning)** | **Option C+ (bootstrap)** |
|--------|------------------------------|---------------------------|
| **Configuration** | ‚úÖ Zero (d√©faut `.env`) | ‚úÖ Zero (d√©faut `.env`) |
| **Signatures hard-cod√©es** | ‚ùå Aucune | ‚ö° 5 domaines minimaux |
| **Universel (tous domaines)** | ‚úÖ 100% (retail, energy, legal...) | ‚ö†Ô∏è 90% (biais vers 5 domaines) |
| **Co√ªt LLM (5 premiers docs)** | $0.06 (5 √ó $0.012) | $0.02 (1-2 LLM calls) |
| **Co√ªt LLM (50 docs)** | $0.25 (5 LLM + 45 clusters) | $0.20 (5 signatures + auto-switch) |
| **Co√ªt LLM (200 docs)** | $0.40 (bootstrap + 95% clusters) | $0.35 (bootstrap + switch rapide) |
| **Latence moyenne (docs 1-5)** | 500ms (LLM) | 50ms (keywords) |
| **Latence moyenne (docs 50+)** | 8ms (cluster match) | 8ms (cluster match) |
| **Qualit√© (200 docs)** | 95% | 94% (biais signatures) |
| **Adaptabilit√©** | ‚úÖ Auto-d√©couverte | ‚ö†Ô∏è Biais initial |
| **Multi-tenant intelligent** | ‚úÖ Clusters par tenant | ‚úÖ Clusters par tenant |

### üéØ Recommandations d'Usage

#### Option C (`self_learning`) - **D√âFAUT PROD**

**Quand l'utiliser** :
- ‚úÖ **Production client** : Garantit universalit√© totale
- ‚úÖ **Domaines inconnus** : Retail, energy, legal, education, etc.
- ‚úÖ **Multi-tenant SaaS** : Chaque tenant a son propre domaine
- ‚úÖ **Scalabilit√© long terme** : Co√ªt d√©croissant avec usage

**Exemple .env** :
```bash
# Production - Self-Learning pur (universel)
DOMAIN_DETECTION_MODE=self_learning
DOMAIN_CLUSTER_SIMILARITY_THRESHOLD=0.75
```

**Comportement** :
- Document 1 ‚Üí LLM d√©tecte "retail" ($0.012) ‚Üí Cr√©e cluster
- Documents 2-5 ‚Üí Match cluster "retail" (gratuit, 5ms)
- Document 50 (nouveau) ‚Üí LLM d√©tecte "energy" ‚Üí Nouveau cluster
- Document 100+ ‚Üí 95% cluster matching (gratuit)

---

#### Option C+ (`bootstrap`) - **TESTS / DEV**

**Quand l'utiliser** :
- ‚úÖ **D√©veloppement local** : Bootstrap rapide avec donn√©es SAP/Pharma/Finance
- ‚úÖ **Tests unitaires** : Latence faible sans attente LLM
- ‚úÖ **D√©mos commerciales** : D√©tection imm√©diate sur domaines courants
- ‚úÖ **Environnement CI/CD** : Co√ªt LLM r√©duit

**Exemple .env** :
```bash
# Dev/Tests - Bootstrap rapide
DOMAIN_DETECTION_MODE=bootstrap
DOMAIN_CLUSTER_SIMILARITY_THRESHOLD=0.75
DOMAIN_BOOTSTRAP_MIN_DOCS=5  # Switch auto apr√®s 5 docs
```

**Comportement** :
- Documents 1-5 ‚Üí Keyword matching sur signatures (gratuit, 50ms)
- Parall√®lement ‚Üí Apprentissage clusters en arri√®re-plan
- Document 6+ ‚Üí **Auto-switch** vers mode self_learning
- Document 50+ ‚Üí Identique √† Option C (clusters uniquement)

### üí° Exemple Concret : Client Retailer

#### Avec Option C (self_learning)
```
Doc 1 "Walmart_Inventory.pdf" ‚Üí LLM: "retail" ($0.012, 480ms) ‚Üí Cluster cr√©√©
Doc 2 "Target_Supply.pdf"      ‚Üí Cluster match (gratuit, 6ms) ‚úÖ
Doc 3 "Amazon_Logistics.pdf"   ‚Üí Cluster match (gratuit, 5ms) ‚úÖ
Doc 4 "Nike_Merchandising.pdf" ‚Üí Cluster match (gratuit, 7ms) ‚úÖ
Doc 5 "Tesla_Battery.pdf"      ‚Üí LLM: "automotive" ($0.012, 490ms) ‚Üí Nouveau cluster

Total co√ªt : $0.024
Total latence moyenne : 120ms/doc
Domaines d√©couverts : retail, automotive (‚úÖ universel)
```

#### Avec Option C+ (bootstrap)
```
Doc 1 "Walmart_Inventory.pdf" ‚Üí Keywords: ‚ùå Pas match signatures ‚Üí LLM: "retail" ($0.012, 480ms)
Doc 2 "Target_Supply.pdf"      ‚Üí Keywords: ‚ùå Pas match ‚Üí Cluster (learning BG, 8ms)
Doc 3-5 similaire
Doc 6+                         ‚Üí Auto-switch vers clusters ‚Üí Gratuit

Total co√ªt : $0.012-0.024 (selon matching)
Total latence moyenne : 100ms/doc
Domaines d√©couverts : retail, automotive (‚úÖ mais d√©tour initial)
```

**Verdict** : Option C plus coh√©rente pour client retailer (domaine non couvert par signatures).

---

### ‚öôÔ∏è Migration Entre Modes

**Mode dynamique possible** :
```python
# Dans osmose_agentique.py
detector = AutoDomainDetector(
    llm_router=llm_router,
    neo4j_client=neo4j_client,
    embeddings_model=embeddings_model
)

# Mode auto-d√©tect√© via .env
result = detector.detect(
    document_text=text,
    document_id=doc_id,
    tenant_id=tenant
)

logger.info(
    f"Domain detected: {result.domain} "
    f"(method={result.method}, confidence={result.confidence:.2f}, "
    f"time={result.execution_time_ms:.1f}ms)"
)
```

**Pas de code √† changer** : Switch entre C et C+ via `.env` uniquement.

---

## 6Ô∏è‚É£ M√©triques de Succ√®s

### M√©triques Techniques

| M√©trique | Baseline (Actuel SAP-only) | Target (Zero-Config) | Mesure |
|----------|---------------------------|---------------------|---------|
| **Config initiale** | 2-4 heures | 0 minutes ‚úÖ | Temps setup tenant |
| **Qualit√© Day-1** | 95% (avec catalogue) | 80-85% üü° | F1-score normalisation |
| **Qualit√© Semaine-4** | 95% | 85-90% ‚úÖ | F1-score apr√®s 50 docs |
| **Qualit√© Mois-3** | 95% | 93-95% ‚úÖ | F1-score apr√®s 200 docs |
| **Adaptabilit√©** | 1 domaine (SAP) | Illimit√© ‚úÖ | Nb domaines support√©s |
| **Co√ªt LLM/doc** | $0.15 (avec catalogue) | $0.18 (+20%) üü° | Cost analysis |
| **Latence normalisation** | 50ms (lookup cache) | 120ms (embedding + LLM) üü° | P95 latency |

### M√©triques Business

| M√©trique | Baseline | Target | Impact |
|----------|----------|--------|--------|
| **Time-to-Value** | 1 semaine (config + training) | 30 minutes ‚úÖ | Onboarding client |
| **TAM (Total Addressable Market)** | $50B (SAP ecosystem) | $1.5T (multi-industry) ‚úÖ | Revenue potential |
| **Churn Risk** | √âlev√© (si client non-SAP) | Faible (adaptatif) ‚úÖ | Customer retention |
| **Support Tickets** | 15/mois (config help) | 5/mois (-66%) ‚úÖ | Ops cost |

### M√©triques Qualit√© (KPIs)

**√Ä mesurer sur 3 datasets (SAP, Pharma, Finance):**

1. **Precision Normalisation**
   - Formule: `correct_normalizations / total_normalizations`
   - Target: 85% (Day-1), 90% (Semaine-4), 95% (Mois-3)

2. **Recall Concept Extraction**
   - Formule: `concepts_found / concepts_expected`
   - Target: 80% (constant, pas de r√©gression vs baseline)

3. **Cluster Purity**
   - Formule: `correct_variants_in_cluster / total_variants_in_cluster`
   - Target: 90% (Semaine-4), 95% (Mois-3)

4. **User Satisfaction (NPS)**
   - Sondage: "How likely would you recommend Zero-Config mode?"
   - Target: NPS > 50 (promoters > detractors)

---

## 7Ô∏è‚É£ Annexes Techniques

### Annexe A: Sch√©ma Neo4j

**Nouveau node type : `AdaptiveCluster`**

```cypher
// Cr√©er contrainte
CREATE CONSTRAINT adaptive_cluster_id IF NOT EXISTS
FOR (c:AdaptiveCluster) REQUIRE (c.cluster_id, c.tenant_id) IS NODE KEY;

// Cr√©er index
CREATE INDEX adaptive_cluster_canonical IF NOT EXISTS
FOR (c:AdaptiveCluster) ON (c.canonical_name);

// Structure node
(:AdaptiveCluster {
  cluster_id: "uuid",
  tenant_id: "default",
  canonical_name: "SAP S/4HANA Cloud, Private Edition",
  variants: ["S4 PCE", "S/4HANA Private", "SAP S4 Private Cloud"],
  centroid: [0.123, 0.456, ...],  // Embedding (1024D)
  mention_count: 47,
  confidence: 0.92,
  concept_type: "ENTITY",
  domain: "technology",
  created_at: datetime(),
  updated_at: datetime()
})
```

---

### Annexe B: Co√ªts LLM Compar√©s

**Hypoth√®ses:**
- Document moyen: 5000 tokens
- Concepts extraits: 15/document
- LLM: GPT-4o ($2.50/1M tokens input, $10/1M tokens output)

#### Mode Actuel (avec catalogue)
```
Extraction metadata: 5000 tokens input + 200 output = $0.0145
Extraction concepts: 15 √ó (300 tokens input + 50 output) = $0.0195
Normalisation: 15 √ó fuzzy match (0 cost) = $0.00
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL: $0.034/document
```

#### Mode Zero-Config
```
Extraction metadata: 5000 tokens input + 200 output = $0.0145
Extraction concepts: 15 √ó (300 tokens input + 50 output) = $0.0195
Normalisation:
  - 10 matches cluster (0 cost) = $0.00
  - 5 nouveaux clusters (LLM canonical name):
    5 √ó (200 tokens input + 20 output) = $0.0030
Auto-detection domaine: 3000 tokens input + 50 output = $0.0080
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL: $0.045/document (+32% vs actuel)
```

**Conclusion:** Co√ªt l√©g√®rement sup√©rieur (acceptable pour gain en autonomie).

**Optimisations possibles:**
- Cache LLM canonical names (si m√™me nom brut r√©appara√Æt)
- Batch LLM calls (5 canonical names en 1 appel)
- ‚Üí Ram√®nerait co√ªt √† ~$0.038/document (+12% seulement)

---

### Annexe C: Exemples D√©tection Domaine

**Exemple 1: Document Pharmaceutical**

```python
text = """
Clinical trial protocol for Phase 3 study of mRNA-1273 vaccine.
Study design follows FDA guidance and ICH GCP standards.
Primary endpoint: vaccine efficacy against COVID-19 infection.
Safety monitoring per 21 CFR Part 11 requirements.
Adverse events reported to EMA within 24 hours.
"""

result = domain_detector.detect(text)
# Output:
# DomainDetectionResult(
#   domain="pharmaceutical",
#   confidence=0.94,
#   method="keyword_density",
#   signals={
#     "pharmaceutical": 0.94,
#     "finance": 0.02,
#     "technology": 0.04
#   }
# )
```

**Exemple 2: Document Finance**

```python
text = """
Trading strategy for equity derivatives portfolio.
Compliance with MiFID II and EMIR reporting requirements.
Risk metrics: VaR 99%, Expected Shortfall, stress testing.
Collateral management via Bloomberg Terminal integration.
Basel III capital adequacy maintained above regulatory minimum.
"""

result = domain_detector.detect(text)
# Output:
# DomainDetectionResult(
#   domain="finance",
#   confidence=0.89,
#   method="keyword_density",
#   signals={
#     "pharmaceutical": 0.03,
#     "finance": 0.89,
#     "technology": 0.08
#   }
# )
```

**Exemple 3: Document Ambigue (‚Üí LLM arbitrage)**

```python
text = """
Project Phoenix: Digital transformation initiative.
Objectives: Improve operational efficiency, reduce costs.
Stakeholders: IT, Finance, Operations departments.
Timeline: 18 months, budget $2M.
"""

# Keyword scores trop faibles ‚Üí LLM arbitrage
result = domain_detector.detect(text)
# Output:
# DomainDetectionResult(
#   domain="consulting",
#   confidence=0.75,
#   method="llm_zero_shot",
#   signals={"consulting": 0.75, "general": 0.25}
# )
```

---

### Annexe D: FAQ

#### Q1: Que se passe-t-il si l'AdaptiveOntology se trompe ?

**R:** Feedback loop via UI admin.

1. Admin voit normalisation incorrecte dans dashboard
2. Corrige via UI: "S4 PCE" devrait √™tre "SAP S/4HANA" (pas "S4 Private Cloud Edition")
3. Syst√®me apprend ‚Üí Mise √† jour cluster
4. Future normalizations corrig√©es automatiquement

**M√©canisme:**
```python
adaptive_ontology.learn_from_correction(
    raw_name="S4 PCE",
    corrected_canonical="SAP S/4HANA Cloud, Private Edition"
)
```

---

#### Q2: Comment importer une ontologie custom si souhait√© ?

**R:** Via UI admin ou API.

**Option 1: UI Admin**
```typescript
// Upload YAML/CSV
<OntologyImport onImport={file => importCustomOntology(file)} />
```

**Option 2: API**
```bash
POST /api/ontology/adaptive/import
Content-Type: multipart/form-data

{
  "file": ontology_custom.yaml,
  "mode": "merge" | "replace"  # Fusionner ou remplacer clusters existants
}
```

**Format YAML attendu:**
```yaml
clusters:
  - canonical_name: "Internal CRM System v3.2"
    variants: ["CRM", "Customer System", "Sales Platform"]
    concept_type: "ENTITY"
    confidence: 1.0
```

---

#### Q3: Combien de documents faut-il pour atteindre 95% qualit√© ?

**R:** D√©pend de la diversit√© vocabulaire.

**Estimations:**
- **Domaine homog√®ne** (ex: docs internes entreprise, vocabulaire r√©current):
  - 50-100 documents ‚Üí 95% qualit√©

- **Domaine h√©t√©rog√®ne** (ex: docs publics multi-sources):
  - 200-300 documents ‚Üí 95% qualit√©

**Acc√©l√©ration possible:**
- Import ontologie partielle (10-20 concepts cl√©s)
- ‚Üí R√©duit besoin documents √† ~50 pour 95%

---

#### Q4: Le mode Zero-Config augmente-t-il les co√ªts LLM ?

**R:** Oui, +32% co√ªt/document initialement, mais d√©cro√Æt avec usage.

**√âvolution co√ªts:**
```
Document 1-50:   +32% co√ªt (beaucoup de nouveaux clusters ‚Üí LLM)
Document 50-200: +15% co√ªt (moins nouveaux clusters)
Document 200+:   +5% co√ªt (rare nouveaux clusters, matching clusters existants)
```

**Optimisations impl√©ment√©es:**
- Cache LLM canonical names (si r√©p√©tition exacte)
- Batch LLM calls (plusieurs noms en 1 appel)
- ‚Üí Co√ªt final: +10-15% seulement vs mode catalogue statique

**Trade-off acceptable** pour gain autonomie + adaptabilit√©.

---

#### Q5: Peut-on d√©sactiver mode Zero-Config et revenir au mode catalogue ?

**R:** Oui, backward compatible complet.

**Configuration:**
```yaml
# .env ou config/knowbase.yaml
KNOWBASE_MODE=custom  # ou "zero_config" (default)
```

**Si `mode=custom`:**
- Gatekeeper utilise `EntityNormalizer` (ancien comportement)
- Catalogue `sap_solutions.yaml` requis
- Pas d'AdaptiveOntology

**Si `mode=zero_config`:**
- Gatekeeper utilise `AdaptiveOntology`
- Catalogue optionnel (ignor√©)
- Auto-learning activ√©

---

## üéØ Conclusion

### Synth√®se Approche

**Architecture Zero-Config + Self-Learning** √©limine d√©pendances m√©tier tout en maintenant qualit√© production gr√¢ce √†:

1. **LLM Extraction Pure** (connaissances internes GPT-4o/Claude)
2. **Auto-D√©tection Domaine** (keyword + NER + LLM zero-shot)
3. **Ontologie Adaptive** (clustering s√©mantique auto-am√©lioration)
4. **Feedback Loop** (corrections humaines optionnelles)

### Diff√©renciation March√©

| Crit√®re | Microsoft Copilot | Google Gemini | **KnowWhere Zero-Config** |
|---------|-------------------|---------------|---------------------------|
| Config initiale | Aucune | Aucune | Aucune ‚úÖ |
| Ontologie m√©tier | ‚ùå Non | ‚ùå Non | ‚úÖ Auto-construite |
| M√©moire vocabulaire | ‚ùå Aucune | ‚ùå Aucune | ‚úÖ Persistent (Neo4j) |
| Am√©lioration avec usage | ‚ùå Non | ‚ùå Non | ‚úÖ Self-learning |
| Multi-tenant | ‚úÖ Oui | ‚úÖ Oui | ‚úÖ 1 ontologie/client |

**USP unique:**
> "La seule solution qui apprend VOTRE vocabulaire m√©tier sans configuration"

### Next Steps

1. **Validation stakeholders** : Approuver sp√©cifications
2. **Kickoff d√©veloppement** : Semaine 1 (Foundation)
3. **POC multi-domaine** : Tester sur SAP + Pharma + Finance
4. **It√©ration feedback** : Ajuster based on r√©sultats POC
5. **Release Phase 2.0** : Mode Zero-Config en production

---

**Document r√©dig√© par:** Claude Code (Architecture Agent)
**Version:** 1.0
**Date:** 2025-10-17
**Statut:** Sp√©cifications compl√®tes - Pr√™t pour impl√©mentation
