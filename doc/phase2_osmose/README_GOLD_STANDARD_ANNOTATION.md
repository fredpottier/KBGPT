# ğŸ“Š Script Annotation Gold Standard - Phase 2

**Objectif:** CrÃ©er un dataset de rÃ©fÃ©rence avec 450 relations annotÃ©es manuellement (50 par type core) pour valider la precision/recall du RelationExtractionEngine.

---

## ğŸ¯ Qu'est-ce que le Gold Standard ?

### DÃ©finition

Le **Gold Standard** est un dataset de rÃ©fÃ©rence annotÃ© **manuellement par des humains** qui sert de vÃ©ritÃ© terrain pour :

1. **EntraÃ®ner** des algorithmes (si ML supervisÃ©)
2. **Valider** la performance d'extraction automatique
3. **Calculer** precision, recall, F1-score
4. **Comparer** diffÃ©rentes approches (pattern-based vs LLM)

### Exemple Concret

```json
{
  "relation_id": "gold_001",
  "source_concept": "SAP Fiori",
  "target_concept": "SAP S/4HANA Cloud",
  "relation_type": "PART_OF",
  "context": "SAP Fiori is a component of SAP S/4HANA Cloud providing user experience layer...",
  "document_id": "doc_sap_s4hana_overview.pptx",
  "chunk_id": "chunk_12",
  "annotator": "john.doe@company.com",
  "confidence_human": 1.0,
  "notes": "Clear compositional relationship, explicitly stated",
  "created_at": "2025-10-19T14:30:00Z"
}
```

---

## ğŸ› ï¸ Script `annotate_relations_gold_standard.py`

### Vue d'Ensemble

```python
# scripts/annotate_relations_gold_standard.py

"""
Script interactif pour annoter manuellement des relations entre concepts.

Usage:
    python scripts/annotate_relations_gold_standard.py \
        --corpus data/phase2_test/ \
        --types PART_OF,REQUIRES,USES,INTEGRATES_WITH,SUBTYPE_OF,VERSION_OF,PRECEDES,REPLACES,DEPRECATES \
        --samples_per_type 50 \
        --annotators 2 \
        --output data/phase2_gold_standard.json

Output:
    - data/phase2_gold_standard.json : Relations annotÃ©es
    - data/phase2_gold_standard_stats.json : Statistiques inter-annotator agreement
"""
```

---

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. EXTRACTION CANDIDATE RELATIONS                      â”‚
â”‚    - Parse corpus documents (PPTX/PDF)                 â”‚
â”‚    - Detect concept pairs co-occurring                 â”‚
â”‚    - Generate 500+ candidate relations                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. SAMPLING STRATIFIED                                 â”‚
â”‚    - Sample 50 relations par type (balanced)           â”‚
â”‚    - Assurer diversitÃ© domaines (Software, Pharma...)  â”‚
â”‚    - Ã‰viter biais sur documents populaires             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ANNOTATION INTERFACE (CLI ou Web)                   â”‚
â”‚    - PrÃ©senter context chunk                           â”‚
â”‚    - Proposer (Concept A, Concept B)                   â”‚
â”‚    - Demander : Relation type ? [PART_OF|NONE|...]     â”‚
â”‚    - Valider : Confidence ? [0.5|0.75|1.0]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. INTER-ANNOTATOR AGREEMENT                           â”‚
â”‚    - 2 annotateurs indÃ©pendants                        â”‚
â”‚    - Cohen's Kappa calculation                         â”‚
â”‚    - RÃ©solution conflits (3e annotateur si Kappa<0.75) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. EXPORT GOLD STANDARD                                â”‚
â”‚    - JSON avec 450 relations validÃ©es                  â”‚
â”‚    - Stats : Kappa, confusion matrix                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Ã‰tapes DÃ©taillÃ©es

### Ã‰tape 1 : Extraction Candidate Relations

```python
def extract_candidate_relations(corpus_path: str) -> List[CandidateRelation]:
    """
    Parse documents et extrait paires de concepts co-occurrents.

    Returns:
        Liste de 500+ candidates (non-annotÃ©s) pour sÃ©lection.
    """
    candidates = []

    for doc_path in glob(f"{corpus_path}/**/*.pptx"):
        # Parse document (rÃ©utiliser PPTXPipeline Phase 1.5)
        text, chunks = extract_text_chunks(doc_path)

        # Detect concepts (simple NER ou rÃ©utiliser Phase 1.5 concepts)
        concepts = detect_concepts(text)

        # Co-occurrence dans mÃªme chunk (fenÃªtre 500 caractÃ¨res)
        for chunk in chunks:
            chunk_concepts = [c for c in concepts if c.text in chunk.text]

            # GÃ©nÃ©rer paires (A, B)
            for i, concept_a in enumerate(chunk_concepts):
                for concept_b in chunk_concepts[i+1:]:
                    candidates.append(CandidateRelation(
                        source=concept_a.text,
                        target=concept_b.text,
                        context=chunk.text[:500],  # Limite 500 chars
                        document_id=doc_path,
                        chunk_id=chunk.id
                    ))

    return candidates
```

**Output Attendu:**
- ~500-1000 candidate relations extraites du corpus
- Stockage temporaire : `data/phase2_candidates.json`

---

### Ã‰tape 2 : Sampling StratifiÃ©

```python
def stratified_sampling(
    candidates: List[CandidateRelation],
    types: List[str],
    samples_per_type: int = 50
) -> List[CandidateRelation]:
    """
    SÃ©lection Ã©quilibrÃ©e de relations Ã  annoter.

    Args:
        candidates: Liste complÃ¨te candidates
        types: 9 types core relations
        samples_per_type: 50 relations par type

    Returns:
        450 candidates sÃ©lectionnÃ©s (50 Ã— 9 types)
    """
    selected = []

    for relation_type in types:
        # Filtrer candidates pertinents pour ce type (heuristiques basiques)
        # Ex: PART_OF â†’ chercher "component", "module", "part of" dans context
        type_candidates = filter_by_type_heuristic(candidates, relation_type)

        # DiversitÃ© domaines
        balanced = balance_by_domain(type_candidates, domains=["Software", "Pharma", "Retail", "Other"])

        # Sample alÃ©atoire 50
        sampled = random.sample(balanced, min(50, len(balanced)))
        selected.extend(sampled)

    return selected
```

**StratÃ©gie Balancing:**
- 40% Software (SAP, Oracle, etc.)
- 20% Pharma (mÃ©dicaments, essais cliniques)
- 20% Retail (e-commerce, supply chain)
- 20% Other (Manufacturing, Finance, Legal)

---

### Ã‰tape 3 : Interface Annotation CLI

```python
def annotate_cli(candidates: List[CandidateRelation], annotator_id: str) -> List[AnnotatedRelation]:
    """
    Interface CLI interactive pour annotation manuelle.
    """
    annotations = []

    for i, candidate in enumerate(candidates, 1):
        print(f"\n{'='*80}")
        print(f"Relation {i}/{len(candidates)}")
        print(f"{'='*80}")
        print(f"\nDocument: {candidate.document_id}")
        print(f"\nContext:\n{candidate.context}\n")
        print(f"Concept A: [{candidate.source}]")
        print(f"Concept B: [{candidate.target}]")
        print(f"\nQuelle relation existe entre A et B ?")
        print("Options:")
        print("  1. PART_OF         (A est composant de B)")
        print("  2. SUBTYPE_OF      (A est sous-catÃ©gorie de B)")
        print("  3. REQUIRES        (A nÃ©cessite B - obligatoire)")
        print("  4. USES            (A utilise B - optionnel)")
        print("  5. INTEGRATES_WITH (A s'intÃ¨gre avec B)")
        print("  6. VERSION_OF      (A est version de B)")
        print("  7. PRECEDES        (A prÃ©cÃ¨de B chronologiquement)")
        print("  8. REPLACES        (A remplace B)")
        print("  9. DEPRECATES      (A dÃ©prÃ©cie B)")
        print("  0. NONE            (Aucune relation)")

        choice = input("\nVotre choix [0-9]: ").strip()

        if choice == "0":
            relation_type = "NONE"
            confidence = 1.0
        else:
            relation_type = RELATION_TYPES[int(choice)]
            confidence = float(input("Confidence [0.5/0.75/1.0]: ").strip())

        notes = input("Notes optionnelles: ").strip()

        annotations.append(AnnotatedRelation(
            relation_id=f"gold_{annotator_id}_{i:03d}",
            source=candidate.source,
            target=candidate.target,
            relation_type=relation_type,
            context=candidate.context,
            document_id=candidate.document_id,
            chunk_id=candidate.chunk_id,
            annotator=annotator_id,
            confidence_human=confidence,
            notes=notes,
            created_at=datetime.utcnow().isoformat()
        ))

        # Sauv egarde progressive tous les 10 annotations
        if i % 10 == 0:
            save_checkpoint(annotations, f"data/phase2_gold_{annotator_id}_checkpoint.json")

    return annotations
```

**Interface Alternative (Web Streamlit) :**

```python
# scripts/annotate_relations_streamlit.py

import streamlit as st

def annotate_web():
    """Interface web Streamlit pour annotation plus conviviale"""

    st.title("ğŸ“Š Gold Standard Annotation - Phase 2 OSMOSE")

    # Load candidates
    candidates = load_candidates("data/phase2_candidates.json")

    # Progress bar
    progress = st.progress(0)

    # Annotation form
    for i, candidate in enumerate(candidates):
        st.header(f"Relation {i+1}/{len(candidates)}")

        # Context display
        st.text_area("Context", candidate.context, height=150, disabled=True)

        # Concepts
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"**Concept A:** {candidate.source}")
        with col2:
            st.info(f"**Concept B:** {candidate.target}")

        # Relation type selection
        relation_type = st.selectbox(
            "Type de relation",
            ["NONE", "PART_OF", "SUBTYPE_OF", "REQUIRES", "USES",
             "INTEGRATES_WITH", "VERSION_OF", "PRECEDES", "REPLACES", "DEPRECATES"]
        )

        # Confidence
        confidence = st.slider("Confidence", 0.5, 1.0, 1.0, step=0.25)

        # Notes
        notes = st.text_input("Notes optionnelles")

        # Submit
        if st.button("Valider"):
            save_annotation(candidate, relation_type, confidence, notes)
            st.success("Annotation sauvegardÃ©e !")
            progress.progress((i+1) / len(candidates))
```

---

### Ã‰tape 4 : Inter-Annotator Agreement

```python
def calculate_inter_annotator_agreement(
    annotations_a: List[AnnotatedRelation],
    annotations_b: List[AnnotatedRelation]
) -> Dict[str, float]:
    """
    Calcule Cohen's Kappa entre 2 annotateurs.

    Returns:
        {
            "kappa": 0.82,           # Cohen's Kappa
            "agreement_rate": 0.89,  # % accord total
            "confusion_matrix": {...}
        }
    """
    from sklearn.metrics import cohen_kappa_score, confusion_matrix

    # Alignment annotations (mÃªme source, target)
    aligned = align_annotations(annotations_a, annotations_b)

    # Extract labels
    labels_a = [a.relation_type for a in aligned["annotator_a"]]
    labels_b = [b.relation_type for b in aligned["annotator_b"]]

    # Cohen's Kappa
    kappa = cohen_kappa_score(labels_a, labels_b)

    # Agreement rate
    agreement_rate = sum(1 for a, b in zip(labels_a, labels_b) if a == b) / len(labels_a)

    # Confusion matrix
    cm = confusion_matrix(labels_a, labels_b, labels=RELATION_TYPES + ["NONE"])

    return {
        "kappa": kappa,
        "agreement_rate": agreement_rate,
        "confusion_matrix": cm.tolist(),
        "interpretation": interpret_kappa(kappa)
    }

def interpret_kappa(kappa: float) -> str:
    """InterprÃ©tation Cohen's Kappa"""
    if kappa >= 0.81:
        return "âœ… Excellent agreement"
    elif kappa >= 0.61:
        return "âœ… Substantial agreement"
    elif kappa >= 0.41:
        return "âš ï¸ Moderate agreement - Review conflicts"
    else:
        return "âŒ Poor agreement - Retraining needed"
```

**CritÃ¨res QualitÃ©:**
- **Cohen's Kappa â‰¥ 0.75** : Target minimum (substantial agreement)
- **Cohen's Kappa â‰¥ 0.85** : Excellent (idÃ©al pour gold standard)
- **Cohen's Kappa < 0.75** : NÃ©cessite rÃ©solution conflits (3e annotateur)

**RÃ©solution Conflits:**

```python
def resolve_conflicts(
    annotations_a: List[AnnotatedRelation],
    annotations_b: List[AnnotatedRelation],
    kappa: float
) -> List[AnnotatedRelation]:
    """
    RÃ©solution conflits si Kappa < 0.75
    """
    if kappa >= 0.75:
        # MajoritÃ© agreement â†’ merger avec vote majoritaire
        return merge_by_majority_vote(annotations_a, annotations_b)

    # Identifier dÃ©saccords
    conflicts = []
    for ann_a, ann_b in zip(annotations_a, annotations_b):
        if ann_a.relation_type != ann_b.relation_type:
            conflicts.append((ann_a, ann_b))

    print(f"âš ï¸ {len(conflicts)} conflicts detected (Kappa={kappa:.2f})")

    # 3e annotateur pour rÃ©solution
    print("ğŸ” Requiring 3rd annotator for conflict resolution...")
    resolved = annotate_conflicts_by_third_annotator(conflicts)

    # Merger avec rÃ©solutions
    final = merge_with_resolutions(annotations_a, annotations_b, resolved)

    return final
```

---

### Ã‰tape 5 : Export Gold Standard

```python
def export_gold_standard(
    annotations: List[AnnotatedRelation],
    output_path: str = "data/phase2_gold_standard.json"
):
    """
    Export final gold standard avec stats.
    """
    # Filter out NONE relations
    valid_relations = [a for a in annotations if a.relation_type != "NONE"]

    # Statistics
    stats = {
        "total_relations": len(valid_relations),
        "relations_per_type": {
            rel_type: len([a for a in valid_relations if a.relation_type == rel_type])
            for rel_type in RELATION_TYPES
        },
        "domains_distribution": calculate_domain_distribution(valid_relations),
        "avg_confidence": np.mean([a.confidence_human for a in valid_relations]),
        "inter_annotator_kappa": 0.82,  # From previous step
        "created_at": datetime.utcnow().isoformat()
    }

    # Export JSON
    output = {
        "metadata": stats,
        "relations": [asdict(a) for a in valid_relations]
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"âœ… Gold Standard exported: {output_path}")
    print(f"   Total relations: {len(valid_relations)}")
    print(f"   Cohen's Kappa: {stats['inter_annotator_kappa']}")
    print(f"   Avg confidence: {stats['avg_confidence']:.2f}")
```

**Output Format:**

```json
{
  "metadata": {
    "total_relations": 423,
    "relations_per_type": {
      "PART_OF": 48,
      "SUBTYPE_OF": 45,
      "REQUIRES": 50,
      "USES": 47,
      "INTEGRATES_WITH": 43,
      "VERSION_OF": 50,
      "PRECEDES": 46,
      "REPLACES": 49,
      "DEPRECATES": 45
    },
    "domains_distribution": {
      "Software": 169,
      "Pharma": 85,
      "Retail": 85,
      "Other": 84
    },
    "avg_confidence": 0.92,
    "inter_annotator_kappa": 0.82,
    "created_at": "2025-10-19T18:45:00Z"
  },
  "relations": [
    {
      "relation_id": "gold_001",
      "source": "SAP Fiori",
      "target": "SAP S/4HANA Cloud",
      "relation_type": "PART_OF",
      "context": "SAP Fiori is a component of SAP S/4HANA Cloud...",
      "document_id": "doc_sap_s4hana_overview.pptx",
      "chunk_id": "chunk_12",
      "annotator": "john.doe@company.com",
      "confidence_human": 1.0,
      "notes": "Clear compositional relationship",
      "created_at": "2025-10-19T14:30:00Z"
    }
    // ... 422 autres relations
  ]
}
```

---

## ğŸ“ˆ Utilisation pour Validation

### Calcul Precision/Recall

```python
# scripts/evaluate_relation_extraction.py

def evaluate_against_gold_standard(
    gold_standard_path: str,
    predictions_path: str
) -> Dict[str, float]:
    """
    Compare prÃ©dictions automatiques vs gold standard.

    Returns:
        {
            "precision": 0.82,
            "recall": 0.67,
            "f1_score": 0.74,
            "per_type_metrics": {...}
        }
    """
    from sklearn.metrics import precision_recall_fscore_support

    # Load gold standard
    gold = load_json(gold_standard_path)["relations"]

    # Load predictions (extraction automatique)
    predictions = load_json(predictions_path)

    # Alignment (mÃªme source, target)
    aligned_gold, aligned_pred = align_gold_with_predictions(gold, predictions)

    # Extract labels
    y_true = [g["relation_type"] for g in aligned_gold]
    y_pred = [p["relation_type"] for p in aligned_pred]

    # Calculate metrics
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average="weighted"
    )

    # Per-type metrics
    per_type = {}
    for rel_type in RELATION_TYPES:
        type_gold = [1 if g == rel_type else 0 for g in y_true]
        type_pred = [1 if p == rel_type else 0 for p in y_pred]
        p, r, f, _ = precision_recall_fscore_support(type_gold, type_pred, average="binary")
        per_type[rel_type] = {"precision": p, "recall": r, "f1": f}

    return {
        "precision": precision,
        "recall": recall,
        "f1_score": f1,
        "per_type_metrics": per_type
    }
```

**Exemple Utilisation:**

```bash
# 1. CrÃ©er gold standard (S14 J3)
python scripts/annotate_relations_gold_standard.py \
    --corpus data/phase2_test/ \
    --types PART_OF,SUBTYPE_OF,REQUIRES,USES,INTEGRATES_WITH,VERSION_OF,PRECEDES,REPLACES,DEPRECATES \
    --samples_per_type 50 \
    --annotators 2 \
    --output data/phase2_gold_standard.json

# 2. Extraire relations automatiquement (S15 J10)
python scripts/extract_relations_auto.py \
    --corpus data/phase2_test/ \
    --engine hybrid \
    --output data/phase2_predictions_S15.json

# 3. Ã‰valuer performance
python scripts/evaluate_relation_extraction.py \
    --gold_standard data/phase2_gold_standard.json \
    --predictions data/phase2_predictions_S15.json \
    --output reports/phase2_evaluation_S15.json
```

**Output Attendu:**

```json
{
  "precision": 0.82,
  "recall": 0.67,
  "f1_score": 0.74,
  "per_type_metrics": {
    "PART_OF": {"precision": 0.88, "recall": 0.72, "f1": 0.79},
    "SUBTYPE_OF": {"precision": 0.75, "recall": 0.60, "f1": 0.67},
    "REQUIRES": {"precision": 0.85, "recall": 0.70, "f1": 0.77},
    "USES": {"precision": 0.78, "recall": 0.64, "f1": 0.70},
    // ... autres types
  },
  "evaluation_date": "2025-10-25T10:00:00Z"
}
```

---

## ğŸ¯ Planning IntÃ©gration Semaine 14

### Jour 3 (Setup corpus)

1. **SÃ©lectionner 100 documents multi-domaines**
   - 40 docs Software (SAP, Oracle, Salesforce...)
   - 20 docs Pharma (Clinical trials, Drug protocols...)
   - 20 docs Retail (E-commerce, Supply chain...)
   - 20 docs Other (Manufacturing, Finance, Legal)

2. **Extraire candidates relations**
   ```bash
   python scripts/annotate_relations_gold_standard.py \
       --corpus data/phase2_test/ \
       --extract_candidates_only \
       --output data/phase2_candidates.json
   ```
   â†’ Output: ~500-1000 candidate relations

3. **Stratified sampling**
   ```bash
   python scripts/annotate_relations_gold_standard.py \
       --candidates data/phase2_candidates.json \
       --sample_stratified \
       --types PART_OF,SUBTYPE_OF,REQUIRES,USES,INTEGRATES_WITH,VERSION_OF,PRECEDES,REPLACES,DEPRECATES \
       --samples_per_type 50 \
       --output data/phase2_to_annotate.json
   ```
   â†’ Output: 450 relations Ã  annoter (50 Ã— 9 types)

### Jour 3-5 (Annotation manuelle - en parallÃ¨le dev)

**Annotateur 1:**
```bash
python scripts/annotate_relations_cli.py \
    --input data/phase2_to_annotate.json \
    --annotator john.doe@company.com \
    --output data/phase2_annotations_john.json
```

**Annotateur 2:**
```bash
python scripts/annotate_relations_cli.py \
    --input data/phase2_to_annotate.json \
    --annotator jane.smith@company.com \
    --output data/phase2_annotations_jane.json
```

**Temps estimÃ©:**
- 450 relations Ã— 30 secondes/relation = 225 minutes = **~4 heures par annotateur**
- Total : 8 heures annotation (parallÃ©lisable avec dev J4-J7)

### Jour 7 (Inter-annotator agreement)

```bash
python scripts/calculate_inter_annotator_agreement.py \
    --annotations_a data/phase2_annotations_john.json \
    --annotations_b data/phase2_annotations_jane.json \
    --output data/phase2_agreement_stats.json
```

**Si Kappa â‰¥ 0.75:**
```bash
python scripts/merge_annotations.py \
    --annotations_a data/phase2_annotations_john.json \
    --annotations_b data/phase2_annotations_jane.json \
    --output data/phase2_gold_standard.json
```

**Si Kappa < 0.75:**
```bash
python scripts/resolve_conflicts.py \
    --annotations_a data/phase2_annotations_john.json \
    --annotations_b data/phase2_annotations_jane.json \
    --third_annotator tom.brown@company.com \
    --output data/phase2_gold_standard.json
```

---

## ğŸ“Š KPIs Gold Standard

| MÃ©trique | Target | Critique |
|----------|--------|----------|
| **Total relations annotÃ©es** | 450 (50 Ã— 9 types) | âœ… OUI |
| **Cohen's Kappa** | â‰¥ 0.75 | âœ… OUI |
| **Avg confidence humaine** | â‰¥ 0.85 | âš ï¸ Nice-to-have |
| **Balance domaines** | 40/20/20/20 | âœ… OUI |
| **Balance types** | 50 Â± 5 par type | âœ… OUI |

---

## ğŸ”— Ressources

### Documentation Externe
- [Cohen's Kappa - Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)
- [Inter-rater Reliability - Wikipedia](https://en.wikipedia.org/wiki/Inter-rater_reliability)
- [Gold Standard Dataset Best Practices](https://aclanthology.org/L18-1239.pdf)

### Scripts Fournis (Ã  crÃ©er)
- `scripts/annotate_relations_gold_standard.py` - Script principal
- `scripts/annotate_relations_cli.py` - Interface CLI annotation
- `scripts/annotate_relations_streamlit.py` - Interface web Streamlit
- `scripts/calculate_inter_annotator_agreement.py` - Calcul Kappa
- `scripts/evaluate_relation_extraction.py` - Validation performance

---

**RÃ©sumÃ©:** Le Gold Standard est essentiel pour valider que ton RelationExtractionEngine atteint les KPIs (Precision â‰¥ 80%, Recall â‰¥ 65%). Sans ce dataset de rÃ©fÃ©rence, tu n'as aucune vÃ©ritÃ© terrain pour mesurer la performance rÃ©elle de ton systÃ¨me.
