# Phase 2 : Intelligence Relationnelle & Graph-Powered Features

**P√©riode** : Semaines 11-20 (Post Phase 1 Semantic Core)
**Statut** : üü° Planification
**Pr√©requis** : ‚úÖ Phase 1 Compl√®te (Cross-r√©f√©rence Neo4j ‚Üî Qdrant op√©rationnelle)

---

## üéØ Objectifs Phase 2

Phase 2 capitalise sur la **cross-r√©f√©rence bidirectionnelle** √©tablie en Phase 1 pour d√©livrer des fonctionnalit√©s intelligentes impossibles avec un RAG vectoriel seul.

**Pivot strat√©gique** : Transformer le Proto-KG en v√©ritable **moteur d'intelligence relationnelle** exploitant la synergie Graphe ‚Üî Embeddings.

### Diff√©renciation vs Copilot/Gemini

| Capacit√© | Microsoft Copilot | Google Gemini | **KnowWhere OSMOSE** |
|----------|-------------------|---------------|----------------------|
| RAG Vectoriel | ‚úÖ | ‚úÖ | ‚úÖ |
| Graph Knowledge | ‚ùå | ‚ö†Ô∏è (limit√©) | ‚úÖ **Native** |
| Cross-r√©f√©rence Chunks ‚Üî Concepts | ‚ùå | ‚ùå | ‚úÖ **Bidirectionnelle** |
| Graph-Guided RAG | ‚ùå | ‚ùå | ‚úÖ **C≈ìur** |
| Evolution Tracking | ‚ùå | ‚ùå | ‚úÖ **USP Killer** |
| Provenance Explicite | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚úÖ **Granulaire** |

---

## üöÄ Use Cases Critiques (Exploitant Cross-r√©f√©rence)

### Priorit√© 1 : Fondations (Semaines 11-13)

#### UC1.1 : Explanation & Provenance Automatique üîç

**Probl√®me** : L'utilisateur voit "SAP BTP Security" dans le graphe mais ne sait pas d'o√π √ßa vient, ni comment c'est justifi√©.

**Solution** :
```python
# API: /api/v1/concepts/{concept_id}/explain
def explain_concept(concept_id: str) -> ExplanationResponse:
    """
    Retourne provenance compl√®te d'un concept avec citations sources.

    Exploite: concept.chunk_ids (Neo4j ‚Üí Qdrant)
    """
    # 1. R√©cup√©rer concept Neo4j
    concept = neo4j_client.get_concept(concept_id)

    # 2. R√©cup√©rer chunks via chunk_ids
    chunks = qdrant_client.retrieve(
        collection_name="knowbase",
        ids=concept.chunk_ids
    )

    # 3. Grouper par document source
    by_document = group_chunks_by_document(chunks)

    return {
        "concept_name": concept.canonical_name,
        "definition": concept.unified_definition,
        "confidence": concept.quality_score,
        "mentions_count": len(chunks),
        "source_documents": [
            {
                "document_name": doc.name,
                "excerpts": [
                    {
                        "text": chunk.text,
                        "page": extract_page(chunk),
                        "relevance": chunk.score
                    }
                    for chunk in doc_chunks[:5]  # Top 5
                ],
                "total_mentions": len(doc_chunks)
            }
            for doc, doc_chunks in by_document.items()
        ],
        "related_concepts": get_neighbors(concept_id, depth=1)
    }
```

**Valeur Business** :
- ‚úÖ Transparence totale (audit trail)
- ‚úÖ Conformit√© r√©glementaire (ISO 27001, SOC2)
- ‚úÖ Confiance utilisateur (+40% adoption)

**M√©triques de Succ√®s** :
- 100% des concepts ont ‚â•1 chunk source
- Latence API < 200ms (p95)
- Taux de citation document√©e > 95%

---

#### UC1.2 : Graph-Guided RAG (Recherche Hybride) üöÄ

**Probl√®me** : Recherche vectorielle seule = pertinence limit√©e, pas de contexte conceptuel structur√©.

**Solution** :
```python
# API: /api/v1/search/graph-guided
def graph_guided_search(query: str) -> SearchResponse:
    """
    Recherche hybride : Graphe ‚Üí Expansion contexte ‚Üí Vector ranking

    Exploite: canonical_concept_ids (Qdrant ‚Üí Neo4j)
    """
    # √âtape 1: Identifier concepts cl√©s dans la requ√™te
    query_concepts = extract_concepts_from_query(query)
    # Ex: "s√©curiser SAP BTP" ‚Üí ["SAP BTP", "Security"]

    # √âtape 2: Expansion graphe (1-hop neighbors)
    expanded_concepts = []
    for concept_name in query_concepts:
        concept = neo4j_client.find_concept(concept_name)
        neighbors = neo4j_client.get_neighbors(
            concept.id,
            relations=["SECURES", "INTEGRATES_WITH", "DEPENDS_ON"]
        )
        expanded_concepts.extend([concept] + neighbors)

    # Ex: Expansion ajoute ["RBAC", "Cloud Connector", "Identity Authentication"]

    # √âtape 3: R√©cup√©rer chunks de TOUS les concepts
    candidate_chunks = []
    for concept in expanded_concepts:
        chunks = qdrant_client.retrieve(ids=concept.chunk_ids)
        candidate_chunks.extend(chunks)

    # Ex: 200+ chunks contextuellement pertinents (vs 10-20 en RAG na√Øf)

    # √âtape 4: Rerank vectoriel sur les candidats enrichis
    query_embedding = embed_query(query)
    final_results = rerank_by_cosine_similarity(
        query_embedding,
        candidate_chunks,
        top_k=10
    )

    return {
        "results": final_results,
        "reasoning_path": [c.name for c in expanded_concepts],
        "graph_expansion_gain": len(candidate_chunks) / len(initial_vector_results)
    }
```

**Valeur Business** :
- ‚úÖ Pr√©cision +40% vs RAG vectoriel seul
- ‚úÖ Contexte structur√© (pas de r√©ponses hors-sujet)
- ‚úÖ Raisonnement explicable (chemin dans le graphe)

**M√©triques de Succ√®s** :
- NDCG@10 > 0.85 (vs 0.60 baseline vectoriel)
- Taux de r√©ponses pertinentes > 90%
- Latence < 500ms (avec expansion graphe)

**Impl√©mentation** :
- Semaine 11 : API endpoint `/search/graph-guided`
- Semaine 12 : Optimisation expansion graphe (cache, batch)
- Semaine 13 : A/B test vs recherche vectorielle classique

---

### Priorit√© 2 : USP Diff√©renciateurs (Semaines 14-17)

#### UC2.1 : CRR Evolution Tracker üìä **(Cas d'Usage KILLER)**

**Probl√®me** : Suivre l'√©volution d'un concept SAP √† travers 10+ CRR sur 3 ans impossible avec outils actuels.

**Solution** :
```python
# API: /api/v1/concepts/{concept_id}/evolution
def track_concept_evolution(
    concept_id: str,
    time_range: TimeRange
) -> EvolutionAnalysis:
    """
    Analyse √©volution s√©mantique d'un concept dans le temps.

    Exploite:
    - concept.chunk_ids ‚Üí retrouver tous les contextes
    - chunk.document_id + metadata.date ‚Üí timeline
    """
    # 1. R√©cup√©rer tous les chunks du concept
    concept = neo4j_client.get_concept(concept_id)
    chunks = qdrant_client.retrieve(ids=concept.chunk_ids)

    # 2. Grouper par document et date
    timeline = []
    for chunk in chunks:
        doc = get_document_metadata(chunk.document_id)
        timeline.append({
            "date": doc.publication_date,
            "document": doc.name,
            "chunk_text": chunk.text,
            "embedding": chunk.vector
        })

    timeline.sort(key=lambda x: x["date"])

    # 3. Analyser √©volution s√©mantique (drift analysis)
    evolution_metrics = []
    for i in range(1, len(timeline)):
        prev = timeline[i-1]
        curr = timeline[i]

        # Calculer distance s√©mantique
        semantic_drift = cosine_distance(
            prev["embedding"],
            curr["embedding"]
        )

        # Extraire th√®mes √©mergents
        theme_shift = llm_analyze_theme_change(
            prev["chunk_text"],
            curr["chunk_text"]
        )

        evolution_metrics.append({
            "period": f"{prev['date']} ‚Üí {curr['date']}",
            "semantic_drift": semantic_drift,
            "theme_shift": theme_shift,
            "documents": [prev["document"], curr["document"]]
        })

    # 4. Identifier tendances macro
    trends = identify_trends(evolution_metrics)
    # Ex: "Migration focus" ‚Üí "AI Features focus" (2023 Q1 ‚Üí Q4)

    return {
        "concept": concept.canonical_name,
        "total_mentions": len(chunks),
        "time_span": f"{timeline[0]['date']} - {timeline[-1]['date']}",
        "evolution_timeline": evolution_metrics,
        "key_trends": trends,
        "visualization_data": generate_timeline_viz(timeline)
    }
```

**Valeur Business** :
- ‚úÖ **USP Unique** : Impossible avec Copilot/Gemini/ChatGPT
- ‚úÖ ROI Consultant : D√©tecter shifts strat√©giques SAP
- ‚úÖ Sales Enablement : Prouver expertise √©volution produits

**Use Case Concret** :
> "Analyser l'√©volution de 'SAP S/4HANA Cloud' dans 15 CRR (2022-2024)"
> - 2022 Q1 : Focus "Migration ECC ‚Üí S/4"
> - 2023 Q2 : Shift vers "Green Ledger & Sustainability"
> - 2024 Q1 : √âmergence "Joule AI-powered ERP"
> ‚Üí Insight strat√©gique pour positionnement commercial

**Impl√©mentation** :
- Semaine 14 : Backend timeline construction + drift calculation
- Semaine 15 : LLM theme extraction + trend detection
- Semaine 16 : Frontend visualization (timeline interactif)
- Semaine 17 : Multi-concept comparison (benchmark concepts)

---

#### UC2.2 : Quality Assurance & Validation ‚úÖ

**Probl√®me** : Le graphe dit "X INTEGRATES_WITH Y" mais les chunks sources disent le contraire ‚Üí hallucination.

**Solution** :
```python
# Background job: Relation Validation
def validate_graph_relations(batch_size: int = 100):
    """
    Valide coh√©rence relations Neo4j avec chunks sources Qdrant.
    """
    # 1. R√©cup√©rer relations √† valider
    relations = neo4j_client.get_relations(
        filters={"confidence": {"$lt": 0.8}}  # Basse confiance
    )

    for relation in relations[:batch_size]:
        # 2. R√©cup√©rer chunks des 2 concepts
        source_chunks = qdrant_client.retrieve(
            ids=relation.source_concept.chunk_ids
        )
        target_chunks = qdrant_client.retrieve(
            ids=relation.target_concept.chunk_ids
        )

        # 3. Trouver co-occurrences
        common_chunks = find_common_contexts(
            source_chunks,
            target_chunks
        )

        # 4. LLM validation
        if len(common_chunks) >= 3:  # Seuil minimum
            validation = llm_validate_relation(
                relation_type=relation.type,
                source=relation.source_concept.name,
                target=relation.target_concept.name,
                evidence_chunks=common_chunks
            )

            # 5. Mettre √† jour metadata Neo4j
            neo4j_client.update_relation(
                relation.id,
                confidence=validation.confidence,
                evidence_count=len(common_chunks),
                caveats=validation.caveats
            )
```

**Valeur Business** :
- ‚úÖ Graphe de confiance (d√©tection hallucinations)
- ‚úÖ Qualit√© garantie (audit automatique)
- ‚úÖ Am√©lioration continue (feedback loop)

**M√©triques de Succ√®s** :
- 95% relations ont confidence > 0.7
- D√©tection +80% des incoh√©rences graphe/texte
- Faux positifs < 5%

---

### Priorit√© 3 : Auto-Apprentissage (Semaines 18-20)

#### UC3.1 : Concept Enrichment Dynamique üí°

**Solution** :
```python
# Cron job quotidien
def enrich_concept_definitions():
    """
    Enrichit d√©finitions concepts avec contextes r√©els.
    """
    for concept in neo4j_client.get_all_concepts():
        # R√©cup√©rer tous les chunks
        chunks = qdrant_client.retrieve(ids=concept.chunk_ids)

        if len(chunks) < 10:
            continue  # Pas assez de donn√©es

        # Clustering th√©matique des chunks
        facets = cluster_chunks_by_theme(chunks)
        # Ex: "Change Management" ‚Üí 3 facettes:
        #   - Technical (deployment, testing)
        #   - Organizational (training, adoption)
        #   - Governance (approval workflows)

        # G√©n√©rer d√©finition enrichie
        enriched_def = llm_synthesize_definition(
            concept_name=concept.name,
            facets=facets,
            representative_chunks=select_representative_chunks(chunks)
        )

        # Mettre √† jour Neo4j
        neo4j_client.update_concept(
            concept.id,
            enriched_definition=enriched_def,
            facets=facets
        )
```

**Valeur Business** :
- ‚úÖ Ontologie vivante (d√©finitions bas√©es usage r√©el)
- ‚úÖ Auto-apprentissage (pas de maintenance manuelle)
- ‚úÖ Multi-facette (nuances contextuelles)

---

#### UC3.2 : Co-occurrence Mining & Relation Discovery üîó

**Solution** :
```python
# Background job hebdomadaire
def discover_implicit_relations():
    """
    D√©couvre relations implicites non captur√©es par extraction initiale.
    """
    # 1. Analyser co-occurrences dans chunks
    co_occurrences = analyze_concept_cooccurrences(
        min_frequency=10,  # Apparaissent ensemble ‚â•10 fois
        window_size=512    # Dans une fen√™tre de 512 tokens
    )

    for (concept_a, concept_b), frequency in co_occurrences.items():
        # 2. V√©rifier si relation existe d√©j√†
        existing_relation = neo4j_client.find_relation(
            concept_a.id,
            concept_b.id
        )

        if existing_relation:
            continue

        # 3. R√©cup√©rer chunks communs
        common_chunks = get_common_chunks(
            concept_a.chunk_ids,
            concept_b.chunk_ids
        )

        # 4. LLM extraction relation
        relation = llm_extract_relation(
            concept_a=concept_a.name,
            concept_b=concept_b.name,
            evidence_chunks=common_chunks
        )

        # 5. Cr√©er ProtoConcept (soumis √† Gatekeeper)
        if relation.confidence > 0.7:
            create_proto_relation(
                source=concept_a.id,
                target=concept_b.id,
                relation_type=relation.type,
                confidence=relation.confidence,
                evidence_chunks=common_chunks
            )
```

**Valeur Business** :
- ‚úÖ D√©couverte automatique (pas besoin extraction manuelle)
- ‚úÖ Ontologie auto-apprenante (s'enrichit dans le temps)
- ‚úÖ D√©tection patterns cach√©s

---

#### UC3.3 : InferenceEngine - D√©couverte de Connaissances Cach√©es üß† **(KILLER FEATURE)**

**Probl√®me** : Les documents contiennent des connaissances **implicites** non directement lisibles :
- Inf√©rences transitives (A‚ÜíB, B‚ÜíC implique A~C)
- Signaux faibles (mentions rares mais critiques)
- Corr√©lations cach√©es (patterns non √©vidents)
- Contradictions inter-documents
- Trous structurels (concepts li√©s mais non connect√©s)

**Diff√©renciation MASSIVE** : Aucun concurrent (Copilot, Gemini, ChatGPT) ne peut faire cela car ils n'ont pas de graphe de connaissances exploitable.

**Solution Architecture** (100% composants GRATUITS) :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     INFERENCE ENGINE                         ‚îÇ
‚îÇ                (Composants 100% Open Source)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Neo4j GDS      ‚îÇ  ‚îÇ    PyKEEN       ‚îÇ  ‚îÇ   LLM        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Community      ‚îÇ  ‚îÇ    (MIT)        ‚îÇ  ‚îÇ   Validator  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (GPLv3 Free)   ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ   (optionnel)‚îÇ ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ PageRank      ‚îÇ  ‚îÇ ‚Ä¢ TransE        ‚îÇ  ‚îÇ ‚Ä¢ Valide     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Louvain       ‚îÇ  ‚îÇ ‚Ä¢ RotatE        ‚îÇ  ‚îÇ   inf√©rences ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ WCC           ‚îÇ  ‚îÇ ‚Ä¢ ComplEx       ‚îÇ  ‚îÇ ‚Ä¢ G√©n√®re     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Betweenness   ‚îÇ  ‚îÇ ‚Ä¢ Link Predict. ‚îÇ  ‚îÇ   explications‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Similarity    ‚îÇ  ‚îÇ ‚Ä¢ Embedding KG  ‚îÇ  ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                    ‚îÇ                   ‚îÇ         ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                ‚ñº                             ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ                    ‚îÇ  Insight Ranker   ‚îÇ                     ‚îÇ
‚îÇ                    ‚îÇ  & Deduplicator   ‚îÇ                     ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                                ‚îÇ                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Hidden Insights  ‚îÇ
                    ‚îÇ  Dashboard        ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Licences et Co√ªts** :

| Composant | Licence | Co√ªt | Limitations |
|-----------|---------|------|-------------|
| **Neo4j Community** | GPLv3 | **GRATUIT** | Single instance |
| **Neo4j GDS Community** | GPLv3 | **GRATUIT** | 4 CPU cores max |
| **PyKEEN** | MIT | **GRATUIT** | Aucune |
| **NetworkX** (fallback) | BSD | **GRATUIT** | Aucune |

**Impl√©mentation** :

```python
# src/knowbase/semantic/inference_engine.py

from dataclasses import dataclass
from enum import Enum
from typing import Optional
import asyncio


class InsightType(Enum):
    """Types d'insights d√©couvrables."""
    TRANSITIVE_INFERENCE = "transitive"      # A‚ÜíB‚ÜíC implique A~C
    WEAK_SIGNAL = "weak_signal"              # Mention rare mais critique
    STRUCTURAL_HOLE = "structural_hole"      # Lien manquant √©vident
    CONTRADICTION = "contradiction"          # Conflit inter-documents
    HIDDEN_CLUSTER = "hidden_cluster"        # Communaut√© non √©vidente
    BRIDGE_CONCEPT = "bridge_concept"        # Concept connecteur cl√©


@dataclass
class DiscoveredInsight:
    """Insight d√©couvert par l'InferenceEngine."""
    insight_type: InsightType
    title: str
    description: str
    confidence: float  # 0.0 - 1.0
    evidence: list[str]  # Chunks/documents sources
    affected_concepts: list[str]
    business_impact: Optional[str] = None


class InferenceEngine:
    """
    Moteur de d√©couverte de connaissances cach√©es.

    Utilise UNIQUEMENT des composants gratuits :
    - Neo4j GDS Community (graph algorithms)
    - PyKEEN (knowledge graph embeddings)
    - LLM optionnel (validation/explication)
    """

    def __init__(
        self,
        neo4j_client,
        qdrant_client,
        llm_client=None  # Optionnel pour validation
    ):
        self.neo4j = neo4j_client
        self.qdrant = qdrant_client
        self.llm = llm_client

    async def discover_insights(
        self,
        scope: Optional[str] = None,  # Filtre domaine
        methods: list[str] = None     # M√©thodes √† utiliser
    ) -> list[DiscoveredInsight]:
        """
        Lance la d√©couverte d'insights sur le graphe.

        Args:
            scope: Filtrer par domaine (ex: "SAP BTP")
            methods: Liste de m√©thodes ["transitive", "weak_signal", ...]
                     Si None, utilise toutes les m√©thodes
        """
        methods = methods or [
            "transitive", "weak_signal", "structural_hole",
            "contradiction", "community"
        ]

        insights = []

        # Ex√©cuter m√©thodes en parall√®le
        tasks = []
        if "transitive" in methods:
            tasks.append(self._find_transitive_inferences(scope))
        if "weak_signal" in methods:
            tasks.append(self._detect_weak_signals(scope))
        if "structural_hole" in methods:
            tasks.append(self._find_structural_holes(scope))
        if "contradiction" in methods:
            tasks.append(self._detect_contradictions(scope))
        if "community" in methods:
            tasks.append(self._discover_hidden_communities(scope))

        results = await asyncio.gather(*tasks)
        for result in results:
            insights.extend(result)

        # D√©dupliquer et ranker
        insights = self._rank_and_deduplicate(insights)

        return insights

    # ============================================================
    # M√âTHODE 1: Inf√©rences Transitives (Cypher natif - GRATUIT)
    # ============================================================
    async def _find_transitive_inferences(
        self,
        scope: Optional[str] = None
    ) -> list[DiscoveredInsight]:
        """
        Trouve les chemins transitifs A‚ÜíB‚ÜíC o√π A et C ne sont pas
        directement li√©s mais devraient l'√™tre.

        Utilise: Cypher natif (aucun plugin requis)
        """
        query = """
        // Trouver concepts connect√©s via interm√©diaire mais pas directement
        MATCH path = (a:Concept)-[r1]->(b:Concept)-[r2]->(c:Concept)
        WHERE a <> c
        AND NOT (a)-[]-(c)  // Pas de lien direct
        AND a.quality_score > 0.6
        AND c.quality_score > 0.6

        // Calculer force inf√©rence
        WITH a, b, c, r1, r2,
             (a.quality_score + c.quality_score) / 2 AS confidence
        WHERE confidence > 0.7

        RETURN
            a.canonical_name AS source,
            type(r1) AS rel1,
            b.canonical_name AS bridge,
            type(r2) AS rel2,
            c.canonical_name AS target,
            confidence,
            a.chunk_ids AS source_chunks,
            c.chunk_ids AS target_chunks
        ORDER BY confidence DESC
        LIMIT 50
        """

        results = await self.neo4j.execute_query(query)
        insights = []

        for row in results:
            insight = DiscoveredInsight(
                insight_type=InsightType.TRANSITIVE_INFERENCE,
                title=f"Lien implicite: {row['source']} ‚Üî {row['target']}",
                description=(
                    f"'{row['source']}' est li√© √† '{row['target']}' "
                    f"via '{row['bridge']}' "
                    f"({row['rel1']} ‚Üí {row['rel2']}), "
                    f"mais aucun lien direct n'existe."
                ),
                confidence=row['confidence'],
                evidence=row['source_chunks'][:3] + row['target_chunks'][:3],
                affected_concepts=[row['source'], row['bridge'], row['target']],
                business_impact=self._assess_transitive_impact(row)
            )
            insights.append(insight)

        return insights

    # ============================================================
    # M√âTHODE 2: Signaux Faibles (Neo4j GDS Community - GRATUIT)
    # ============================================================
    async def _detect_weak_signals(
        self,
        scope: Optional[str] = None
    ) -> list[DiscoveredInsight]:
        """
        D√©tecte les concepts rarement mentionn√©s mais √† haute valeur.

        Utilise: PageRank + analyse mentions (Neo4j GDS Community)
        """
        # √âtape 1: Calculer PageRank pour importance structurelle
        pagerank_query = """
        CALL gds.pageRank.stream('concept-graph', {
            maxIterations: 20,
            dampingFactor: 0.85
        })
        YIELD nodeId, score
        WITH gds.util.asNode(nodeId) AS concept, score AS pagerank

        // Trouver concepts √† haut PageRank mais peu de chunks
        WHERE size(concept.chunk_ids) < 5  // Rarement mentionn√©
        AND pagerank > 0.1                  // Mais structurellement important

        RETURN
            concept.canonical_name AS name,
            concept.unified_definition AS definition,
            size(concept.chunk_ids) AS mention_count,
            pagerank,
            concept.chunk_ids AS chunks
        ORDER BY pagerank DESC
        LIMIT 20
        """

        results = await self.neo4j.execute_query(pagerank_query)
        insights = []

        for row in results:
            insight = DiscoveredInsight(
                insight_type=InsightType.WEAK_SIGNAL,
                title=f"Signal faible: {row['name']}",
                description=(
                    f"'{row['name']}' n'est mentionn√© que {row['mention_count']} fois "
                    f"mais poss√®de une importance structurelle √©lev√©e "
                    f"(PageRank: {row['pagerank']:.3f}). "
                    f"Ce concept m√©rite une attention particuli√®re."
                ),
                confidence=min(row['pagerank'] * 2, 0.95),
                evidence=row['chunks'],
                affected_concepts=[row['name']],
                business_impact="Concept potentiellement sous-estim√©"
            )
            insights.append(insight)

        return insights

    # ============================================================
    # M√âTHODE 3: Trous Structurels (Neo4j GDS Community - GRATUIT)
    # ============================================================
    async def _find_structural_holes(
        self,
        scope: Optional[str] = None
    ) -> list[DiscoveredInsight]:
        """
        Identifie les paires de concepts qui devraient √™tre li√©es
        (voisins communs, similarit√© s√©mantique) mais ne le sont pas.

        Utilise: Node Similarity (Neo4j GDS Community)
        """
        # Projeter graphe pour GDS
        project_query = """
        CALL gds.graph.project(
            'similarity-graph',
            'Concept',
            {
                RELATES_TO: {orientation: 'UNDIRECTED'},
                INTEGRATES_WITH: {orientation: 'UNDIRECTED'},
                DEPENDS_ON: {orientation: 'UNDIRECTED'}
            }
        )
        """

        # Calculer similarit√© nodale (voisins communs)
        similarity_query = """
        CALL gds.nodeSimilarity.stream('similarity-graph', {
            topK: 10,
            similarityCutoff: 0.5
        })
        YIELD node1, node2, similarity
        WITH gds.util.asNode(node1) AS c1,
             gds.util.asNode(node2) AS c2,
             similarity

        // Filtrer paires sans lien direct
        WHERE NOT (c1)-[]-(c2)
        AND similarity > 0.6

        RETURN
            c1.canonical_name AS concept1,
            c2.canonical_name AS concept2,
            similarity,
            c1.chunk_ids AS chunks1,
            c2.chunk_ids AS chunks2
        ORDER BY similarity DESC
        LIMIT 30
        """

        try:
            await self.neo4j.execute_query(project_query)
            results = await self.neo4j.execute_query(similarity_query)
        finally:
            # Nettoyer projection
            await self.neo4j.execute_query(
                "CALL gds.graph.drop('similarity-graph', false)"
            )

        insights = []
        for row in results:
            insight = DiscoveredInsight(
                insight_type=InsightType.STRUCTURAL_HOLE,
                title=f"Lien manquant: {row['concept1']} ‚Üî {row['concept2']}",
                description=(
                    f"'{row['concept1']}' et '{row['concept2']}' partagent "
                    f"de nombreux voisins communs (similarit√©: {row['similarity']:.2f}) "
                    f"mais n'ont aucun lien direct. "
                    f"Un lien devrait probablement exister."
                ),
                confidence=row['similarity'],
                evidence=row['chunks1'][:2] + row['chunks2'][:2],
                affected_concepts=[row['concept1'], row['concept2']],
                business_impact="Relation potentiellement non document√©e"
            )
            insights.append(insight)

        return insights

    # ============================================================
    # M√âTHODE 4: D√©tection Contradictions (LLM + Qdrant)
    # ============================================================
    async def _detect_contradictions(
        self,
        scope: Optional[str] = None
    ) -> list[DiscoveredInsight]:
        """
        Trouve les affirmations contradictoires entre documents.

        Utilise: Qdrant similarity + LLM validation
        """
        if not self.llm:
            return []  # LLM requis pour cette m√©thode

        # R√©cup√©rer concepts avec plusieurs sources
        query = """
        MATCH (c:Concept)
        WHERE size(c.chunk_ids) >= 3
        RETURN c.canonical_name AS name, c.chunk_ids AS chunks
        LIMIT 100
        """

        concepts = await self.neo4j.execute_query(query)
        insights = []

        for concept in concepts:
            # R√©cup√©rer chunks du concept
            chunks = await self.qdrant.retrieve(
                collection_name="knowbase",
                ids=concept['chunks']
            )

            if len(chunks) < 2:
                continue

            # Comparer paires de chunks pour contradictions
            for i, chunk_a in enumerate(chunks[:-1]):
                for chunk_b in chunks[i+1:]:
                    # Skip si m√™me document
                    if (chunk_a.payload.get('document_id') ==
                        chunk_b.payload.get('document_id')):
                        continue

                    # LLM v√©rifie contradiction
                    contradiction = await self._check_contradiction(
                        concept['name'],
                        chunk_a.payload['text'],
                        chunk_b.payload['text']
                    )

                    if contradiction and contradiction['is_contradiction']:
                        insight = DiscoveredInsight(
                            insight_type=InsightType.CONTRADICTION,
                            title=f"Contradiction: {concept['name']}",
                            description=contradiction['explanation'],
                            confidence=contradiction['confidence'],
                            evidence=[chunk_a.id, chunk_b.id],
                            affected_concepts=[concept['name']],
                            business_impact="Information incoh√©rente √† r√©soudre"
                        )
                        insights.append(insight)
                        break  # Une contradiction suffit par concept

        return insights

    # ============================================================
    # M√âTHODE 5: Communaut√©s Cach√©es (Louvain - GDS Community)
    # ============================================================
    async def _discover_hidden_communities(
        self,
        scope: Optional[str] = None
    ) -> list[DiscoveredInsight]:
        """
        D√©couvre des clusters de concepts non √©vidents.

        Utilise: Louvain Community Detection (Neo4j GDS Community)
        """
        # Projeter et d√©tecter communaut√©s
        community_query = """
        CALL gds.louvain.stream('concept-graph', {
            maxLevels: 10,
            maxIterations: 10
        })
        YIELD nodeId, communityId
        WITH communityId, collect(gds.util.asNode(nodeId)) AS members
        WHERE size(members) >= 3 AND size(members) <= 15

        // R√©cup√©rer infos communaut√©
        RETURN
            communityId,
            [m IN members | m.canonical_name] AS concept_names,
            size(members) AS size,
            reduce(s = 0.0, m IN members | s + m.quality_score) / size(members) AS avg_quality
        ORDER BY size DESC
        LIMIT 20
        """

        results = await self.neo4j.execute_query(community_query)
        insights = []

        for row in results:
            # V√©rifier si communaut√© est "surprenante" (concepts de domaines diff√©rents)
            is_surprising = await self._is_surprising_cluster(row['concept_names'])

            if is_surprising:
                insight = DiscoveredInsight(
                    insight_type=InsightType.HIDDEN_CLUSTER,
                    title=f"Cluster cach√©: {', '.join(row['concept_names'][:3])}...",
                    description=(
                        f"Un groupe de {row['size']} concepts forme une communaut√© "
                        f"non √©vidente: {', '.join(row['concept_names'])}. "
                        f"Ces concepts sont fortement interconnect√©s dans la documentation."
                    ),
                    confidence=row['avg_quality'],
                    evidence=[],  # Pas de chunks sp√©cifiques
                    affected_concepts=row['concept_names'],
                    business_impact="Synergie potentielle √† explorer"
                )
                insights.append(insight)

        return insights

    # ============================================================
    # M√©thodes utilitaires
    # ============================================================
    def _rank_and_deduplicate(
        self,
        insights: list[DiscoveredInsight]
    ) -> list[DiscoveredInsight]:
        """Trie par confiance et d√©duplique insights similaires."""
        # Trier par confiance
        insights.sort(key=lambda x: x.confidence, reverse=True)

        # D√©dupliquer (concepts similaires)
        seen_concepts = set()
        unique_insights = []

        for insight in insights:
            key = frozenset(insight.affected_concepts)
            if key not in seen_concepts:
                seen_concepts.add(key)
                unique_insights.append(insight)

        return unique_insights[:50]  # Top 50

    async def _check_contradiction(
        self,
        concept_name: str,
        text_a: str,
        text_b: str
    ) -> Optional[dict]:
        """Utilise LLM pour v√©rifier contradiction."""
        prompt = f"""
        Analyse ces deux extraits concernant "{concept_name}".

        Extrait A: {text_a[:500]}
        Extrait B: {text_b[:500]}

        Ces extraits contiennent-ils une CONTRADICTION factuelle ?
        R√©ponds en JSON: {{"is_contradiction": bool, "confidence": float, "explanation": str}}
        """

        response = await self.llm.complete(prompt)
        # Parser JSON response...
        return response

    def _assess_transitive_impact(self, row: dict) -> str:
        """√âvalue impact business d'une inf√©rence transitive."""
        rel_types = {row['rel1'], row['rel2']}

        if 'DEPENDS_ON' in rel_types:
            return "D√©pendance indirecte potentielle"
        elif 'SECURES' in rel_types:
            return "Implication s√©curit√© √† v√©rifier"
        elif 'INTEGRATES_WITH' in rel_types:
            return "Int√©gration possible non document√©e"
        else:
            return "Relation √† investiguer"

    async def _is_surprising_cluster(self, concepts: list[str]) -> bool:
        """V√©rifie si cluster est surprenant (cross-domaine)."""
        # Logique simplifi√©e : surprenant si > 1 domaine
        domains = set()
        domain_keywords = {
            'security': ['security', 'auth', 'sso', 'rbac'],
            'integration': ['api', 'integration', 'connector'],
            'analytics': ['analytics', 'report', 'dashboard'],
            'cloud': ['cloud', 'btp', 'azure', 'aws']
        }

        for concept in concepts:
            concept_lower = concept.lower()
            for domain, keywords in domain_keywords.items():
                if any(kw in concept_lower for kw in keywords):
                    domains.add(domain)

        return len(domains) >= 2  # Cross-domaine
```

**API Endpoints** :

```python
# src/knowbase/api/routers/inference.py

from fastapi import APIRouter, Query
from typing import Optional

router = APIRouter(prefix="/api/v1/inference", tags=["inference"])

@router.get("/discover")
async def discover_insights(
    scope: Optional[str] = Query(None, description="Filtrer par domaine"),
    methods: Optional[str] = Query(
        None,
        description="M√©thodes (comma-sep): transitive,weak_signal,structural_hole,contradiction,community"
    ),
    limit: int = Query(20, le=50)
):
    """
    D√©couvre des connaissances cach√©es dans le graphe.

    Returns:
        Liste d'insights avec type, description, confiance et preuves.
    """
    method_list = methods.split(',') if methods else None

    engine = InferenceEngine(
        neo4j_client=get_neo4j(),
        qdrant_client=get_qdrant(),
        llm_client=get_llm()  # Optionnel
    )

    insights = await engine.discover_insights(
        scope=scope,
        methods=method_list
    )

    return {
        "total": len(insights),
        "insights": [
            {
                "type": i.insight_type.value,
                "title": i.title,
                "description": i.description,
                "confidence": i.confidence,
                "affected_concepts": i.affected_concepts,
                "business_impact": i.business_impact,
                "evidence_count": len(i.evidence)
            }
            for i in insights[:limit]
        ]
    }

@router.get("/insights/{insight_type}")
async def get_insights_by_type(
    insight_type: str,
    limit: int = Query(10, le=30)
):
    """R√©cup√®re insights d'un type sp√©cifique."""
    # Implementation...
    pass
```

**Valeur Business** :
- ‚úÖ **USP KILLER** : Aucun concurrent ne peut d√©couvrir des connaissances cach√©es
- ‚úÖ **Due Diligence** : D√©tecter risques/contradictions avant d√©cision
- ‚úÖ **Innovation** : Identifier opportunit√©s cross-domaine non √©videntes
- ‚úÖ **Audit** : Rep√©rer incoh√©rences documentaires automatiquement

**Use Cases Concrets** :

| Insight Type | Exemple R√©el | Impact Business |
|--------------|--------------|-----------------|
| **Transitive** | "SAP BTP ‚Üí Cloud Connector ‚Üí S/4HANA" implique BTP‚ÜîS/4 | D√©pendance critique non document√©e |
| **Weak Signal** | "Green Ledger" mentionn√© 2x mais PageRank √©lev√© | Trend √©mergent √† surveiller |
| **Structural Hole** | "RBAC" et "SSO" jamais li√©s mais voisins communs | Int√©gration s√©curit√© √† documenter |
| **Contradiction** | Doc A: "BTP supporte X", Doc B: "X n'est pas support√©" | Incoh√©rence √† r√©soudre |
| **Hidden Cluster** | {Analytics, ML, Joule, BTP} forment communaut√© | Convergence IA SAP |

**M√©triques de Succ√®s** :
- 50+ insights pertinents par run
- Precision des insights > 70% (validation humaine)
- Temps d'ex√©cution < 30s (graphe 10K concepts)
- Adoption : 80% users trouvent ‚â•1 insight actionable

**Impl√©mentation** :
- Semaine 18 : InferenceEngine core (transitive, weak_signal)
- Semaine 19 : M√©thodes avanc√©es (structural_hole, community)
- Semaine 20 : API + Dashboard insights + Validation LLM

---

## üèóÔ∏è Architecture Technique

### Nouveaux Composants Phase 2

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      API Layer (FastAPI)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  /concepts/{id}/explain          ‚îÇ UC1.1 Provenance           ‚îÇ
‚îÇ  /search/graph-guided            ‚îÇ UC1.2 Hybrid Search        ‚îÇ
‚îÇ  /concepts/{id}/evolution        ‚îÇ UC2.1 Evolution            ‚îÇ
‚îÇ  /relations/{id}/validate        ‚îÇ UC2.2 Validation           ‚îÇ
‚îÇ  /inference/discover             ‚îÇ UC3.3 Hidden Knowledge üÜï  ‚îÇ
‚îÇ  /inference/insights/{type}      ‚îÇ UC3.3 Insights by Type üÜï  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Graph-Powered Services Layer                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ GraphGuidedSearchService    (UC1.2)                        ‚îÇ
‚îÇ  ‚Ä¢ ConceptExplainerService     (UC1.1)                        ‚îÇ
‚îÇ  ‚Ä¢ EvolutionAnalyzerService    (UC2.1)                        ‚îÇ
‚îÇ  ‚Ä¢ RelationValidatorService    (UC2.2)                        ‚îÇ
‚îÇ  ‚Ä¢ CooccurrenceMinerService    (UC3.2)                        ‚îÇ
‚îÇ  ‚Ä¢ InferenceEngine             (UC3.3) üÜï KILLER FEATURE      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚ñº                 ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Neo4j Graph   ‚îÇ  ‚îÇ  Neo4j GDS   ‚îÇ  ‚îÇ  Qdrant Vector   ‚îÇ
‚îÇ   Community     ‚îÇ  ‚îÇ  Community   ‚îÇ  ‚îÇ                  ‚îÇ
‚îÇ                 ‚îÇ  ‚îÇ  (GRATUIT)   ‚îÇ  ‚îÇ                  ‚îÇ
‚îÇ ‚Ä¢ chunk_ids []  ‚îÇ  ‚îÇ ‚Ä¢ PageRank   ‚îÇ  ‚îÇ ‚Ä¢ canonical_ids  ‚îÇ
‚îÇ ‚Ä¢ Cypher natif  ‚îÇ  ‚îÇ ‚Ä¢ Louvain    ‚îÇ  ‚îÇ ‚Ä¢ embeddings     ‚îÇ
‚îÇ                 ‚îÇ  ‚îÇ ‚Ä¢ Similarity ‚îÇ  ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Cross-Ref ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      Bidirectionnelle
```

### Stack Technique InferenceEngine (100% GRATUIT)

| Composant | R√¥le | Licence | Co√ªt |
|-----------|------|---------|------|
| **Neo4j Community** | Base graphe | GPLv3 | GRATUIT |
| **Neo4j GDS Community** | Algorithmes graphe (PageRank, Louvain, Similarity) | GPLv3 | GRATUIT |
| **PyKEEN** | Embeddings KG (TransE, RotatE) | MIT | GRATUIT |
| **NetworkX** | Fallback Python natif | BSD | GRATUIT |
| **LLM** (optionnel) | Validation insights | - | Usage existant |

### Services √† D√©velopper

#### 1. `GraphGuidedSearchService` (Semaines 11-13)
```python
class GraphGuidedSearchService:
    """
    Recherche hybride exploitant expansion graphe.
    """
    def __init__(
        self,
        neo4j_client: Neo4jClient,
        qdrant_client: QdrantClient,
        embedder: SentenceTransformer
    ):
        self.neo4j = neo4j_client
        self.qdrant = qdrant_client
        self.embedder = embedder

    async def search(
        self,
        query: str,
        expansion_depth: int = 1,
        top_k: int = 10
    ) -> SearchResponse:
        # 1. Extract concepts from query
        concepts = await self._extract_query_concepts(query)

        # 2. Expand via graph (1-hop)
        expanded = await self._expand_concepts_graph(
            concepts,
            depth=expansion_depth
        )

        # 3. Retrieve all chunks from expanded concepts
        candidate_chunks = await self._get_chunks_from_concepts(
            expanded
        )

        # 4. Vector rerank
        query_embedding = self.embedder.encode(query)
        ranked = self._rerank_chunks(
            query_embedding,
            candidate_chunks,
            top_k=top_k
        )

        return SearchResponse(
            results=ranked,
            reasoning_path=[c.name for c in expanded],
            expansion_gain=len(candidate_chunks)
        )
```

#### 2. `EvolutionAnalyzerService` (Semaines 14-17)
```python
class EvolutionAnalyzerService:
    """
    Analyse √©volution s√©mantique concepts dans le temps.
    """
    async def analyze_evolution(
        self,
        concept_id: str,
        time_range: Optional[TimeRange] = None
    ) -> EvolutionAnalysis:
        # 1. Get all chunks with timestamps
        chunks_timeline = await self._build_chunks_timeline(
            concept_id,
            time_range
        )

        # 2. Calculate semantic drift
        drift_metrics = await self._calculate_semantic_drift(
            chunks_timeline
        )

        # 3. Extract theme shifts
        theme_evolution = await self._analyze_theme_shifts(
            chunks_timeline
        )

        # 4. Identify macro trends
        trends = self._identify_trends(
            drift_metrics,
            theme_evolution
        )

        return EvolutionAnalysis(
            timeline=chunks_timeline,
            drift_metrics=drift_metrics,
            theme_evolution=theme_evolution,
            trends=trends
        )
```

---

## üìä M√©triques de Succ√®s Phase 2

### KPIs Techniques

| M√©trique | Target | Mesure |
|----------|--------|--------|
| **Graph-Guided Search Precision** | NDCG@10 > 0.85 | A/B test vs baseline |
| **Provenance Coverage** | 100% concepts | % concepts avec ‚â•1 chunk |
| **Evolution Tracking Latency** | < 2s (p95) | API response time |
| **Relation Validation Accuracy** | > 90% | Precision/Recall validation |
| **Cross-ref Integrity** | 99.9% | Audit Neo4j.chunk_ids ‚Üî Qdrant.canonical_ids |

### KPIs Business

| M√©trique | Target | Impact |
|----------|--------|--------|
| **User Trust Score** | > 4.5/5 | Survey "Je fais confiance aux r√©sultats" |
| **Adoption Rate** | +40% vs Phase 1 | DAU (Daily Active Users) |
| **CRR Evolution Demos** | 10 clients | Sales enablement showcase |
| **Query Success Rate** | > 90% | % requ√™tes avec r√©ponse pertinente |

---

## üóìÔ∏è Roadmap D√©taill√©e Phase 2

### Semaines 11-13 : Fondations (Priorit√© 1)

**Objectif** : D√©livrer valeur imm√©diate avec cross-r√©f√©rence

**Livrables** :
- ‚úÖ API `/concepts/{id}/explain` (UC1.1)
- ‚úÖ API `/search/graph-guided` (UC1.2)
- ‚úÖ Tests A/B vs recherche vectorielle classique
- ‚úÖ Documentation API + exemples

**Crit√®res de Succ√®s** :
- Graph-Guided Search : NDCG@10 > 0.80
- Provenance Coverage : 95% concepts
- Latence API < 300ms (p95)

---

### Semaines 14-17 : USP Diff√©renciateurs (Priorit√© 2)

**Objectif** : CRR Evolution Tracker (d√©mo commerciale)

**Livrables** :
- ‚úÖ Backend Evolution Analysis (UC2.1)
- ‚úÖ Frontend Timeline Visualization
- ‚úÖ Multi-concept Comparison
- ‚úÖ Relation Validation automatique (UC2.2)
- ‚úÖ D√©mo client pr√™te (use case SAP S/4HANA)

**Crit√®res de Succ√®s** :
- 10 demos CRR Evolution aupr√®s clients
- Relation validation accuracy > 85%
- User feedback > 4/5 sur timeline viz

---

### Semaines 18-20 : Auto-Apprentissage & D√©couverte (Priorit√© 3)

**Objectif** : Ontologie auto-apprenante + **InferenceEngine (KILLER FEATURE)**

**Livrables** :
- ‚úÖ Concept Enrichment quotidien (UC3.1)
- ‚úÖ Co-occurrence Mining hebdomadaire (UC3.2)
- ‚úÖ **InferenceEngine core** (UC3.3) üÜï
  - Inf√©rences transitives (Cypher natif)
  - Signaux faibles (PageRank - Neo4j GDS Community)
  - Trous structurels (Node Similarity - Neo4j GDS Community)
- ‚úÖ **API `/inference/discover`** üÜï
- ‚úÖ **Dashboard Hidden Insights** üÜï
- ‚úÖ Dashboard admin (monitoring auto-learning)
- ‚úÖ Documentation patterns d√©couverts

**Crit√®res de Succ√®s** :
- 50+ relations d√©couvertes automatiquement
- Concept enrichment : 80% concepts ont facets
- Zero-intervention uptime : 7 jours
- **InferenceEngine** üÜï :
  - 50+ insights pertinents par run
  - Pr√©cision insights > 70% (validation humaine)
  - Temps d'ex√©cution < 30s (graphe 10K concepts)
  - 80% users trouvent ‚â•1 insight actionable

---

## üß™ Proof of Concept (POC) Recommand√©

**Avant d√©marrage Phase 2**, valider l'approche avec mini-POC :

### POC : "Explain this Concept" (2-3 jours)

**Objectif** : Prouver valeur cross-r√©f√©rence Neo4j ‚Üî Qdrant

**Scope** :
```python
# Script POC simple
def poc_explain_concept(concept_name: str):
    # 1. Find concept in Neo4j
    concept = neo4j_client.find_concept_by_name(concept_name)

    # 2. Retrieve chunks via chunk_ids
    chunks = qdrant_client.retrieve(
        collection_name="knowbase",
        ids=concept.chunk_ids
    )

    # 3. Display provenance
    print(f"Concept: {concept.canonical_name}")
    print(f"Definition: {concept.unified_definition}")
    print(f"\nMentions ({len(chunks)} total):\n")

    for i, chunk in enumerate(chunks[:5], 1):
        doc_name = chunk.payload.get("document_name", "Unknown")
        print(f"{i}. [{doc_name}]")
        print(f"   {chunk.payload['text'][:200]}...")
        print()
```

**Crit√®res Validation POC** :
- ‚úÖ 100% concepts test√©s ont ‚â•1 chunk
- ‚úÖ Temps execution < 500ms
- ‚úÖ Feedback positif (3+ stakeholders)

**Si POC r√©ussit** ‚Üí Green light Phase 2 compl√®te

---

## üéì Conclusion Phase 2

Phase 2 transforme KnowWhere d'un **simple RAG vectoriel** en un v√©ritable **moteur d'intelligence relationnelle**.

**Diff√©renciation strat√©gique** :
- ‚úÖ **CRR Evolution Tracker** : USP impossible √† copier
- ‚úÖ **Graph-Guided RAG** : Pr√©cision +40% vs concurrents
- ‚úÖ **Provenance Explicite** : Confiance & conformit√©

**Pr√™t pour Phase 3** : Production KG avec qualit√© garantie et auto-apprentissage valid√©.

---

**Prochaine √©tape** : Validation POC "Explain this Concept" (2-3 jours) avant d√©marrage complet Phase 2.
