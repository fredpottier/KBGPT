# Rapport d'Analyse - Refactoring pptx_pipeline.py

**Date:** 2025-11-17
**Commit refactoring:** `269be4c` - "refactor(ingestion): Modulariser pptx_pipeline.py en composants rÃ©utilisables"

## RÃ©sumÃ© ExÃ©cutif

Analyse comparative systÃ©matique entre l'ancien monolithe `pptx_pipeline.py` et la nouvelle architecture modulaire.

**RÃ©sultat:** 3 bugs critiques dÃ©tectÃ©s et corrigÃ©s.

---

## ğŸ”´ BUGS CRITIQUES DÃ‰TECTÃ‰S

### 1. âŒ Extraction MegaParse - Division par regex au lieu de lignes

**Fichier:** `src/knowbase/ingestion/components/extractors/binary_parser.py`
**Fonction:** `extract_slides_from_megaparse()` et `split_megaparse_by_slide_count()`

**ProblÃ¨me:**
- **Ancien code (fonctionnel):** Division proportionnelle du contenu MegaParse en N parties Ã©gales par **lignes**
- **Nouveau code (cassÃ©):** Recherche de patterns regex (`---`, `Slide X`, `Page X`) qui n'existent PAS dans l'output MegaParse
- **Impact:** Extraction de seulement 1 slide au lieu de 94, perte de 99% du contenu

**Correction appliquÃ©e:**
```python
# AVANT (cassÃ©)
slide_pattern = r"(?:^|\n)(?:---+|Slide\s+\d+|Page\s+\d+)"
parts = re.split(slide_pattern, content)  # Ne trouve rien !

# APRÃˆS (restaurÃ©)
content_lines = content.split("\n")
lines_per_slide = len(content_lines) // slide_count
for slide_num in range(1, slide_count + 1):
    start_line = (slide_num - 1) * lines_per_slide
    end_line = slide_num * lines_per_slide if slide_num < slide_count else len(content_lines)
    slide_content = "\n".join(content_lines[start_line:end_line])
```

**Status:** âœ… CORRIGÃ‰ (restauration logique originale)

---

### 2. âŒ clean_gpt_response - Suppression logique rÃ©paration JSON tronquÃ©

**Fichier:** `src/knowbase/ingestion/components/utils/text_utils.py`
**Fonction:** `clean_gpt_response()`

**ProblÃ¨me:**
- **Ancien code (robuste):** 60 lignes avec rÃ©paration automatique de JSON tronquÃ© (timeout LLM, rÃ©ponse incomplÃ¨te)
- **Nouveau code (fragile):** Extraction simple regex sans rÃ©paration
- **Impact:** Ã‰chec parsing JSON lors de timeouts LLM â†’ perte de concepts extraits

**Logique supprimÃ©e:**
- DÃ©tection JSON tronquÃ© au milieu d'une string (`s.endswith('"')`)
- DÃ©tection JSON tronquÃ© aprÃ¨s virgule (`s.endswith(',')`)
- Fermeture automatique des brackets manquants (`]`, `}`)
- Retry avec validation JSON aprÃ¨s rÃ©paration

**Correction appliquÃ©e:**
```python
# Restauration COMPLÃˆTE de la logique originale
# - Validation JSON (json.loads())
# - RÃ©paration automatique selon patterns dÃ©tectÃ©s
# - Fallback vers "[]" si irrÃ©parable
# - Logging dÃ©taillÃ© des tentatives
```

**Status:** âœ… CORRIGÃ‰ (restauration logique complÃ¨te + ajout paramÃ¨tre logger optionnel)

---

### 3. âŒ recursive_chunk - DÃ©coupage caractÃ¨res au lieu de tokens

**Fichier:** `src/knowbase/ingestion/components/utils/text_utils.py`
**Fonction:** `recursive_chunk()`

**ProblÃ¨me:**
- **Ancien code (correct):** DÃ©coupage par **TOKENS** (mots) - respecte les limites LLM
- **Nouveau code (incorrect):** DÃ©coupage par **CARACTÃˆRES** - ne respecte plus max_tokens
- **Impact:** Possibles dÃ©passements de tokens LLM, chunking incorrect des concepts longs

**Exemple d'impact:**
```python
text = "mot " * 1000  # 1000 tokens

# ANCIEN (correct)
chunks = recursive_chunk(text, max_len=400)  # 3 chunks de ~400 tokens
# â†’ Compatible max_tokens LLM

# NOUVEAU (cassÃ©)
chunks = recursive_chunk(text, max_len=400)  # ~10 chunks de ~400 CHARS
# â†’ DÃ©coupage trop fin, perte de contexte
```

**Correction appliquÃ©e:**
```python
# AVANT (cassÃ©)
chunks.append(text[start:end])  # DÃ©coupage par INDEX de caractÃ¨res

# APRÃˆS (restaurÃ©)
tokens = text.split()
chunk = tokens[i : i + max_len]
chunks.append(" ".join(chunk))  # DÃ©coupage par TOKENS (mots)
```

**Status:** âœ… CORRIGÃ‰ (restauration logique tokens)

---

## âœ… COMPOSANTS VÃ‰RIFIÃ‰S SANS MODIFICATIONS

### Extraction
| Fonction | Fichier | Status |
|----------|---------|--------|
| `extract_notes_and_text()` | binary_parser.py | âœ… Identique |
| `extract_with_python_pptx()` | binary_parser.py | âœ… Identique (tables + charts) |

### Vision Processing
| Fonction | Fichier | Status |
|----------|---------|--------|
| `ask_gpt_slide_analysis()` | vision_analyzer.py | âœ… Identique (prompt rendering, LLM call, JSON parsing) |
| Image encoding base64 | vision_analyzer.py | âœ… Identique |
| Heartbeat worker | vision_analyzer.py | âœ… Identique |

### Utils
| Fonction | Fichier | Status |
|----------|---------|--------|
| `get_language_iso2()` | text_utils.py | âœ… Identique |
| `estimate_tokens()` | text_utils.py | âœ… Identique |

---

## ğŸ“Š SynthÃ¨se des Modifications Refactoring

### Modifications Intentionnelles (OK)
- Modularisation architecture (composants sÃ©parÃ©s)
- Ajout paramÃ¨tres optionnels (logger, llm_router, prompt_registry)
- Ajout docstrings dÃ©taillÃ©es
- Export fonctions via `__init__.py`

### Modifications Non-Intentionnelles (BUGS)
1. âŒ Changement logique extraction MegaParse (regex vs lignes)
2. âŒ Suppression rÃ©paration JSON tronquÃ©
3. âŒ Changement chunking tokens â†’ caractÃ¨res

---

## ğŸ”„ Actions Correctives AppliquÃ©es

### 1. binary_parser.py
- âœ… RestaurÃ© fonction `split_megaparse_by_slide_count()` avec logique lignes
- âœ… AjoutÃ© commentaire "LOGIQUE ORIGINALE (Ã©prouvÃ©e)"
- âœ… ConservÃ© approche hybride (python-pptx pour count + MegaParse pour contenu)

### 2. text_utils.py - clean_gpt_response
- âœ… RestaurÃ© 60 lignes logique rÃ©paration JSON
- âœ… AjoutÃ© paramÃ¨tre `logger` optionnel
- âœ… ConservÃ© gestion erreurs robuste

### 3. text_utils.py - recursive_chunk
- âœ… RestaurÃ© dÃ©coupage par tokens (`.split()`)
- âœ… Mis Ã  jour docstring (prÃ©ciser "TOKENS/mots" vs "caractÃ¨res")
- âœ… ConservÃ© signature fonction identique

---

## ğŸ§ª Tests RecommandÃ©s

### Test 1: Extraction 94 slides
```bash
docker exec knowbase-worker python -c "
from pathlib import Path
from knowbase.ingestion.components.extractors.binary_parser import extract_notes_and_text
slides = extract_notes_and_text(Path('/data/docs_done/SAP_S4HANA_Cloud__public_edition-Security_and_Compliance__20251117_161407.pptx'), None)
assert len(slides) == 94, f'Expected 94 slides, got {len(slides)}'
print('âœ… Extraction OK: 94 slides')
"
```

### Test 2: RÃ©paration JSON tronquÃ©
```python
from knowbase.ingestion.components.utils.text_utils import clean_gpt_response

# JSON tronquÃ© au milieu d'une string
truncated = '{"concepts": [{"name": "SAP S/4HANA'
result = clean_gpt_response(truncated)
assert result == "[]"  # Fallback array vide

# JSON tronquÃ© avec bracket manquant
truncated = '{"concepts": [{"name": "test"}'
result = clean_gpt_response(truncated)
# Devrait rÃ©parer en ajoutant ]}
```

### Test 3: Chunking par tokens
```python
from knowbase.ingestion.components.utils.text_utils import recursive_chunk

text = " ".join(["word"] * 1000)  # 1000 tokens
chunks = recursive_chunk(text, max_len=400, overlap_ratio=0.15)

# VÃ©rifier nombre de chunks
assert len(chunks) == 3, f"Expected 3 chunks, got {len(chunks)}"

# VÃ©rifier taille chunks (en tokens)
for chunk in chunks:
    token_count = len(chunk.split())
    assert token_count <= 400, f"Chunk exceeds max_len: {token_count} tokens"
```

---

## ğŸ“ Recommandations

### Court Terme
1. âœ… **Rebuild worker avec corrections** (en cours)
2. ğŸ”„ **Test import complet** d'un document 94 slides
3. ğŸ”„ **VÃ©rifier logs** pour JSON repairs et chunking

### Moyen Terme
1. **Tests unitaires** pour `clean_gpt_response()` (cas JSON tronquÃ©)
2. **Tests unitaires** pour `recursive_chunk()` (dÃ©coupage tokens)
3. **Tests d'intÃ©gration** extraction MegaParse (nombre slides)

### Long Terme
1. **CI/CD checks** avant merge refactoring
2. **Tests de rÃ©gression** automatisÃ©s
3. **Validation outputs** avant/aprÃ¨s refactoring

---

## ğŸ¯ Conclusion

**3 bugs critiques** identifiÃ©s et corrigÃ©s lors de l'analyse comparative post-refactoring :

1. âœ… Extraction MegaParse (1 slide â†’ 94 slides)
2. âœ… RÃ©paration JSON tronquÃ© (robustesse LLM timeouts)
3. âœ… Chunking par tokens (respect max_tokens LLM)

La modularisation architecturale reste valide, mais la **logique mÃ©tier originale** a Ã©tÃ© restaurÃ©e pour garantir la stabilitÃ© du pipeline.

**Prochaine Ã©tape:** Test import complet avec les corrections appliquÃ©es.
