# Architecture Mode Burst Spot - OSMOSE KnowWhere

*SpÃ©cification technique pour le mode Burst : compute LLM/Embeddings dÃ©portÃ© sur EC2 Spot*

**Version:** 2.1 (Qwen 14B AWQ + Deep Learning AMI)
**Date:** 2025-12-27
**Statut:** Draft - En attente validation

---

## 1. Vue d'ensemble

### 1.1 Clarification fondamentale

**CE QUE BURST FAIT :**
- DÃ©porter **uniquement le compute LLM + Embeddings** sur EC2 Spot
- Le pipeline d'ingestion **reste local**
- Qdrant/Neo4j **restent locaux**
- L'EC2 Spot expose des **endpoints API** que le local consomme

**CE QUE BURST NE FAIT PAS :**
- âŒ Ne dÃ©place PAS les documents vers S3
- âŒ Ne fait PAS tourner le pipeline d'ingestion sur EC2
- âŒ Ne rend PAS Qdrant/Neo4j distants
- âŒ Ne modifie PAS le workflow existant (juste les providers)

### 1.2 Principe simple

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MODE NORMAL (actuel)                         â”‚
â”‚                                                                     â”‚
â”‚  Pipeline Local â†’ OpenAI API (LLM) â†’ GPU Local (Embeddings)        â”‚
â”‚       â†“                                                             â”‚
â”‚  Qdrant/Neo4j (local)                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MODE BURST (nouveau)                         â”‚
â”‚                                                                     â”‚
â”‚  Pipeline Local â†’ EC2 Spot vLLM (LLM) â†’ EC2 Spot GPU (Embeddings)  â”‚
â”‚       â†“                                                             â”‚
â”‚  Qdrant/Neo4j (local)  â† MÃªme destination, diffÃ©rent provider      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**L'EC2 Spot est un "remote provider" temporaire, pas un worker.**

### 1.3 Workflow simplifiÃ©

1. **Admin active Burst** â†’ Demande instance Spot
2. **Attente capacitÃ©** â†’ Normal pour Spot (secondes Ã  minutes)
3. **Instance prÃªte** â†’ vLLM + Embeddings exposÃ©s via API
4. **LLMRouter bascule** â†’ Pointe vers EC2 au lieu d'OpenAI
5. **Import batch local** â†’ Pipeline existant, providers diffÃ©rents
6. **Fin** â†’ Instance Spot terminÃ©e, retour mode normal

### 1.4 Ã‰conomies attendues

| CoÃ»t | Mode Normal | Mode Burst (14B AWQ) | Ã‰conomie |
|------|-------------|----------------------|----------|
| LLM (100 docs) | ~$15 (OpenAI) | ~$1.00 (Spot g6.2xlarge 1.5h) | **93%** |
| Embeddings | GPU local saturÃ© | GPU EC2 dÃ©diÃ© | LibÃ¨re local |
| Vision GPT-4o | ~$3 (40 calls/doc) | ~$1.20 (gating 60%) | **60%** |

**Note:** Qwen 2.5 14B AWQ offre une qualitÃ© nettement supÃ©rieure au 7B pour un coÃ»t Spot lÃ©gÃ¨rement plus Ã©levÃ© (~$0.70-0.90/h pour g6.2xlarge vs ~$0.32/h pour g5.xlarge), mais reste trÃ¨s Ã©conomique face Ã  OpenAI.

---

## 2. Architecture technique

### 2.1 Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LOCAL (Machine User)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    PIPELINE INGESTION                         â”‚  â”‚
â”‚  â”‚                    (INCHANGÃ‰)                                 â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  docs_in/ â†’ pptx_pipeline.py â†’ osmose_agentique.py â†’        â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚   LLMRouter     â”‚     â”‚   EmbeddingManager          â”‚    â”‚  â”‚
â”‚  â”‚  â”‚                 â”‚     â”‚                             â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  Mode Normal:   â”‚     â”‚  Mode Normal:               â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â†’ OpenAI API   â”‚     â”‚  â†’ GPU local (RTX 5070 Ti)  â”‚    â”‚  â”‚
â”‚  â”‚  â”‚                 â”‚     â”‚                             â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  Mode Burst:    â”‚     â”‚  Mode Burst:                â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â†’ EC2 vLLM API â”‚     â”‚  â†’ EC2 Embeddings API       â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚           â”‚                             â”‚                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                             â”‚                       â”‚
â”‚              â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚              â”‚    â”‚                                                â”‚
â”‚              â–¼    â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    STOCKAGE LOCAL                             â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Qdrant   â”‚  â”‚   Neo4j    â”‚  â”‚  data/extraction_cache â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   :6333    â”‚  â”‚   :7474    â”‚  â”‚  (cache LLM responses) â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                 BURST ORCHESTRATOR                            â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  - Provision CloudFormation Spot Fleet                       â”‚  â”‚
â”‚  â”‚  - Wait for instance READY (healthchecks)                    â”‚  â”‚
â”‚  â”‚  - Configure providers (LLMRouter, EmbeddingManager)         â”‚  â”‚
â”‚  â”‚  - Monitor instance health                                    â”‚  â”‚
â”‚  â”‚  - Handle interruptions (retry/resume)                       â”‚  â”‚
â”‚  â”‚  - Teardown when done                                        â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â”‚ HTTPS (API calls)
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EC2 SPOT INSTANCE                           â”‚
â”‚                    (Provider de compute Ã©phÃ©mÃ¨re)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Instance: g6.2xlarge / g6e.xlarge (NVIDIA L4 24GB)                â”‚
â”‚  Fallback: g5.2xlarge (NVIDIA A10G 24GB)                           â”‚
â”‚  AMI: Deep Learning AMI (PyTorch 2.5, Ubuntu 22.04)                â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚   â”‚      vLLM Server        â”‚  â”‚   Embeddings Server      â”‚ â”‚  â”‚
â”‚  â”‚   â”‚                         â”‚  â”‚                          â”‚ â”‚  â”‚
â”‚  â”‚   â”‚  Model: Qwen2.5-14B-AWQ â”‚  â”‚  Model: E5-Large         â”‚ â”‚  â”‚
â”‚  â”‚   â”‚  Quantization: AWQ 4bit â”‚  â”‚  Port: 8001 (TEI 1.5)    â”‚ â”‚  â”‚
â”‚  â”‚   â”‚  Port: 8000             â”‚  â”‚                          â”‚ â”‚  â”‚
â”‚  â”‚   â”‚                         â”‚  â”‚                          â”‚ â”‚  â”‚
â”‚  â”‚   â”‚  API: OpenAI-compatible â”‚  â”‚  API: /embed endpoint    â”‚ â”‚  â”‚
â”‚  â”‚   â”‚  /v1/chat/completions   â”‚  â”‚                          â”‚ â”‚  â”‚
â”‚  â”‚   â”‚                         â”‚  â”‚                          â”‚ â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚   â”‚                  Health Endpoint                      â”‚   â”‚  â”‚
â”‚  â”‚   â”‚                  GET /health â†’ 200 OK                 â”‚   â”‚  â”‚
â”‚  â”‚   â”‚                  (VÃ©rifie vLLM + Embeddings ready)    â”‚   â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  Spot Interruption Handler:                                        â”‚
â”‚  - Monitore http://169.254.169.254/latest/meta-data/spot/          â”‚
â”‚  - Signal 2 min avant terminaison                                  â”‚
â”‚  - Log + graceful shutdown                                          â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 RÃ´le de S3 (minimal)

S3 n'est **PAS** utilisÃ© pour stocker les documents. Il sert uniquement Ã  :

```
s3://knowwhere-burst-{account}/
â”œâ”€â”€ state/
â”‚   â””â”€â”€ burst_state.json      # Ã‰tat du batch (pour reprise)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ {batch_id}/           # Logs CloudWatch export
â””â”€â”€ config/
    â””â”€â”€ burst_config.json     # Config instance (models, etc.)
```

**Pas de staging de documents. Pas d'artifacts. Le pipeline reste 100% local.**

### 2.3 Structure des rÃ©pertoires locaux (inchangÃ©e)

```
data/
â”œâ”€â”€ burst/
â”‚   â””â”€â”€ pending/              # Documents Ã  traiter en batch
â”‚       â”œâ”€â”€ RISE_2025.pptx
â”‚       â””â”€â”€ SAP_Security.pdf
â”‚
â”œâ”€â”€ watch/                    # Mode normal (watcher actif) - INCHANGÃ‰
â”œâ”€â”€ docs_in/                  # Queue import - INCHANGÃ‰
â”œâ”€â”€ docs_done/                # Fichiers traitÃ©s - INCHANGÃ‰
â”œâ”€â”€ extraction_cache/         # Cache LLM - CRITIQUE pour reprise
â””â”€â”€ public/                   # Assets gÃ©nÃ©rÃ©s
```

**Le cache `extraction_cache/` devient encore plus important** car il permet de reprendre aprÃ¨s une interruption Spot sans refaire les appels LLM dÃ©jÃ  effectuÃ©s.

---

## 3. Basculement des providers

### 3.1 LLMRouter - Modification

```python
# src/knowbase/common/llm_router.py

class LLMRouter:
    """Routeur intelligent avec support mode Burst."""

    def __init__(self, config_path: Optional[Path] = None):
        # ... existing init ...
        self._burst_mode = False
        self._burst_endpoint = None

    def enable_burst_mode(self, vllm_url: str):
        """
        Active le mode Burst : redirige les appels LLM vers EC2.

        Args:
            vllm_url: URL du serveur vLLM (ex: http://ec2-xxx:8000)
        """
        self._burst_mode = True
        self._burst_endpoint = vllm_url

        # CrÃ©er client vLLM (OpenAI-compatible)
        from openai import OpenAI
        self._vllm_client = OpenAI(
            api_key="EMPTY",
            base_url=f"{vllm_url}/v1"
        )

        logger.info(f"[LLM_ROUTER] Burst mode ENABLED â†’ {vllm_url}")

    def disable_burst_mode(self):
        """DÃ©sactive le mode Burst, retour aux providers normaux."""
        self._burst_mode = False
        self._burst_endpoint = None
        self._vllm_client = None

        logger.info("[LLM_ROUTER] Burst mode DISABLED â†’ Normal providers")

    def complete(self, task_type: TaskType, messages: List[Dict], **kwargs) -> str:
        """Effectue un appel LLM, routÃ© selon le mode."""

        # Vision reste sur GPT-4o (avec gating)
        if task_type == TaskType.VISION:
            return self._call_openai_vision(messages, **kwargs)

        # Mode Burst : utiliser vLLM distant
        if self._burst_mode and self._vllm_client:
            return self._call_vllm(messages, **kwargs)

        # Mode normal : providers habituels
        return self._call_normal_provider(task_type, messages, **kwargs)
```

### 3.2 EmbeddingManager - Modification

```python
# src/knowbase/common/clients/embeddings.py

class EmbeddingModelManager:
    """Manager embeddings avec support mode Burst."""

    def __init__(self):
        # ... existing init ...
        self._burst_mode = False
        self._burst_endpoint = None

    def enable_burst_mode(self, embeddings_url: str):
        """
        Active le mode Burst : embeddings calculÃ©s sur EC2.

        Args:
            embeddings_url: URL du service embeddings (ex: http://ec2-xxx:8001)
        """
        self._burst_mode = True
        self._burst_endpoint = embeddings_url

        # DÃ©charger le modÃ¨le local pour libÃ©rer GPU
        self._unload_model()

        logger.info(f"[EMBEDDINGS] Burst mode ENABLED â†’ {embeddings_url}")

    def disable_burst_mode(self):
        """DÃ©sactive le mode Burst, retour au GPU local."""
        self._burst_mode = False
        self._burst_endpoint = None

        logger.info("[EMBEDDINGS] Burst mode DISABLED â†’ Local GPU")

    def encode(self, texts: List[str], **kwargs) -> np.ndarray:
        """Encode les textes, routÃ© selon le mode."""

        if self._burst_mode and self._burst_endpoint:
            return self._encode_remote(texts)

        return self._encode_local(texts, **kwargs)

    def _encode_remote(self, texts: List[str]) -> np.ndarray:
        """Appel vers le service embeddings distant."""
        import requests

        response = requests.post(
            f"{self._burst_endpoint}/embed",
            json={"texts": texts},
            timeout=60
        )
        response.raise_for_status()

        embeddings = np.array(response.json()["embeddings"])
        return embeddings
```

### 3.3 Activation/DÃ©sactivation

```python
# src/knowbase/burst/provider_switch.py

def activate_burst_providers(vllm_url: str, embeddings_url: str):
    """
    Active les providers Burst pour le pipeline.
    AppelÃ© par BurstOrchestrator quand l'instance EC2 est prÃªte.
    """
    from knowbase.common.llm_router import get_llm_router
    from knowbase.common.clients.embeddings import get_embedding_manager

    llm_router = get_llm_router()
    embedding_manager = get_embedding_manager()

    llm_router.enable_burst_mode(vllm_url)
    embedding_manager.enable_burst_mode(embeddings_url)

    logger.info("[BURST] All providers switched to EC2 Spot")


def deactivate_burst_providers():
    """
    DÃ©sactive les providers Burst, retour mode normal.
    AppelÃ© quand le batch est terminÃ© ou sur erreur.
    """
    from knowbase.common.llm_router import get_llm_router
    from knowbase.common.clients.embeddings import get_embedding_manager

    llm_router = get_llm_router()
    embedding_manager = get_embedding_manager()

    llm_router.disable_burst_mode()
    embedding_manager.disable_burst_mode()

    logger.info("[BURST] All providers switched back to normal")
```

---

## 4. Gestion des interruptions Spot

### 4.1 Robustesse des appels API

```python
# src/knowbase/burst/resilient_client.py

import time
import requests
from typing import Optional

class ResilientBurstClient:
    """
    Client HTTP rÃ©silient pour appels vers EC2 Spot.
    GÃ¨re timeouts, retries, et dÃ©tection d'interruption.
    """

    def __init__(
        self,
        base_url: str,
        max_retries: int = 3,
        timeout: int = 60,
        backoff_factor: float = 2.0
    ):
        self.base_url = base_url
        self.max_retries = max_retries
        self.timeout = timeout
        self.backoff_factor = backoff_factor

    def post(self, endpoint: str, json: dict) -> dict:
        """POST avec retry et backoff exponentiel."""

        last_exception = None

        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    f"{self.base_url}{endpoint}",
                    json=json,
                    timeout=self.timeout
                )
                response.raise_for_status()
                return response.json()

            except requests.exceptions.Timeout:
                logger.warning(f"[BURST] Timeout attempt {attempt + 1}/{self.max_retries}")
                last_exception = TimeoutError("EC2 Spot timeout")

            except requests.exceptions.ConnectionError as e:
                # Possible interruption Spot
                logger.warning(f"[BURST] Connection error: {e}")
                last_exception = e

            except requests.exceptions.HTTPError as e:
                if e.response.status_code >= 500:
                    # Erreur serveur, retry
                    logger.warning(f"[BURST] Server error {e.response.status_code}")
                    last_exception = e
                else:
                    # Erreur client, pas de retry
                    raise

            # Backoff exponentiel
            if attempt < self.max_retries - 1:
                sleep_time = self.backoff_factor ** attempt
                logger.info(f"[BURST] Retry in {sleep_time}s...")
                time.sleep(sleep_time)

        # Tous les retries Ã©chouÃ©s
        raise BurstProviderUnavailable(
            f"EC2 Spot unreachable after {self.max_retries} attempts",
            last_exception
        )


class BurstProviderUnavailable(Exception):
    """Exception quand le provider Burst n'est plus accessible."""
    pass
```

### 4.2 Reprise via cache

Le `extraction_cache` existant permet la reprise automatique :

```python
# Workflow de reprise aprÃ¨s interruption Spot

def process_document_with_cache(doc_path: Path):
    """
    Traite un document, utilise le cache si disponible.
    Permet la reprise aprÃ¨s interruption Spot.
    """
    from knowbase.ingestion.extraction_cache import ExtractionCacheManager

    cache_manager = ExtractionCacheManager()

    # VÃ©rifier si dÃ©jÃ  dans le cache
    cached = cache_manager.get_cache_for_file(doc_path)

    if cached:
        logger.info(f"[BURST:CACHE] Using cached extraction for {doc_path.name}")
        return cached

    try:
        # Extraction via provider Burst
        result = extract_document(doc_path)

        # Sauvegarder dans le cache
        cache_manager.save_cache(doc_path, result)

        return result

    except BurstProviderUnavailable:
        # Interruption Spot probable
        logger.warning(f"[BURST] Provider unavailable for {doc_path.name}")
        raise  # Propagate pour que l'orchestrateur gÃ¨re
```

### 4.3 Ã‰tats de l'orchestrateur

```python
class BurstStatus(str, Enum):
    """Ã‰tats du mode Burst."""

    IDLE = "idle"                       # Pas de batch actif
    REQUESTING_SPOT = "requesting_spot"  # CloudFormation en cours
    WAITING_CAPACITY = "waiting_capacity" # Attente allocation Spot
    INSTANCE_STARTING = "instance_starting" # Boot + init services
    READY = "ready"                     # Providers disponibles
    PROCESSING = "processing"           # Batch en cours
    INTERRUPTED = "interrupted"         # Spot perdu, reprise en cours
    RESUMING = "resuming"               # Nouvelle instance, reprise
    COMPLETED = "completed"             # Batch terminÃ©
    FAILED = "failed"                   # Erreur fatale
```

---

## 5. Burst Orchestrator

### 5.1 Classe principale

```python
# src/knowbase/burst/orchestrator.py

import os
import time
import boto3
from typing import Optional, List, Dict
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime

from knowbase.burst.provider_switch import (
    activate_burst_providers,
    deactivate_burst_providers
)


@dataclass
class BurstState:
    """Ã‰tat persistant du mode Burst."""
    batch_id: str
    status: BurstStatus
    documents: List[str]
    documents_done: List[str]
    documents_failed: List[str]
    spot_fleet_id: Optional[str] = None
    instance_id: Optional[str] = None
    instance_ip: Optional[str] = None
    instance_type: Optional[str] = None
    started_at: Optional[str] = None
    interruption_count: int = 0
    events: List[Dict] = None


class BurstOrchestrator:
    """
    Orchestre le mode Burst :
    - Provision EC2 Spot
    - Bascule les providers
    - GÃ¨re les interruptions
    - Teardown Ã  la fin
    """

    def __init__(self):
        self.state: Optional[BurstState] = None
        self.cf_client = boto3.client('cloudformation')
        self.ec2_client = boto3.client('ec2')

        # Config
        self.vllm_port = 8000
        self.embeddings_port = 8001
        self.health_check_interval = 10
        self.health_check_timeout = 600  # 10 min max pour boot

    def start_burst_batch(self, document_paths: List[Path]) -> str:
        """
        DÃ©marre un batch en mode Burst.

        1. CrÃ©e le state
        2. Lance CloudFormation
        3. Attend instance ready
        4. Bascule providers
        5. Retourne batch_id pour suivi
        """
        batch_id = f"burst-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

        self.state = BurstState(
            batch_id=batch_id,
            status=BurstStatus.REQUESTING_SPOT,
            documents=[str(p) for p in document_paths],
            documents_done=[],
            documents_failed=[],
            events=[]
        )

        self._add_event("batch_created", f"Batch crÃ©Ã© avec {len(document_paths)} documents")

        try:
            # 1. DÃ©ployer CloudFormation
            self._deploy_spot_infrastructure(batch_id)

            # 2. Attendre instance ready
            self._wait_for_instance_ready()

            # 3. Basculer providers
            self._switch_to_burst_providers()

            self.state.status = BurstStatus.READY
            self._add_event("ready", "Instance prÃªte, providers activÃ©s")

            return batch_id

        except Exception as e:
            self.state.status = BurstStatus.FAILED
            self._add_event("error", f"Ã‰chec dÃ©marrage: {e}", severity="error")
            self._cleanup()
            raise

    def process_batch(self):
        """
        Traite le batch de documents.
        AppelÃ© aprÃ¨s start_burst_batch.
        """
        if self.state.status != BurstStatus.READY:
            raise ValueError(f"Cannot process in status: {self.state.status}")

        self.state.status = BurstStatus.PROCESSING
        self.state.started_at = datetime.now().isoformat()

        pending = [d for d in self.state.documents
                   if d not in self.state.documents_done
                   and d not in self.state.documents_failed]

        for doc_path in pending:
            try:
                self._add_event("doc_started", f"Traitement: {Path(doc_path).name}")

                # Appeler le pipeline existant (qui utilisera les providers Burst)
                self._process_single_document(Path(doc_path))

                self.state.documents_done.append(doc_path)
                self._add_event("doc_completed", f"TerminÃ©: {Path(doc_path).name}")

            except BurstProviderUnavailable:
                # Interruption Spot probable
                self._add_event("spot_interrupted", "Instance Spot interrompue", severity="warning")
                self.state.status = BurstStatus.INTERRUPTED
                self._handle_interruption()
                break

            except Exception as e:
                self.state.documents_failed.append(doc_path)
                self._add_event("doc_failed", f"Ã‰chec: {Path(doc_path).name} - {e}", severity="error")

        # VÃ©rifier si tout est fait
        if len(self.state.documents_done) == len(self.state.documents):
            self._complete_batch()

    def _handle_interruption(self):
        """GÃ¨re une interruption Spot."""
        self.state.interruption_count += 1
        self._add_event("resuming", f"Tentative reprise #{self.state.interruption_count}")

        # DÃ©sactiver providers
        deactivate_burst_providers()

        # Nouvelle instance Spot
        self.state.status = BurstStatus.RESUMING

        try:
            # Attendre nouvelle instance (CloudFormation maintient la fleet)
            self._wait_for_instance_ready()
            self._switch_to_burst_providers()

            # Reprendre le traitement
            self.state.status = BurstStatus.READY
            self.process_batch()  # Recursive, reprend les pending

        except Exception as e:
            self._add_event("resume_failed", f"Ã‰chec reprise: {e}", severity="error")
            self.state.status = BurstStatus.FAILED

    def _complete_batch(self):
        """Finalise le batch."""
        self.state.status = BurstStatus.COMPLETED

        # DÃ©sactiver providers
        deactivate_burst_providers()

        # Teardown infrastructure
        self._teardown_infrastructure()

        self._add_event("batch_completed",
            f"Batch terminÃ©: {len(self.state.documents_done)} rÃ©ussis, "
            f"{len(self.state.documents_failed)} Ã©checs, "
            f"{self.state.interruption_count} interruptions")

    def _deploy_spot_infrastructure(self, batch_id: str):
        """DÃ©ploie le stack CloudFormation."""
        self._add_event("cloudformation_started", "DÃ©ploiement infrastructure")

        stack_name = f"knowwhere-burst-{batch_id}"

        # Charger template
        template_path = Path(__file__).parent / "cloudformation" / "burst-spot.yaml"
        with open(template_path) as f:
            template_body = f.read()

        self.cf_client.create_stack(
            StackName=stack_name,
            TemplateBody=template_body,
            Parameters=[
                {"ParameterKey": "BatchId", "ParameterValue": batch_id},
                # ... autres params
            ],
            Capabilities=["CAPABILITY_IAM"]
        )

        # Attendre crÃ©ation
        self.state.status = BurstStatus.WAITING_CAPACITY
        self._add_event("spot_requested", "En attente de capacitÃ© Spot")

        waiter = self.cf_client.get_waiter('stack_create_complete')
        waiter.wait(StackName=stack_name)

        self.state.spot_fleet_id = stack_name
        self._add_event("cloudformation_completed", "Infrastructure dÃ©ployÃ©e")

    def _wait_for_instance_ready(self):
        """Attend que l'instance soit prÃªte (healthcheck)."""
        self.state.status = BurstStatus.INSTANCE_STARTING

        # RÃ©cupÃ©rer IP de l'instance
        self._update_instance_info()

        if not self.state.instance_ip:
            raise RuntimeError("No instance IP available")

        self._add_event("instance_starting",
            f"Instance {self.state.instance_id} ({self.state.instance_type}) dÃ©marrage")

        # Healthcheck loop
        vllm_url = f"http://{self.state.instance_ip}:{self.vllm_port}"
        embeddings_url = f"http://{self.state.instance_ip}:{self.embeddings_port}"

        start_time = time.time()

        while time.time() - start_time < self.health_check_timeout:
            try:
                # Check vLLM
                resp_vllm = requests.get(f"{vllm_url}/health", timeout=5)
                # Check embeddings
                resp_emb = requests.get(f"{embeddings_url}/health", timeout=5)

                if resp_vllm.ok and resp_emb.ok:
                    self._add_event("services_ready", "vLLM + Embeddings prÃªts")
                    return

            except requests.exceptions.RequestException:
                pass

            time.sleep(self.health_check_interval)

        raise TimeoutError("Instance not ready within timeout")

    def _switch_to_burst_providers(self):
        """Bascule les providers vers EC2."""
        vllm_url = f"http://{self.state.instance_ip}:{self.vllm_port}"
        embeddings_url = f"http://{self.state.instance_ip}:{self.embeddings_port}"

        activate_burst_providers(vllm_url, embeddings_url)

        self._add_event("providers_switched", "Providers basculÃ©s vers EC2 Spot")

    def _add_event(self, event_type: str, message: str, severity: str = "info"):
        """Ajoute un Ã©vÃ©nement Ã  la timeline."""
        event = {
            "timestamp": datetime.now().isoformat(),
            "type": event_type,
            "message": message,
            "severity": severity
        }
        self.state.events.append(event)
        logger.info(f"[BURST:{event_type.upper()}] {message}")

    # ... autres mÃ©thodes (_process_single_document, _teardown_infrastructure, etc.)
```

---

## 6. Services EC2 Spot

### 6.1 Configuration Qwen 2.5 14B AWQ

**ModÃ¨le choisi :** `Qwen/Qwen2.5-14B-Instruct-AWQ`
- Quantification AWQ 4-bit â†’ ~8GB VRAM (vs 28GB pour FP16)
- QualitÃ© supÃ©rieure au 7B, proche du 14B full precision
- Compatible avec GPU 24GB (L4, A10G)

**Configuration vLLM pour AWQ :**
```bash
# ParamÃ¨tres obligatoires pour AWQ
--quantization awq      # Active le dÃ©codeur AWQ
--dtype half            # FP16 pour infÃ©rence quantifiÃ©e
--gpu-memory-utilization 0.85  # ~20GB sur 24GB (14B AWQ + TEI)
--max-model-len 8192    # Context window raisonnable
--max-num-seqs 32       # Limite concurrence pour stabilitÃ©
```

### 6.2 User Data (Deep Learning AMI)

Utilise AWS Deep Learning AMI avec NVIDIA drivers prÃ©installÃ©s (boot rapide):

```bash
#!/bin/bash
set -ex

# Variables injectÃ©es par CloudFormation
VLLM_MODEL="${VllmModel:-Qwen/Qwen2.5-14B-Instruct-AWQ}"
EMBEDDINGS_MODEL="${EmbeddingsModel:-intfloat/multilingual-e5-large}"
VLLM_QUANTIZATION="${VllmQuantization:-awq}"
VLLM_DTYPE="${VllmDtype:-half}"
VLLM_GPU_MEM="${VllmGpuMemoryUtilization:-0.85}"
VLLM_MAX_MODEL_LEN="${VllmMaxModelLen:-8192}"
VLLM_MAX_NUM_SEQS="${VllmMaxNumSeqs:-32}"

# Deep Learning AMI - Docker dÃ©jÃ  installÃ©, juste dÃ©marrer
systemctl start docker
systemctl enable docker

# Pull images en parallÃ¨le
docker pull vllm/vllm-openai:latest &
docker pull ghcr.io/huggingface/text-embeddings-inference:1.5 &
wait

# Construire les arguments vLLM
VLLM_ARGS="--model $VLLM_MODEL --max-model-len $VLLM_MAX_MODEL_LEN"
VLLM_ARGS="$VLLM_ARGS --gpu-memory-utilization $VLLM_GPU_MEM"
VLLM_ARGS="$VLLM_ARGS --max-num-seqs $VLLM_MAX_NUM_SEQS --trust-remote-code"

# Ajouter quantization si spÃ©cifiÃ© (AWQ)
if [ "$VLLM_QUANTIZATION" != "none" ]; then
    VLLM_ARGS="$VLLM_ARGS --quantization $VLLM_QUANTIZATION"
fi

# Ajouter dtype
VLLM_ARGS="$VLLM_ARGS --dtype $VLLM_DTYPE"

# Start vLLM (port 8000)
docker run -d --gpus all \
  -p 8000:8000 \
  --name vllm \
  -e HF_TOKEN="${HfToken:-}" \
  vllm/vllm-openai:latest \
  $VLLM_ARGS

# Start Embeddings TEI 1.5 (port 8001)
docker run -d --gpus all \
  -p 8001:80 \
  --name embeddings \
  ghcr.io/huggingface/text-embeddings-inference:1.5 \
  --model-id $EMBEDDINGS_MODEL

# Health check endpoint combinÃ©
cat > /opt/health.py << 'HEALTHEOF'
from http.server import HTTPServer, BaseHTTPRequestHandler
import requests

class HealthHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        try:
            vllm = requests.get("http://localhost:8000/health", timeout=5)
            emb = requests.get("http://localhost:8001/health", timeout=5)
            if vllm.ok and emb.ok:
                self.send_response(200)
                self.end_headers()
                self.wfile.write(b'{"status": "healthy", "vllm": "ok", "embeddings": "ok"}')
            else:
                self.send_response(503)
                self.end_headers()
                self.wfile.write(b'{"status": "starting"}')
        except:
            self.send_response(503)
            self.end_headers()
            self.wfile.write(b'{"status": "starting"}')

HTTPServer(("", 8080), HealthHandler).serve_forever()
HEALTHEOF

pip install requests  # Peut Ãªtre absent de DLAMI
python3 /opt/health.py &

echo "Bootstrap complete - services starting"
```

**Temps de boot estimÃ© :**
- Deep Learning AMI : ~2-3 min (vs 10+ min avec drivers Ã  installer)
- Pull images : ~2-3 min (vLLM + TEI en parallÃ¨le)
- Chargement modÃ¨le 14B AWQ : ~3-5 min
- **Total : ~8-12 min** (vs 15-20 min avec AMI standard)

### 6.3 CloudFormation (rÃ©sumÃ©)

Le template CloudFormation complet (`burst-spot.yaml`) inclut :

```yaml
# ParamÃ¨tres principaux
Parameters:
  VllmModel:
    Default: "Qwen/Qwen2.5-14B-Instruct-AWQ"
  VllmQuantization:
    Default: "awq"
    AllowedValues: ["awq", "gptq", "squeezellm", "none"]
  VllmDtype:
    Default: "half"
  VllmGpuMemoryUtilization:
    Default: "0.85"
  VllmMaxModelLen:
    Default: "8192"
  VllmMaxNumSeqs:
    Default: "32"
  EmbeddingsModel:
    Default: "intfloat/multilingual-e5-large"

# AMI Deep Learning via SSM (rÃ©solution dynamique)
ImageId: !Sub "{{resolve:ssm:/aws/service/deeplearning/ami/x86_64/oss-nvidia-driver-gpu-pytorch-2.5-ubuntu-22.04/latest/ami-id}}"

# Instances prioritaires (L4 GPU prÃ©fÃ©rÃ©)
LaunchTemplateOverrides:
  - InstanceType: g6.2xlarge   # L4 24GB - prioritÃ© 1
  - InstanceType: g6e.xlarge   # L4 24GB - prioritÃ© 2
  - InstanceType: g5.2xlarge   # A10G 24GB - fallback

# StratÃ©gie Spot
AllocationStrategy: capacityOptimizedPrioritized
SpotMaxTotalPrice: "1.20"  # Max $1.20/h
```

**Points clÃ©s :**
- AMI rÃ©solu dynamiquement via SSM Parameter Store (toujours la derniÃ¨re version)
- 3 types d'instances avec prioritÃ© (g6 prÃ©fÃ©rÃ©, g5 en fallback)
- Allocation optimisÃ©e par capacitÃ© avec prioritÃ©
- Budget max $1.20/h pour contrÃ´le des coÃ»ts

---

## 7. Interface Admin (simplifiÃ©e)

### 7.1 Ã‰tats principaux

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Admin > Burst Mode                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STATUS: âšª IDLE / ğŸ”„ PROCESSING / âš ï¸ INTERRUPTED / âœ… DONE â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   FICHIERS PENDING          â”‚  â”‚   TIMELINE                 â”‚   â”‚
â”‚  â”‚   data/burst/pending/       â”‚  â”‚                            â”‚   â”‚
â”‚  â”‚                             â”‚  â”‚   14:30 Batch crÃ©Ã©         â”‚   â”‚
â”‚  â”‚   ğŸ“„ RISE_2025.pptx         â”‚  â”‚   14:31 Spot demandÃ©       â”‚   â”‚
â”‚  â”‚   ğŸ“„ SAP_Security.pdf       â”‚  â”‚   14:35 Instance allouÃ©e   â”‚   â”‚
â”‚  â”‚   ğŸ“„ BTP_Overview.pptx      â”‚  â”‚   14:37 Services ready     â”‚   â”‚
â”‚  â”‚   ...                       â”‚  â”‚   14:38 Doc #1 started     â”‚   â”‚
â”‚  â”‚                             â”‚  â”‚   ...                      â”‚   â”‚
â”‚  â”‚   Total: 100 fichiers       â”‚  â”‚                            â”‚   â”‚
â”‚  â”‚                             â”‚  â”‚                            â”‚   â”‚
â”‚  â”‚   [ğŸš€ Lancer Burst]         â”‚  â”‚                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PROGRESSION: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  48/100                â”‚   â”‚
â”‚  â”‚  âœ… Done: 48  ğŸ”„ Current: 1  â³ Pending: 51  âŒ Failed: 0    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  Instance: i-0abc123 (g5.xlarge) @ $0.32/h                         â”‚
â”‚  Interruptions: 0                                                   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Ã‰tats spÃ©cifiques

**WAITING_CAPACITY:**
```
â³ En attente de capacitÃ© Spot...
   Temps d'attente: 3 min 22 sec

   â„¹ï¸ Normal pour Spot, peut prendre quelques minutes
```

**INTERRUPTED:**
```
âš ï¸ Instance Spot interrompue par AWS

   Documents traitÃ©s: 48/100
   Tentative reprise: 2/5

   ğŸ”„ Demande nouvelle instance en cours...
```

---

## 8. Configuration

### 8.1 Variables d'environnement

```bash
# .env

# === Mode Burst ===
BURST_MODE_ENABLED=true

# === AWS ===
BURST_AWS_REGION=eu-west-1
BURST_VPC_ID=vpc-xxx
BURST_SUBNET_ID=subnet-xxx

# === Spot (g6 pour L4 GPU, g5 en fallback) ===
BURST_SPOT_MAX_PRICE=1.20
BURST_SPOT_INSTANCE_TYPES=g6.2xlarge,g6e.xlarge,g5.2xlarge

# === Models (sur EC2) ===
BURST_VLLM_MODEL=Qwen/Qwen2.5-14B-Instruct-AWQ
BURST_EMBEDDINGS_MODEL=intfloat/multilingual-e5-large

# === vLLM AWQ Configuration ===
BURST_VLLM_QUANTIZATION=awq        # awq, gptq, squeezellm, none
BURST_VLLM_DTYPE=half              # FP16 pour AWQ
BURST_VLLM_GPU_MEMORY_UTILIZATION=0.85
BURST_VLLM_MAX_MODEL_LEN=8192
BURST_VLLM_MAX_NUM_SEQS=32

# === Deep Learning AMI ===
BURST_USE_DEEP_LEARNING_AMI=true
BURST_DEEP_LEARNING_AMI_OS=ubuntu-22.04  # ou amazon-linux-2023

# === Timeouts (augmentÃ©s pour 14B) ===
BURST_INSTANCE_BOOT_TIMEOUT=900    # 15 min (DLAMI + modÃ¨le 14B)
BURST_MODEL_LOAD_TIMEOUT=600       # 10 min pour le modÃ¨le seul
BURST_HEALTHCHECK_INTERVAL=15
BURST_HEALTHCHECK_TIMEOUT=10
BURST_MAX_RETRIES=3
BURST_MAX_INTERRUPTION_RETRIES=5
```

---

## 9. Plan d'implÃ©mentation

### Mise Ã  jour v2.1 : Qwen 14B AWQ + Deep Learning AMI âœ… COMPLÃ‰TÃ‰ 2025-12-27

Suite Ã  l'analyse de la configuration initiale, les modifications suivantes ont Ã©tÃ© apportÃ©es :

**1. ModÃ¨le upgradÃ© de 7B Ã  14B AWQ :**
- `Qwen/Qwen2.5-7B-Instruct` â†’ `Qwen/Qwen2.5-14B-Instruct-AWQ`
- QualitÃ© nettement supÃ©rieure, mÃªme coÃ»t Spot grÃ¢ce Ã  la quantification AWQ

**2. Configuration vLLM pour AWQ ajoutÃ©e :**
- `--quantization awq` : Active le dÃ©codeur AWQ
- `--dtype half` : FP16 obligatoire pour AWQ
- `--gpu-memory-utilization 0.85` : Cohabitation avec TEI
- `--max-model-len 8192` : Context window
- `--max-num-seqs 32` : Limite concurrence

**3. Instances GPU optimisÃ©es :**
- `g5.xlarge` â†’ `g6.2xlarge, g6e.xlarge, g5.2xlarge`
- L4 GPU (g6) prÃ©fÃ©rÃ© : plus rÃ©cent, meilleur rapport performance/prix
- A10G (g5) en fallback pour disponibilitÃ©

**4. Deep Learning AMI :**
- AMI Amazon Linux 2023 â†’ AWS Deep Learning AMI (PyTorch 2.5, Ubuntu 22.04)
- RÃ©solution dynamique via SSM Parameter Store
- Drivers NVIDIA prÃ©installÃ©s â†’ boot 5-10 min plus rapide

**5. Timeouts augmentÃ©s :**
- Boot timeout : 600s â†’ 900s (modÃ¨le 14B plus long Ã  charger)
- Model load timeout : 600s (nouveau paramÃ¨tre)

**Fichiers modifiÃ©s :**
- `src/knowbase/ingestion/burst/types.py` : +nouveaux paramÃ¨tres AWQ et AMI
- `src/knowbase/ingestion/burst/cloudformation/burst-spot.yaml` : RÃ©Ã©crit pour 14B AWQ

---

### Phase 1: Provider Switch (1-2 jours) âœ… COMPLÃ‰TÃ‰ 2025-12-27
- [x] Modifier `LLMRouter` pour support burst mode (+140 lignes)
- [x] Modifier `EmbeddingManager` pour support burst mode (+80 lignes)
- [x] CrÃ©er `provider_switch.py` (210 lignes)
- [x] CrÃ©er `resilient_client.py` (290 lignes)
- [x] Tests imports validÃ©s

**Fichiers modifiÃ©s/crÃ©Ã©s :**
- `src/knowbase/common/llm_router.py` : `enable_burst_mode()`, `disable_burst_mode()`, `_call_burst_vllm()`
- `src/knowbase/common/clients/embeddings.py` : `enable_burst_mode()`, `disable_burst_mode()`, `_encode_remote()`
- `src/knowbase/ingestion/burst/provider_switch.py` : activation/dÃ©sactivation coordonnÃ©e
- `src/knowbase/ingestion/burst/resilient_client.py` : retry/backoff pour appels EC2

### Phase 2: Orchestrateur (2-3 jours) âœ… COMPLÃ‰TÃ‰ 2025-12-27
- [x] CrÃ©er `BurstOrchestrator` (550 lignes)
- [x] CrÃ©er `types.py` avec BurstState, BurstStatus, BurstConfig (250 lignes)
- [x] CloudFormation Spot Fleet template (340 lignes)
- [x] Healthcheck logic
- [x] Interruption handling avec reprise automatique
- [x] Timeline d'Ã©vÃ©nements

**Fichiers crÃ©Ã©s :**
- `src/knowbase/ingestion/burst/types.py` : 12 Ã©tats, dataclasses sÃ©rialisables
- `src/knowbase/ingestion/burst/orchestrator.py` : cycle de vie complet
- `src/knowbase/ingestion/burst/cloudformation/burst-spot.yaml` : template Spot Fleet

### Phase 3: Services EC2 (1-2 jours) âœ… INCLUS DANS PHASE 2
- [x] Script bootstrap (UserData dans CloudFormation)
- [x] Docker vLLM + TEI configurÃ©
- [x] Health endpoint Python

### Phase 4: API & Admin (2 jours) âœ… COMPLÃ‰TÃ‰ 2025-12-27
- [x] Endpoints `/api/burst/*` (570 lignes)
  - GET /status - Statut actuel du mode Burst
  - GET /config - Configuration Burst
  - POST /prepare - PrÃ©parer un batch de documents
  - POST /start - DÃ©marrer l'infrastructure Spot
  - POST /process - Lancer le traitement du batch
  - POST /cancel - Annuler le batch en cours
  - GET /events - Timeline des Ã©vÃ©nements
  - GET /documents - Statut des documents du batch
  - GET /providers - Statut des providers (LLM/Embeddings)
- [x] Page admin simplifiÃ©e (550 lignes)
  - Dashboard avec statut, progression, statistiques
  - Actions: PrÃ©parer/DÃ©marrer/Lancer/Annuler
  - Timeline Ã©vÃ©nements temps rÃ©el
  - Liste documents avec statuts
  - Configuration affichÃ©e
- [x] Timeline events frontend avec auto-refresh 5s

**Fichiers crÃ©Ã©s:**
- `src/knowbase/api/routers/burst.py` : Endpoints API complets
- `frontend/src/app/admin/burst/page.tsx` : Page admin Chakra UI

### Phase 5: Tests E2E (1-2 jours) â³ Ã€ FAIRE
- [ ] Test batch complet
- [ ] Test interruption Spot
- [ ] Documentation

**Total estimÃ©: 7-11 jours**
**RÃ©alisÃ©: Phases 1-4 en 2 sessions (env. 3-4h)**

---

## 10. RÃ©sumÃ©

### Architecture

| Aspect | Avant (v1.0) | AprÃ¨s (v2.0/2.1) |
|--------|--------------|------------------|
| Documents | Upload S3 | Restent locaux |
| Pipeline | Sur EC2 | Local (inchangÃ©) |
| EC2 Spot | Worker complet | Provider API |
| Qdrant/Neo4j | Import depuis S3 | Local direct |
| ComplexitÃ© | Haute | Moyenne |
| S3 usage | Documents + artifacts | Ã‰tat minimal |

### Configuration v2.1

| Composant | Configuration |
|-----------|---------------|
| **ModÃ¨le LLM** | Qwen/Qwen2.5-14B-Instruct-AWQ (4-bit quantifiÃ©) |
| **Quantification** | AWQ avec dtype=half |
| **Embeddings** | intfloat/multilingual-e5-large (TEI 1.5) |
| **Instances** | g6.2xlarge (L4), g6e.xlarge, g5.2xlarge (fallback) |
| **AMI** | Deep Learning AMI PyTorch 2.5 Ubuntu 22.04 |
| **VRAM** | ~10GB vLLM + ~2GB TEI = ~12GB / 24GB |
| **CoÃ»t max** | $1.20/h (Spot) |
| **Boot time** | ~8-12 min (DLAMI optimisÃ©) |

**L'EC2 Spot est un endpoint de compute temporaire, pas un worker.**

---

*Document v2.1 - Ajout support Qwen 14B AWQ + Deep Learning AMI*
