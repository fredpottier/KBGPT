"""
Module Test Analyzer - Analyse couverture de tests.

Fonctionnalit√©s:
- Ex√©cution des tests avec couverture
- D√©tection de fonctions sans tests
- Analyse de la qualit√© des tests
- Suggestions de tests manquants
"""
import subprocess
import json
from pathlib import Path
from typing import Dict, List, Any
import ast


class TestAnalyzer:
    """Analyseur de tests et couverture."""

    def __init__(self, project_root: Path):
        """
        Initialise l'analyseur de tests.

        Args:
            project_root: Racine du projet
        """
        self.project_root = project_root
        self.src_dir = project_root / "src" / "knowbase"
        self.tests_dir = project_root / "tests"
        self.results = {
            "coverage": {},
            "missing_tests": [],
            "test_quality": [],
            "execution_results": {}
        }

    def analyze(self) -> Dict[str, Any]:
        """
        Ex√©cute l'analyse compl√®te des tests.

        Returns:
            R√©sultats de l'analyse
        """
        print("üß™ Test Analyzer - D√©marrage analyse...")

        # 1. Ex√©cuter les tests avec couverture
        self._run_tests_with_coverage()

        # 2. Identifier les fonctions sans tests
        self._detect_missing_tests()

        # 3. Analyser la qualit√© des tests existants
        self._analyze_test_quality()

        return {
            "total_tests": self.results["execution_results"].get("total", 0),
            "passed": self.results["execution_results"].get("passed", 0),
            "failed": self.results["execution_results"].get("failed", 0),
            "coverage_percent": self.results["coverage"].get("total_percent", 0),
            "missing_tests_count": len(self.results["missing_tests"]),
            "details": self.results,
            "summary": self._generate_summary()
        }

    def _run_tests_with_coverage(self):
        """Ex√©cute les tests avec couverture."""
        print("  üèÉ Ex√©cution tests avec couverture...")

        try:
            # Ex√©cuter pytest avec coverage
            result = subprocess.run(
                [
                    "docker-compose", "exec", "-T", "app",
                    "pytest",
                    "--cov=src/knowbase",
                    "--cov-report=json",
                    "--cov-report=term",
                    "-v"
                ],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5 minutes max
            )

            # Parser la sortie pytest
            self._parse_pytest_output(result.stdout)

            # Lire le rapport de couverture JSON
            coverage_file = self.project_root / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file, 'r') as f:
                    coverage_data = json.load(f)
                    self.results["coverage"] = {
                        "total_percent": coverage_data.get("totals", {}).get("percent_covered", 0),
                        "files": self._extract_coverage_details(coverage_data)
                    }

            print(f"    ‚úì Tests ex√©cut√©s - Couverture: {self.results['coverage'].get('total_percent', 0):.1f}%")

        except subprocess.TimeoutExpired:
            print("    ‚ö† Timeout tests (> 5 min)")
            self.results["execution_results"]["error"] = "Timeout"
        except Exception as e:
            print(f"    ‚ö† Erreur ex√©cution tests: {e}")
            self.results["execution_results"]["error"] = str(e)

    def _parse_pytest_output(self, output: str):
        """
        Parse la sortie de pytest.

        Args:
            output: Sortie stdout de pytest
        """
        lines = output.split('\n')
        total = 0
        passed = 0
        failed = 0

        for line in lines:
            if 'passed' in line.lower():
                # Format: "X passed in Y.Ys"
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == 'passed' and i > 0:
                        try:
                            passed = int(parts[i - 1])
                        except (ValueError, IndexError):
                            pass
                    elif part == 'failed' and i > 0:
                        try:
                            failed = int(parts[i - 1])
                        except (ValueError, IndexError):
                            pass

        total = passed + failed

        self.results["execution_results"] = {
            "total": total,
            "passed": passed,
            "failed": failed,
            "success_rate": (passed / total * 100) if total > 0 else 0
        }

    def _extract_coverage_details(self, coverage_data: Dict) -> List[Dict]:
        """
        Extrait les d√©tails de couverture par fichier.

        Args:
            coverage_data: Donn√©es de couverture pytest

        Returns:
            Liste des fichiers avec leur couverture
        """
        files_coverage = []

        for file_path, file_data in coverage_data.get("files", {}).items():
            if "src/knowbase" in file_path:
                summary = file_data.get("summary", {})
                percent = summary.get("percent_covered", 0)

                # Identifier les fichiers avec faible couverture
                if percent < 70:
                    files_coverage.append({
                        "file": file_path,
                        "coverage": percent,
                        "missing_lines": file_data.get("missing_lines", []),
                        "severity": "critical" if percent < 50 else "warning"
                    })

        # Trier par couverture croissante
        files_coverage.sort(key=lambda x: x["coverage"])

        return files_coverage

    def _detect_missing_tests(self):
        """D√©tecte les fonctions/classes sans tests."""
        print("  üîç D√©tection fonctions sans tests...")

        # Collecter toutes les fonctions publiques
        functions_in_src = self._collect_functions(self.src_dir)

        # Collecter toutes les fonctions test√©es
        tested_functions = set()
        if self.tests_dir.exists():
            for test_file in self.tests_dir.rglob("test_*.py"):
                try:
                    with open(test_file, 'r', encoding='utf-8-sig') as f:
                        content = f.read()
                        # Extraire les noms de fonctions mentionn√©es dans les tests
                        for func_name in functions_in_src:
                            if func_name in content:
                                tested_functions.add(func_name)
                except Exception as e:
                    print(f"    ‚ö† Erreur lecture {test_file.name}: {e}")

        # Identifier les fonctions sans tests
        for func_info in functions_in_src.values():
            if func_info["name"] not in tested_functions:
                self.results["missing_tests"].append({
                    "function": func_info["name"],
                    "file": func_info["file"],
                    "line": func_info["line"],
                    "type": func_info["type"]
                })

        print(f"    ‚úì {len(self.results['missing_tests'])} fonctions sans tests d√©tect√©es")

    def _collect_functions(self, directory: Path) -> Dict[str, Dict]:
        """
        Collecte toutes les fonctions publiques d'un r√©pertoire.

        Args:
            directory: R√©pertoire √† analyser

        Returns:
            Dict {fonction_name: info}
        """
        functions = {}

        for py_file in directory.rglob("*.py"):
            if py_file.name.startswith("__"):
                continue

            try:
                with open(py_file, 'r', encoding='utf-8-sig') as f:
                    tree = ast.parse(f.read(), filename=str(py_file))

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        # Ignorer les fonctions priv√©es
                        if not node.name.startswith('_'):
                            functions[node.name] = {
                                "name": node.name,
                                "file": str(py_file.relative_to(self.project_root)),
                                "line": node.lineno,
                                "type": "function"
                            }
                    elif isinstance(node, ast.ClassDef):
                        # Collecter les m√©thodes publiques
                        for item in node.body:
                            if isinstance(item, ast.FunctionDef) and not item.name.startswith('_'):
                                full_name = f"{node.name}.{item.name}"
                                functions[full_name] = {
                                    "name": full_name,
                                    "file": str(py_file.relative_to(self.project_root)),
                                    "line": item.lineno,
                                    "type": "method"
                                }

            except Exception as e:
                print(f"    ‚ö† Erreur parsing {py_file.name}: {e}")

        return functions

    def _analyze_test_quality(self):
        """Analyse la qualit√© des tests existants."""
        print("  üìä Analyse qualit√© tests...")

        if not self.tests_dir.exists():
            print("    ‚ö† Dossier tests/ non trouv√©")
            return

        test_files = list(self.tests_dir.rglob("test_*.py"))

        for test_file in test_files:
            try:
                with open(test_file, 'r', encoding='utf-8-sig') as f:
                    tree = ast.parse(f.read(), filename=str(test_file))

                # Compter les assertions
                assertions_count = 0
                test_functions_count = 0

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
                        test_functions_count += 1

                    if isinstance(node, ast.Assert):
                        assertions_count += 1

                # Analyser la qualit√©
                avg_assertions = assertions_count / test_functions_count if test_functions_count > 0 else 0

                if test_functions_count > 0 and avg_assertions < 1:
                    self.results["test_quality"].append({
                        "file": str(test_file.relative_to(self.project_root)),
                        "test_count": test_functions_count,
                        "assertions_count": assertions_count,
                        "avg_assertions": avg_assertions,
                        "issue": "Peu d'assertions par test"
                    })

            except Exception as e:
                print(f"    ‚ö† Erreur analyse {test_file.name}: {e}")

        print(f"    ‚úì {len(test_files)} fichiers de tests analys√©s")

    def _generate_summary(self) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© de l'analyse."""
        return {
            "total_tests": self.results["execution_results"].get("total", 0),
            "success_rate": self.results["execution_results"].get("success_rate", 0),
            "coverage": self.results["coverage"].get("total_percent", 0),
            "missing_tests": len(self.results["missing_tests"]),
            "low_coverage_files": len(self.results["coverage"].get("files", []))
        }


def run_test_analysis(project_root: Path) -> Dict[str, Any]:
    """
    Lance l'analyse des tests.

    Args:
        project_root: Racine du projet

    Returns:
        R√©sultats de l'analyse
    """
    analyzer = TestAnalyzer(project_root)
    return analyzer.analyze()
