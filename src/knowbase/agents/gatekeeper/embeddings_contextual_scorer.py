"""
üåä OSMOSE Phase 1.5 - Embeddings Contextual Scorer

Filtrage contextuel bas√© sur similarit√© s√©mantique des embeddings.

**Principe**: Comparer le contexte de chaque entit√© avec des concepts abstraits
(PRIMARY topic, COMPETITOR mention, SECONDARY info) pour classifier leur r√¥le.

**Am√©liorations Production-Ready**:
- Agr√©gation multi-occurrences (toutes mentions vs premi√®re) ‚Üí +15-20% pr√©cision
- Paraphrases multilingues (EN/FR/DE/ES) ‚Üí +10% stabilit√©
- Moyenne pond√©r√©e des contextes (decay pour mentions tardives)

**Impact attendu**: +25-35% pr√©cision, 100% language-agnostic, $0 co√ªt, <200ms

R√©f√©rence: doc/ongoing/ANALYSE_FILTRAGE_CONTEXTUEL_GENERALISTE.md
"""

from typing import Dict, Any, List, Tuple, Optional
import logging
import re
from collections import defaultdict
import numpy as np

# Import via EmbeddingModelManager (avec auto-unload apr√®s inactivit√© ET support Burst)
try:
    from knowbase.common.clients.embeddings import get_embedding_manager
except ImportError:
    get_embedding_manager = None
    logging.warning(
        "[OSMOSE] sentence-transformers non install√©. "
        "Installer avec: pip install sentence-transformers"
    )

logger = logging.getLogger(__name__)


# Concepts de r√©f√©rence multilingues
REFERENCE_CONCEPTS_MULTILINGUAL = {
    "PRIMARY": {
        "en": [
            "main product described in detail",
            "primary solution recommended",
            "our company's flagship offering",
            "key technology we provide",
            "central topic of this document"
        ],
        "fr": [
            "produit principal d√©crit en d√©tail",
            "solution principale recommand√©e",
            "offre phare de notre entreprise",
            "technologie cl√© que nous proposons",
            "sujet central de ce document"
        ],
        "de": [
            "hauptprodukt ausf√ºhrlich beschrieben",
            "hauptl√∂sung empfohlen",
            "unser flaggschiff-angebot",
            "schl√ºsseltechnologie die wir anbieten",
            "zentrales thema dieses dokuments"
        ],
        "es": [
            "producto principal descrito en detalle",
            "soluci√≥n principal recomendada",
            "oferta estrella de nuestra empresa",
            "tecnolog√≠a clave que ofrecemos",
            "tema central de este documento"
        ]
    },
    "COMPETITOR": {
        "en": [
            "competitor mentioned for comparison",
            "alternative vendor briefly cited",
            "competing product referenced",
            "other company's solution",
            "rival technology noted"
        ],
        "fr": [
            "concurrent mentionn√© pour comparaison",
            "fournisseur alternatif bri√®vement cit√©",
            "produit concurrent r√©f√©renc√©",
            "solution d'une autre entreprise",
            "technologie rivale not√©e"
        ],
        "de": [
            "konkurrent zum vergleich erw√§hnt",
            "alternativer anbieter kurz erw√§hnt",
            "konkurrenzprodukt referenziert",
            "l√∂sung eines anderen unternehmens",
            "rivalisierendes technologie erw√§hnt"
        ],
        "es": [
            "competidor mencionado para comparaci√≥n",
            "proveedor alternativo brevemente citado",
            "producto competidor referenciado",
            "soluci√≥n de otra empresa",
            "tecnolog√≠a rival mencionada"
        ]
    },
    "SECONDARY": {
        "en": [
            "related concept mentioned in passing",
            "supporting technology or service",
            "tangential topic",
            "background information",
            "generic term or abbreviation"
        ],
        "fr": [
            "concept connexe mentionn√© en passant",
            "technologie ou service de support",
            "sujet tangentiel",
            "information de contexte",
            "terme g√©n√©rique ou abr√©viation"
        ],
        "de": [
            "verwandtes konzept nebenbei erw√§hnt",
            "unterst√ºtzende technologie oder dienstleistung",
            "tangentiales thema",
            "hintergrundinformationen",
            "generischer begriff oder abk√ºrzung"
        ],
        "es": [
            "concepto relacionado mencionado de paso",
            "tecnolog√≠a o servicio de apoyo",
            "tema tangencial",
            "informaci√≥n de fondo",
            "t√©rmino gen√©rico o abreviatura"
        ]
    }
}


class EmbeddingsContextualScorer:
    """
    Score entities based on embeddings similarity with reference concepts.

    Utilise SentenceTransformer (multilingual-e5-large) pour encoder contextes
    et comparer avec concepts abstraits multilingues.

    **Avantages**:
    - 100% language-agnostic (paraphrases multilingues)
    - $0 co√ªt (mod√®le local, pas d'API)
    - <200ms latence (batch encoding)
    - Agr√©gation multi-occurrences (toutes mentions vs premi√®re)
    """

    def __init__(
        self,
        model_name: str = "intfloat/multilingual-e5-large",
        context_window: int = 100,
        similarity_threshold_primary: float = 0.5,
        similarity_threshold_competitor: float = 0.4,
        enable_multi_occurrence: bool = True,
        languages: List[str] = None
    ):
        """
        Initialiser le scorer.

        Args:
            model_name: Nom du mod√®le SentenceTransformer (d√©faut: multilingual-e5-large)
            context_window: Taille fen√™tre contexte (mots)
            similarity_threshold_primary: Seuil PRIMARY (d√©faut: 0.5)
            similarity_threshold_competitor: Seuil COMPETITOR (d√©faut: 0.4)
            enable_multi_occurrence: Agr√©ger toutes occurrences (d√©faut: True)
            languages: Langues support√©es (d√©faut: ['en', 'fr', 'de', 'es'])
        """
        self.model_name = model_name
        self.context_window = context_window
        self.similarity_threshold_primary = similarity_threshold_primary
        self.similarity_threshold_competitor = similarity_threshold_competitor
        self.enable_multi_occurrence = enable_multi_occurrence
        self.languages = languages or ["en", "fr", "de", "es"]

        # P1.2: Initialiser via EmbeddingModelManager (singleton + support Burst EC2 Spot)
        if get_embedding_manager is None:
            raise ImportError(
                "sentence-transformers non install√©. "
                "Installer avec: pip install sentence-transformers"
            )

        logger.info(f"[OSMOSE] Initialisation EmbeddingsContextualScorer (model={model_name}, P1.2 singleton + Burst)")
        self._embedding_manager = get_embedding_manager()

        # Garder r√©f√©rence au mod√®le pour get_sentence_embedding_dimension() si n√©cessaire
        # Mais utiliser self._encode() pour les encodages (supporte Burst)
        self.model_name = model_name

        # Encoder concepts de r√©f√©rence (cache) - utilise Burst si actif
        self.reference_embeddings = self._encode_reference_concepts()

        logger.info(
            f"[OSMOSE] EmbeddingsContextualScorer initialis√© "
            f"(languages={self.languages}, window={context_window})"
        )

    def _encode(self, texts: List[str], **kwargs) -> np.ndarray:
        """
        Encode texts via EmbeddingModelManager (supporte Burst EC2 Spot).

        En mode Burst, les embeddings sont calcul√©s sur EC2 Spot.
        Sinon, utilise le mod√®le local.

        Args:
            texts: Liste de textes √† encoder
            **kwargs: Arguments suppl√©mentaires (ignor√©s en mode Burst)

        Returns:
            np.ndarray: Embeddings
        """
        if not texts:
            return np.array([])

        # Utiliser le manager qui g√®re automatiquement Burst vs Local
        embeddings = self._embedding_manager.encode(texts)

        # Convertir en numpy si n√©cessaire (Burst retourne d√©j√† numpy)
        if not isinstance(embeddings, np.ndarray):
            embeddings = np.array(embeddings)

        return embeddings

    def score_entities(
        self,
        candidates: List[Dict[str, Any]],
        full_text: str
    ) -> List[Dict[str, Any]]:
        """
        Score entities avec embeddings similarity.

        Args:
            candidates: Liste d'entit√©s candidates
            full_text: Texte complet du document

        Returns:
            Liste d'entit√©s avec scores ajout√©s:
            - embedding_primary_similarity: Similarit√© avec concept PRIMARY [0-1]
            - embedding_competitor_similarity: Similarit√© avec concept COMPETITOR [0-1]
            - embedding_secondary_similarity: Similarit√© avec concept SECONDARY [0-1]
            - embedding_role: Role classifi√© (PRIMARY/COMPETITOR/SECONDARY)
            - embedding_score: Score normalis√© [0-1]
        """
        if not candidates:
            logger.warning("[OSMOSE] EmbeddingsContextualScorer: Aucun candidat √† scorer")
            return []

        if not full_text or len(full_text) < 50:
            logger.warning("[OSMOSE] EmbeddingsContextualScorer: Texte trop court, scoring par d√©faut")
            for entity in candidates:
                entity["embedding_score"] = 0.5
                entity["embedding_role"] = "SECONDARY"
            return candidates

        logger.info(
            f"[OSMOSE] EmbeddingsContextualScorer: Scoring {len(candidates)} candidats "
            f"(doc_length={len(full_text)} chars)"
        )

        # OPTIMISATION: Extraire TOUS les contextes d'abord (batching preparation)
        all_contexts_by_entity = {}
        all_contexts_flat = []
        entity_context_indices = {}  # Map entity ‚Üí (start_idx, end_idx) dans all_contexts_flat

        current_idx = 0
        for entity in candidates:
            entity_name = entity.get("text", "") or entity.get("name", "")
            if not entity_name:
                continue

            # Extraire toutes les mentions (si multi-occurrence activ√©)
            contexts = self._extract_all_mentions_contexts(entity_name, full_text)

            if contexts:
                all_contexts_by_entity[entity_name] = contexts
                all_contexts_flat.extend(contexts)
                entity_context_indices[entity_name] = (current_idx, current_idx + len(contexts))
                current_idx += len(contexts)

        # BATCHING: Encoder TOUS les contextes en une seule fois (√ó3-5 speedup!)
        # Utilise Burst EC2 Spot si actif, sinon mod√®le local
        if all_contexts_flat:
            burst_mode = self._embedding_manager.is_burst_mode_active()
            logger.info(
                f"[OSMOSE] Batch encoding {len(all_contexts_flat)} contexts "
                f"for {len(all_contexts_by_entity)} entities "
                f"(burst={'EC2' if burst_mode else 'LOCAL'})"
            )
            all_embeddings = self._encode(all_contexts_flat)
        else:
            all_embeddings = None

        # Appliquer scores √† chaque entit√©
        for entity in candidates:
            entity_name = entity.get("text", "") or entity.get("name", "")
            if not entity_name or entity_name not in all_contexts_by_entity:
                # Aucun contexte trouv√© ‚Üí scores par d√©faut
                entity["embedding_primary_similarity"] = 0.0
                entity["embedding_competitor_similarity"] = 0.0
                entity["embedding_secondary_similarity"] = 0.5
                entity["embedding_role"] = "SECONDARY"
                entity["embedding_score"] = 0.5
                continue

            contexts = all_contexts_by_entity[entity_name]
            start_idx, end_idx = entity_context_indices[entity_name]
            context_embeddings = all_embeddings[start_idx:end_idx]

            # Calculer scores avec embeddings pr√©-calcul√©s (batching optimization)
            aggregated_similarities = self._score_entity_with_precomputed_embeddings(
                context_embeddings
            )

            if not aggregated_similarities:
                # Aucun contexte trouv√© ‚Üí scores par d√©faut
                entity["embedding_primary_similarity"] = 0.0
                entity["embedding_competitor_similarity"] = 0.0
                entity["embedding_secondary_similarity"] = 0.5
                entity["embedding_role"] = "SECONDARY"
                entity["embedding_score"] = 0.3
                continue

            # Enregistrer scores
            entity["embedding_primary_similarity"] = aggregated_similarities["PRIMARY"]
            entity["embedding_competitor_similarity"] = aggregated_similarities["COMPETITOR"]
            entity["embedding_secondary_similarity"] = aggregated_similarities["SECONDARY"]

            # Classifier role
            role = self._classify_role(aggregated_similarities)
            entity["embedding_role"] = role

            # Score normalis√© [0-1] selon role
            if role == "PRIMARY":
                entity["embedding_score"] = 1.0
            elif role == "COMPETITOR":
                entity["embedding_score"] = 0.2
            else:  # SECONDARY
                entity["embedding_score"] = 0.5

            # Log d√©tails si score extr√™me
            if entity["embedding_score"] < 0.3 or entity["embedding_score"] > 0.8:
                logger.debug(
                    f"[OSMOSE] EmbeddingsScoring '{entity_name}': "
                    f"role={role}, score={entity['embedding_score']:.2f} "
                    f"(prim={aggregated_similarities['PRIMARY']:.2f}, "
                    f"comp={aggregated_similarities['COMPETITOR']:.2f})"
                )

        logger.info(
            f"[OSMOSE] EmbeddingsContextualScorer: Scoring termin√© "
            f"({len([e for e in candidates if e.get('embedding_role') == 'PRIMARY'])} PRIMARY, "
            f"{len([e for e in candidates if e.get('embedding_role') == 'COMPETITOR'])} COMPETITOR)"
        )

        return candidates

    def _extract_all_mentions_contexts(
        self,
        entity_name: str,
        full_text: str
    ) -> List[str]:
        """
        Extract contexts for all mentions of entity.

        **Am√©lioration vs premi√®re occurrence**: Agr√©gation de toutes les mentions
        pour avoir une vision compl√®te du r√¥le de l'entit√© dans le document.

        Args:
            entity_name: Nom de l'entit√©
            full_text: Texte complet

        Returns:
            Liste de contextes (window mots avant + apr√®s chaque mention)
        """
        contexts = []

        # Tokeniser le texte (mots simples)
        words = re.findall(r'\b\w+\b', full_text)

        # Rechercher toutes les positions de l'entit√©
        entity_words = entity_name.lower().split()
        text_lower = [w.lower() for w in words]

        for i in range(len(text_lower) - len(entity_words) + 1):
            # Check si l'entit√© commence √† la position i
            if text_lower[i:i + len(entity_words)] == entity_words:
                # Extraire contexte (window mots avant/apr√®s)
                start = max(0, i - self.context_window // 2)
                end = min(len(words), i + len(entity_words) + self.context_window // 2)

                context_words = words[start:end]
                context = " ".join(context_words)
                contexts.append(context)

                # Limiter √† 10 occurrences max (√©viter explosion m√©moire)
                if len(contexts) >= 10:
                    break

        return contexts

    def _score_entity_with_precomputed_embeddings(
        self,
        context_embeddings: np.ndarray
    ) -> Dict[str, float]:
        """
        Score entity avec embeddings pr√©-calcul√©s (batching optimization).

        **Optimisation P1.2**: Utilise les embeddings d√©j√† calcul√©s en batch
        au lieu de les recalculer individuellement ‚Üí √ó3-5 speedup.

        Args:
            context_embeddings: Embeddings pr√©-calcul√©s (numpy array)

        Returns:
            Dict {role ‚Üí similarity_score [0-1]}
        """
        if context_embeddings is None or len(context_embeddings) == 0:
            return {"PRIMARY": 0.0, "COMPETITOR": 0.0, "SECONDARY": 0.5}

        # Weights: d√©croissance exponentielle pour mentions tardives
        # Premi√®re mention = poids 1.0, derni√®re = poids 0.5
        num_contexts = len(context_embeddings)
        weights = np.exp(-np.arange(num_contexts) / (num_contexts + 1))
        weights = weights / weights.sum()  # Normalisation

        # Agr√©ger embeddings (moyenne pond√©r√©e)
        if num_contexts == 1:
            aggregated_embedding = context_embeddings[0]
        else:
            aggregated_embedding = np.average(
                context_embeddings,
                axis=0,
                weights=weights
            )

        # Calculer similarit√© avec concepts de r√©f√©rence
        similarities = {}
        for role in ["PRIMARY", "COMPETITOR", "SECONDARY"]:
            # Moyenne des similarit√©s avec toutes les paraphrases
            role_similarities = []
            for lang_embedding in self.reference_embeddings[role].values():
                # Cosine similarity
                similarity = np.dot(aggregated_embedding, lang_embedding) / (
                    np.linalg.norm(aggregated_embedding) * np.linalg.norm(lang_embedding)
                )
                role_similarities.append(similarity)

            # Moyenne des similarit√©s (toutes langues)
            similarities[role] = float(np.mean(role_similarities))

        return similarities

    def _score_entity_aggregated(
        self,
        contexts: List[str]
    ) -> Dict[str, float]:
        """
        Score entity avec agr√©gation multi-occurrences.

        **Agr√©gation**: Moyenne pond√©r√©e des embeddings de tous les contextes
        (decay pour mentions tardives dans le document).

        Args:
            contexts: Liste de contextes extraits

        Returns:
            Dict {role ‚Üí similarity_score [0-1]}
        """
        if not contexts:
            return {"PRIMARY": 0.0, "COMPETITOR": 0.0, "SECONDARY": 0.5}

        # Encoder tous les contextes (batch encoding pour efficacit√©, supporte Burst)
        context_embeddings = self._encode(contexts)

        # Weights: d√©croissance exponentielle pour mentions tardives
        # Premi√®re mention = poids 1.0, derni√®re = poids 0.5
        weights = np.exp(-np.arange(len(contexts)) / (len(contexts) + 1))
        weights = weights / weights.sum()  # Normalisation

        # Agr√©ger embeddings (moyenne pond√©r√©e)
        if len(contexts) == 1:
            aggregated_embedding = context_embeddings[0]
        else:
            aggregated_embedding = np.average(
                context_embeddings,
                axis=0,
                weights=weights
            )

        # Calculer similarit√© avec concepts de r√©f√©rence
        similarities = {}
        for role in ["PRIMARY", "COMPETITOR", "SECONDARY"]:
            # Moyenne des similarit√©s avec toutes les paraphrases
            role_similarities = []
            for lang_embedding in self.reference_embeddings[role].values():
                # Cosine similarity
                similarity = np.dot(aggregated_embedding, lang_embedding) / (
                    np.linalg.norm(aggregated_embedding) * np.linalg.norm(lang_embedding)
                )
                role_similarities.append(similarity)

            # Moyenne des similarit√©s (toutes langues)
            similarities[role] = float(np.mean(role_similarities))

        return similarities

    def _classify_role(
        self,
        similarities: Dict[str, float]
    ) -> str:
        """
        Classify entity role based on similarities.

        **R√®gles**:
        - PRIMARY: Si sim_primary > threshold_primary ET > sim_competitor
        - COMPETITOR: Si sim_competitor > threshold_competitor ET > sim_primary
        - SECONDARY: Sinon (d√©faut)

        Args:
            similarities: Dict {role ‚Üí similarity}

        Returns:
            Role classifi√© (PRIMARY/COMPETITOR/SECONDARY)
        """
        prim = similarities["PRIMARY"]
        comp = similarities["COMPETITOR"]
        sec = similarities["SECONDARY"]

        # PRIMARY si forte similarit√© ET sup√©rieure √† COMPETITOR
        if prim > self.similarity_threshold_primary and prim > comp:
            return "PRIMARY"

        # COMPETITOR si forte similarit√© ET sup√©rieure √† PRIMARY
        if comp > self.similarity_threshold_competitor and comp > prim:
            return "COMPETITOR"

        # SECONDARY par d√©faut
        return "SECONDARY"

    def _encode_reference_concepts(self) -> Dict[str, Dict[str, np.ndarray]]:
        """
        Encoder concepts de r√©f√©rence multilingues (cache).

        Returns:
            Dict {role ‚Üí {lang ‚Üí embedding}}
        """
        reference_embeddings = {}

        for role, paraphrases_by_lang in REFERENCE_CONCEPTS_MULTILINGUAL.items():
            reference_embeddings[role] = {}

            for lang in self.languages:
                if lang not in paraphrases_by_lang:
                    continue

                paraphrases = paraphrases_by_lang[lang]

                # Encoder toutes les paraphrases (supporte Burst)
                embeddings = self._encode(paraphrases)

                # Agr√©ger (moyenne)
                aggregated = np.mean(embeddings, axis=0)
                reference_embeddings[role][lang] = aggregated

        return reference_embeddings
