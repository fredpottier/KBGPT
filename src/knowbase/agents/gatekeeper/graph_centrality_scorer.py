"""
üåä OSMOSE Phase 1.5 - Graph Centrality Scorer

Filtrage contextuel bas√© sur l'analyse de graphe de co-occurrence.

**Principe**: Les entit√©s importantes sont structurellement centrales dans le document
- TF-IDF weighting (pas juste fr√©quence brute)
- Salience score (position + boost titre/abstract)
- Fen√™tre adaptive (30-100 mots selon taille doc)
- Centrality metrics: PageRank, Degree, Betweenness

**Impact attendu**: +20-30% pr√©cision, 100% language-agnostic, $0 co√ªt, <100ms

R√©f√©rence: doc/ongoing/ANALYSE_FILTRAGE_CONTEXTUEL_GENERALISTE.md
"""

from typing import Dict, Any, List, Tuple, Set
import logging
import re
import math
from collections import defaultdict, Counter
import networkx as nx

logger = logging.getLogger(__name__)


class GraphCentralityScorer:
    """
    Score entities based on graph centrality metrics.

    Utilise la structure du document (co-occurrences) plut√¥t que le contenu
    s√©mantique pour identifier les entit√©s importantes.

    **Avantages**:
    - 100% language-agnostic (structure vs s√©mantique)
    - $0 co√ªt (pas d'appel API)
    - <100ms latence
    - Robuste aux erreurs NER (structure pr√©serv√©e)
    """

    def __init__(
        self,
        min_centrality: float = 0.15,
        centrality_weights: Dict[str, float] = None,
        enable_tf_idf: bool = True,
        enable_salience: bool = True
    ):
        """
        Initialiser le scorer.

        Args:
            min_centrality: Seuil minimum de centralit√© (d√©faut: 0.15)
            centrality_weights: Poids pour PageRank/Degree/Betweenness
            enable_tf_idf: Activer TF-IDF weighting
            enable_salience: Activer salience scoring (position)
        """
        self.min_centrality = min_centrality
        self.centrality_weights = centrality_weights or {
            "pagerank": 0.5,
            "degree": 0.3,
            "betweenness": 0.2
        }
        self.enable_tf_idf = enable_tf_idf
        self.enable_salience = enable_salience

        logger.info(
            f"[OSMOSE] GraphCentralityScorer initialis√© "
            f"(min_centrality={min_centrality}, "
            f"tf_idf={enable_tf_idf}, salience={enable_salience})"
        )

    def score_entities(
        self,
        candidates: List[Dict[str, Any]],
        full_text: str
    ) -> List[Dict[str, Any]]:
        """
        Score entities avec m√©triques de centralit√©.

        Args:
            candidates: Liste d'entit√©s candidates
            full_text: Texte complet du document

        Returns:
            Liste d'entit√©s avec scores ajout√©s:
            - tf_idf_score: Score TF-IDF normalis√© [0-1]
            - centrality_score: Score combin√© PageRank+Degree+Betweenness [0-1]
            - salience_score: Score position/importance [0-1]
            - graph_score: Score final combin√© [0-1]
        """
        if not candidates:
            logger.warning("[OSMOSE] GraphCentralityScorer: Aucun candidat √† scorer")
            return []

        if not full_text or len(full_text) < 50:
            logger.warning("[OSMOSE] GraphCentralityScorer: Texte trop court, scoring par d√©faut")
            for entity in candidates:
                entity["graph_score"] = 0.5  # Score neutre
            return candidates

        logger.info(
            f"[OSMOSE] GraphCentralityScorer: Scoring {len(candidates)} candidats "
            f"(doc_length={len(full_text)} chars)"
        )

        # 1. Build co-occurrence graph
        graph = self._build_cooccurrence_graph(candidates, full_text)

        # 2. Calculate TF-IDF weights (optionnel)
        tf_idf_scores = {}
        if self.enable_tf_idf:
            tf_idf_scores = self._calculate_tf_idf(candidates, full_text)

        # 3. Calculate centrality scores
        centrality_scores = self._calculate_centrality(graph)

        # 4. Calculate salience scores (optionnel)
        salience_scores = {}
        if self.enable_salience:
            salience_scores = self._calculate_salience(candidates, full_text)

        # 5. Combine scores
        for entity in candidates:
            entity_name = entity.get("text", "") or entity.get("name", "")
            if not entity_name:
                continue

            # Normalisation du nom (case-insensitive)
            entity_key = entity_name.lower()

            # Scores individuels
            tf_idf = tf_idf_scores.get(entity_key, 0.5)  # D√©faut neutre
            centrality = centrality_scores.get(entity_key, 0.0)
            salience = salience_scores.get(entity_key, 0.5)  # D√©faut neutre

            # Enregistrer scores individuels
            entity["tf_idf_score"] = tf_idf
            entity["centrality_score"] = centrality
            entity["salience_score"] = salience

            # Score combin√© (pond√©ration configurable)
            weights = [0.4, 0.4, 0.2]  # TF-IDF, Centrality, Salience
            scores = [tf_idf, centrality, salience]

            # Ajuster poids si composants d√©sactiv√©s
            if not self.enable_tf_idf:
                weights[0] = 0.0
                weights[1] = 0.6  # Plus de poids sur centrality
            if not self.enable_salience:
                weights[2] = 0.0
                weights[0] += 0.1
                weights[1] += 0.1

            entity["graph_score"] = sum(w * s for w, s in zip(weights, scores))

            # Log d√©tails si score tr√®s bas ou tr√®s haut
            if entity["graph_score"] < 0.2 or entity["graph_score"] > 0.8:
                logger.debug(
                    f"[OSMOSE] GraphScoring '{entity_name}': "
                    f"graph={entity['graph_score']:.2f} "
                    f"(tfidf={tf_idf:.2f}, cent={centrality:.2f}, sal={salience:.2f})"
                )

        logger.info(
            f"[OSMOSE] GraphCentralityScorer: Scoring termin√© "
            f"({len([e for e in candidates if e.get('graph_score', 0) >= self.min_centrality])} "
            f"entities >= {self.min_centrality})"
        )

        return candidates

    def _build_cooccurrence_graph(
        self,
        candidates: List[Dict[str, Any]],
        full_text: str
    ) -> nx.Graph:
        """
        Build co-occurrence graph avec fen√™tre adaptive.

        **Am√©lioration vs basique**:
        - Fen√™tre adaptive (30-100 mots selon taille doc)
        - TF-IDF weighting des edges (pas juste comptage)
        - Normalisation case-insensitive

        Args:
            candidates: Liste d'entit√©s
            full_text: Texte complet

        Returns:
            Graph NetworkX avec poids TF-IDF sur edges
        """
        graph = nx.Graph()

        # D√©terminer fen√™tre adaptive
        window_size = self._get_adaptive_window_size(len(full_text))

        # Extraire tous les noms d'entit√©s (normalis√©s)
        entity_names = [
            (e.get("text", "") or e.get("name", "")).lower()
            for e in candidates
            if (e.get("text") or e.get("name"))
        ]

        if not entity_names:
            return graph

        # Ajouter tous les n≈ìuds
        for name in entity_names:
            graph.add_node(name)

        # Tokeniser le texte (mots simples)
        words = re.findall(r'\b\w+\b', full_text.lower())

        # Sliding window pour d√©tecter co-occurrences
        cooccurrences = defaultdict(int)

        for i in range(len(words) - window_size + 1):
            window = words[i:i + window_size]

            # Trouver entit√©s dans cette fen√™tre
            entities_in_window = []
            for entity_name in entity_names:
                # Check si l'entit√© est dans la fen√™tre (approximatif)
                entity_words = entity_name.split()
                if all(word in window for word in entity_words):
                    entities_in_window.append(entity_name)

            # Ajouter co-occurrences (paires uniques)
            if len(entities_in_window) >= 2:
                for j, e1 in enumerate(entities_in_window):
                    for e2 in entities_in_window[j + 1:]:
                        pair = tuple(sorted([e1, e2]))
                        cooccurrences[pair] += 1

        # Ajouter edges avec poids
        for (e1, e2), count in cooccurrences.items():
            # Poids: log(count) pour √©viter domination des paires fr√©quentes
            weight = math.log(count + 1)
            graph.add_edge(e1, e2, weight=weight)

        logger.debug(
            f"[OSMOSE] Graph construit: {graph.number_of_nodes()} n≈ìuds, "
            f"{graph.number_of_edges()} edges (window={window_size})"
        )

        return graph

    def _calculate_tf_idf(
        self,
        candidates: List[Dict[str, Any]],
        full_text: str
    ) -> Dict[str, float]:
        """
        Calculate TF-IDF scores for entities.

        **Am√©lioration vs fr√©quence brute**: TF-IDF p√©nalise les termes
        trop fr√©quents (stopwords-like) et favorise les termes sp√©cifiques.

        Args:
            candidates: Liste d'entit√©s
            full_text: Texte complet

        Returns:
            Dict {entity_name ‚Üí tf_idf_score [0-1]}
        """
        scores = {}

        # Tokeniser le texte
        words = re.findall(r'\b\w+\b', full_text.lower())
        total_words = len(words)

        if total_words == 0:
            return scores

        # Calculer fr√©quence de chaque mot (pour IDF)
        word_counts = Counter(words)

        # Calculer TF-IDF pour chaque entit√©
        for entity in candidates:
            entity_name = (entity.get("text", "") or entity.get("name", "")).lower()
            if not entity_name:
                continue

            # TF: Fr√©quence du terme dans le document
            entity_words = entity_name.split()
            term_frequency = sum(word_counts.get(word, 0) for word in entity_words)
            tf = term_frequency / total_words if total_words > 0 else 0.0

            # IDF: log(N / df) o√π df = nombre de docs contenant le terme
            # Approximation: IDF bas√© sur fr√©quence relative (single doc)
            # Termes rares (faible fr√©quence) ‚Üí IDF √©lev√©
            # Termes fr√©quents ‚Üí IDF faible
            max_word_freq = max(word_counts.values())
            avg_word_freq = sum(word_counts.get(w, 0) for w in entity_words) / len(entity_words)

            # IDF inversement proportionnel √† la fr√©quence relative
            idf = math.log(max_word_freq / (avg_word_freq + 1) + 1)

            # TF-IDF
            tf_idf = tf * idf

            # Normalisation [0-1] (approximatif)
            # Max th√©orique: TF=0.1 (10% du doc), IDF=log(1000)=6.9 ‚Üí tf_idf=0.69
            normalized_score = min(tf_idf / 0.5, 1.0)  # Seuil arbitraire pour normalisation

            scores[entity_name] = normalized_score

        return scores

    def _calculate_centrality(
        self,
        graph: nx.Graph
    ) -> Dict[str, float]:
        """
        Calculate centrality scores (PageRank, Degree, Betweenness).

        **Combinaison de 3 m√©triques**:
        - PageRank: Importance globale (liens entrants)
        - Degree: Nombre de connexions directes
        - Betweenness: Position de "pont" entre clusters

        Args:
            graph: Graph NetworkX

        Returns:
            Dict {entity_name ‚Üí centrality_score [0-1]}
        """
        scores = {}

        if graph.number_of_nodes() == 0:
            return scores

        # √âviter calculs sur graphes vides ou trop petits
        if graph.number_of_edges() == 0:
            # Graphe sans edges: tous les n≈ìuds ont score 0
            for node in graph.nodes():
                scores[node] = 0.0
            return scores

        # 1. PageRank (importance globale)
        try:
            pagerank = nx.pagerank(graph, weight="weight", max_iter=100)
        except Exception as e:
            logger.warning(f"[OSMOSE] PageRank √©chou√©: {e}, utilisation degree centrality")
            pagerank = {node: 0.0 for node in graph.nodes()}

        # 2. Degree Centrality (nombre de connexions)
        degree_centrality = nx.degree_centrality(graph)

        # 3. Betweenness Centrality (position de pont)
        try:
            betweenness = nx.betweenness_centrality(graph, weight="weight")
        except Exception as e:
            logger.warning(f"[OSMOSE] Betweenness √©chou√©: {e}, utilisation 0.0")
            betweenness = {node: 0.0 for node in graph.nodes()}

        # Normalisation de PageRank [0-1]
        max_pagerank = max(pagerank.values()) if pagerank else 1.0
        pagerank_norm = {
            node: score / max_pagerank if max_pagerank > 0 else 0.0
            for node, score in pagerank.items()
        }

        # Betweenness d√©j√† normalis√© [0-1]

        # 4. Combinaison pond√©r√©e
        for node in graph.nodes():
            pr = pagerank_norm.get(node, 0.0)
            dc = degree_centrality.get(node, 0.0)
            bc = betweenness.get(node, 0.0)

            # Poids configurables
            combined = (
                self.centrality_weights["pagerank"] * pr +
                self.centrality_weights["degree"] * dc +
                self.centrality_weights["betweenness"] * bc
            )

            scores[node] = combined

        return scores

    def _calculate_salience(
        self,
        candidates: List[Dict[str, Any]],
        full_text: str
    ) -> Dict[str, float]:
        """
        Calculate salience scores (position + titre/abstract boost).

        **Heuristique**: Entit√©s mentionn√©es t√¥t (titre, intro) ou
        fr√©quemment = plus importantes.

        Args:
            candidates: Liste d'entit√©s
            full_text: Texte complet

        Returns:
            Dict {entity_name ‚Üí salience_score [0-1]}
        """
        scores = {}

        text_length = len(full_text)
        if text_length == 0:
            return scores

        # Zones sp√©ciales (approximatif)
        # Titre/Abstract: premiers 10% du texte
        title_zone_end = int(text_length * 0.1)

        for entity in candidates:
            entity_name = (entity.get("text", "") or entity.get("name", "")).lower()
            if not entity_name:
                continue

            # Trouver toutes les positions de l'entit√©
            positions = []
            text_lower = full_text.lower()
            start = 0
            while True:
                pos = text_lower.find(entity_name, start)
                if pos == -1:
                    break
                positions.append(pos)
                start = pos + 1

            if not positions:
                scores[entity_name] = 0.0
                continue

            # Score bas√© sur premi√®re position (early mention bonus)
            first_position = positions[0]
            position_ratio = first_position / text_length

            # Score position: 1.0 (d√©but) ‚Üí 0.3 (fin) - d√©croissance lin√©aire
            position_score = max(1.0 - position_ratio, 0.3)

            # Bonus si dans zone titre/abstract (+0.2)
            title_bonus = 0.2 if first_position < title_zone_end else 0.0

            # Bonus fr√©quence: log(count) normalis√©
            frequency_score = min(math.log(len(positions) + 1) / math.log(10), 0.3)

            # Score final
            salience = min(position_score + title_bonus + frequency_score, 1.0)

            scores[entity_name] = salience

        return scores

    def _get_adaptive_window_size(self, text_length: int) -> int:
        """
        Get adaptive window size based on document length.

        **Rationale**: Documents longs n√©cessitent fen√™tres plus grandes
        pour capturer co-occurrences significatives.

        Args:
            text_length: Longueur du texte (caract√®res)

        Returns:
            Taille de fen√™tre (nombre de mots)
        """
        # Conversion approximative: 5 caract√®res/mot
        approx_words = text_length / 5

        if approx_words < 200:  # < 1000 chars
            return 30
        elif approx_words < 1000:  # < 5000 chars
            return 50
        elif approx_words < 4000:  # < 20000 chars
            return 75
        else:
            return 100
