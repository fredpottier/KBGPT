"""
Gestionnaire de mod√®les d'embedding avec d√©chargement automatique GPU.

Phase 2.x: Ajout du m√©canisme de timeout pour lib√©rer la m√©moire GPU
apr√®s une p√©riode d'inactivit√© (utile en d√©veloppement).

Mode Burst: Support pour basculer vers un service embeddings distant (EC2 Spot).
"""

from __future__ import annotations

import gc
import logging
import threading
import time
from typing import Optional, List, Dict, Any

import numpy as np
import requests

from knowbase.config.settings import get_settings

logger = logging.getLogger(__name__)

# Configuration du timeout (en secondes)
# Peut √™tre surcharg√© via variable d'environnement GPU_UNLOAD_TIMEOUT_MINUTES
DEFAULT_UNLOAD_TIMEOUT_MINUTES = 10


class EmbeddingModelManager:
    """
    Gestionnaire singleton pour le mod√®le d'embedding avec d√©chargement automatique.

    Fonctionnalit√©s:
    - Chargement lazy du mod√®le
    - D√©chargement automatique apr√®s X minutes d'inactivit√©
    - M√©thode manuelle pour forcer le d√©chargement
    - Thread-safe
    """

    _instance: Optional["EmbeddingModelManager"] = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        self._model = None
        self._model_name: Optional[str] = None
        self._device: Optional[str] = None
        self._last_access_time: float = 0
        self._model_lock = threading.Lock()
        self._monitor_thread: Optional[threading.Thread] = None
        self._stop_monitor = threading.Event()

        # === Mode Burst ===
        self._burst_mode = False
        self._burst_endpoint: Optional[str] = None
        self._burst_timeout: int = 120  # Timeout HTTP en secondes (augment√© pour gros batches)

        # Timeout configurable (d√©faut: 10 minutes)
        settings = get_settings()
        timeout_env = getattr(settings, 'gpu_unload_timeout_minutes', None)
        self._timeout_seconds = (timeout_env or DEFAULT_UNLOAD_TIMEOUT_MINUTES) * 60

        self._initialized = True
        logger.info(
            f"[EMBEDDINGS] Manager initialized "
            f"(auto-unload after {self._timeout_seconds // 60} min inactivity)"
        )

    def get_model(
        self,
        model_name: Optional[str] = None,
        device: Optional[str] = None,
        cache_folder: Optional[str] = None,
    ):
        """
        R√©cup√®re le mod√®le d'embedding, le charge si n√©cessaire.

        Args:
            model_name: Nom du mod√®le (d√©faut: settings.embeddings_model)
            device: Device cible (cuda, cpu, etc.)
            cache_folder: Dossier de cache pour les mod√®les

        Returns:
            SentenceTransformer model
        """
        from sentence_transformers import SentenceTransformer

        settings = get_settings()
        name = model_name or settings.embeddings_model

        with self._model_lock:
            # Mettre √† jour le temps d'acc√®s
            self._last_access_time = time.time()

            # Si le mod√®le est d√©j√† charg√© avec les m√™mes param√®tres
            if self._model is not None and self._model_name == name:
                return self._model

            # D√©charger l'ancien mod√®le si diff√©rent
            if self._model is not None and self._model_name != name:
                self._unload_model_internal()

            # Charger le nouveau mod√®le
            logger.info(f"[EMBEDDINGS] Loading model: {name}")
            start_time = time.time()

            kwargs: dict[str, object] = {}
            if device is not None:
                kwargs["device"] = device
            if cache_folder is not None:
                kwargs["cache_folder"] = cache_folder

            self._model = SentenceTransformer(name, **kwargs)
            self._model_name = name
            self._device = device

            load_time = time.time() - start_time
            logger.info(f"[EMBEDDINGS] Model loaded in {load_time:.1f}s")

            # D√©marrer le thread de surveillance si pas d√©j√† actif
            self._start_monitor()

            return self._model

    def encode(
        self,
        sentences: List[str],
        **kwargs
    ) -> np.ndarray:
        """
        Encode des phrases avec le mod√®le d'embedding.

        En mode Burst, utilise le service distant EC2 Spot.
        Sinon, utilise le mod√®le local.

        Args:
            sentences: Liste de phrases √† encoder
            **kwargs: Arguments pass√©s √† model.encode() (ignor√©s en mode burst)

        Returns:
            Embeddings numpy array
        """
        # === Mode Burst : utiliser le service distant ===
        if self._burst_mode and self._burst_endpoint:
            return self._encode_remote(sentences)

        # === Mode Normal : utiliser le mod√®le local ===
        model = self.get_model()

        with self._model_lock:
            self._last_access_time = time.time()

        return model.encode(sentences, **kwargs)

    def _encode_remote(
        self,
        sentences: List[str],
        max_batch_size: int = None,  # Auto-d√©tect√© selon config Burst
        max_batch_chars: int = None,  # Auto-d√©tect√© selon config Burst
        max_text_chars: int = 1500,
        max_concurrent: int = None,  # Auto-d√©tect√© selon config Burst
        max_retries: int = 3
    ) -> np.ndarray:
        """
        Encode les textes via le service embeddings distant (EC2 Spot).

        Utilise l'API Text Embeddings Inference (TEI) de HuggingFace.
        Batching adaptatif + requ√™tes parall√®les pour performance optimale.

        Inclut un circuit breaker pour arr√™ter en cas d'√©checs cons√©cutifs.

        Args:
            sentences: Liste de textes √† encoder
            max_batch_size: Nombre max de textes par batch (auto: 4 Burst, 8 local)
            max_batch_chars: Taille max en caract√®res par batch (auto: 6KB Burst, 12KB local)
            max_text_chars: Taille max par texte individuel (d√©faut: 1500)
            max_concurrent: Nombre max de requ√™tes parall√®les (auto-d√©tect√© si None)
            max_retries: Nombre de tentatives en cas d'erreur (d√©faut: 3)

        Returns:
            Embeddings numpy array
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed, CancelledError

        # Auto-d√©tection config selon mode Burst
        circuit_breaker_threshold = 5  # D√©faut
        try:
            from knowbase.ingestion.burst.provider_switch import get_burst_concurrency_config
            config = get_burst_concurrency_config()
            if max_concurrent is None:
                max_concurrent = config.get("max_concurrent_embeddings", 6)
            if max_batch_size is None:
                max_batch_size = config.get("embedding_batch_size", 4)
            if max_batch_chars is None:
                max_batch_chars = config.get("embedding_batch_chars", 6000)
            # 2024-12-30: Lire aussi max_text_chars depuis config Burst
            config_max_text_chars = config.get("embedding_max_text_chars")
            if config_max_text_chars:
                max_text_chars = config_max_text_chars
            circuit_breaker_threshold = config.get("circuit_breaker_threshold", 5)
        except ImportError:
            if max_concurrent is None:
                max_concurrent = 6
            if max_batch_size is None:
                max_batch_size = 8
            if max_batch_chars is None:
                max_batch_chars = 12000

        if not sentences:
            return np.array([])

        # Tronquer les textes trop longs (TEI max_input_length ~512 tokens ‚âà 1500 chars)
        truncated = [
            text[:max_text_chars] if len(text) > max_text_chars else text
            for text in sentences
        ]

        # Cr√©er les batches avec limite de caract√®res
        batches = []
        current_batch = []
        current_chars = 0

        for text in truncated:
            text_len = len(text)
            if current_batch and (
                len(current_batch) >= max_batch_size or
                current_chars + text_len > max_batch_chars
            ):
                batches.append(current_batch)
                current_batch = []
                current_chars = 0
            current_batch.append(text)
            current_chars += text_len

        if current_batch:
            batches.append(current_batch)

        total_batches = len(batches)
        start_time = time.time()
        logger.info(
            f"[EMBEDDINGS:BURST] {len(truncated)} texts ‚Üí {total_batches} batches "
            f"(size={max_batch_size}, chars={max_batch_chars}, concurrent={max_concurrent})"
        )

        # Circuit breaker state
        consecutive_failures = 0
        circuit_broken = False

        def encode_batch_with_retry(batch_idx: int, batch: List[str]) -> tuple:
            """Encode un batch avec retry en cas d'erreur."""
            nonlocal consecutive_failures, circuit_broken

            # Check circuit breaker before starting
            if circuit_broken:
                raise RuntimeError("Circuit breaker tripped - too many consecutive failures")

            last_error = None
            for attempt in range(max_retries):
                try:
                    response = requests.post(
                        f"{self._burst_endpoint}/embed",
                        json={"inputs": batch},
                        timeout=self._burst_timeout,
                        headers={"Content-Type": "application/json"}
                    )
                    response.raise_for_status()
                    # Success - reset consecutive failures
                    consecutive_failures = 0
                    return batch_idx, np.array(response.json())
                except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                    last_error = e
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 2  # Backoff: 2s, 4s, 6s
                        logger.warning(
                            f"[EMBEDDINGS:BURST] Batch {batch_idx+1} attempt {attempt+1} failed, "
                            f"retrying in {wait_time}s..."
                        )
                        time.sleep(wait_time)

            # All retries failed - increment consecutive failures
            consecutive_failures += 1
            if consecutive_failures >= circuit_breaker_threshold:
                circuit_broken = True
                logger.error(
                    f"[EMBEDDINGS:BURST] ‚ö° Circuit breaker tripped after {consecutive_failures} "
                    f"consecutive failures"
                )
            raise last_error

        # Ex√©cuter les batches en parall√®le
        results = [None] * total_batches
        completed = 0
        errors = 0

        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            futures = {
                executor.submit(encode_batch_with_retry, idx, batch): idx
                for idx, batch in enumerate(batches)
            }

            for future in as_completed(futures):
                batch_idx = futures[future]
                try:
                    idx, embeddings = future.result()
                    results[idx] = embeddings
                    completed += 1

                    # Log progression tous les 10% ou 100 batches
                    if total_batches > 50 and completed % max(1, total_batches // 10) == 0:
                        elapsed = time.time() - start_time
                        rate = completed / elapsed if elapsed > 0 else 0
                        eta = (total_batches - completed) / rate if rate > 0 else 0
                        logger.info(
                            f"[EMBEDDINGS:BURST] Progress: {completed}/{total_batches} "
                            f"({100*completed//total_batches}%) - ETA: {eta:.0f}s"
                        )

                except RuntimeError as e:
                    if "Circuit breaker" in str(e):
                        # Cancel remaining futures
                        for f in futures:
                            f.cancel()
                        logger.error(f"[EMBEDDINGS:BURST] Aborting - circuit breaker active")
                        raise
                    errors += 1
                    raise
                except requests.exceptions.Timeout:
                    errors += 1
                    logger.error(
                        f"[EMBEDDINGS:BURST] Timeout on batch {batch_idx+1}/{total_batches} "
                        f"after {max_retries} retries (consecutive_failures={consecutive_failures})"
                    )
                    if circuit_broken:
                        # Cancel remaining futures
                        for f in futures:
                            f.cancel()
                        raise RuntimeError(f"Circuit breaker: {consecutive_failures} consecutive failures")
                    raise
                except requests.exceptions.ConnectionError as e:
                    errors += 1
                    logger.error(f"[EMBEDDINGS:BURST] Connection error on batch {batch_idx+1}: {e}")
                    if circuit_broken:
                        for f in futures:
                            f.cancel()
                        raise RuntimeError(f"Circuit breaker: {consecutive_failures} consecutive failures")
                    raise
                except CancelledError:
                    # Batch was cancelled due to circuit breaker
                    pass
                except Exception as e:
                    errors += 1
                    logger.error(f"[EMBEDDINGS:BURST] Error on batch {batch_idx+1}: {e}")
                    raise

        # V√©rifier que tous les r√©sultats sont pr√©sents
        valid_results = [r for r in results if r is not None]
        if len(valid_results) != total_batches:
            raise RuntimeError(
                f"[EMBEDDINGS:BURST] Incomplete: {len(valid_results)}/{total_batches} batches succeeded"
            )

        embeddings = np.vstack(valid_results) if valid_results else np.array([])
        elapsed = time.time() - start_time
        logger.info(
            f"[EMBEDDINGS:BURST] ‚úÖ Encoded {len(truncated)} texts ‚Üí {embeddings.shape} "
            f"in {elapsed:.1f}s ({len(truncated)/elapsed:.0f} texts/s)"
        )
        return embeddings

    def unload_model(self):
        """
        Force le d√©chargement du mod√®le et lib√®re la m√©moire GPU.
        Peut √™tre appel√© manuellement.
        """
        with self._model_lock:
            self._unload_model_internal()

    def _unload_model_internal(self):
        """D√©charge le mod√®le (appel√© avec le lock d√©j√† acquis)."""
        if self._model is None:
            return

        logger.info(f"[EMBEDDINGS] Unloading model: {self._model_name}")

        # Supprimer la r√©f√©rence au mod√®le
        model_name = self._model_name
        self._model = None
        self._model_name = None
        self._device = None

        # Forcer le garbage collection
        gc.collect()

        # Lib√©rer le cache CUDA si disponible
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("[EMBEDDINGS] GPU memory cache cleared")
        except ImportError:
            pass
        except Exception as e:
            logger.warning(f"[EMBEDDINGS] Error clearing GPU cache: {e}")

        logger.info(f"[EMBEDDINGS] Model {model_name} unloaded")

    def _start_monitor(self):
        """D√©marre le thread de surveillance pour le d√©chargement automatique."""
        if self._monitor_thread is not None and self._monitor_thread.is_alive():
            return

        self._stop_monitor.clear()
        self._monitor_thread = threading.Thread(
            target=self._monitor_loop,
            daemon=True,
            name="EmbeddingModelMonitor"
        )
        self._monitor_thread.start()
        logger.debug("[EMBEDDINGS] Monitor thread started")

    def _monitor_loop(self):
        """Boucle de surveillance pour le d√©chargement automatique."""
        check_interval = 30  # V√©rifier toutes les 30 secondes

        while not self._stop_monitor.is_set():
            time.sleep(check_interval)

            with self._model_lock:
                if self._model is None:
                    # Pas de mod√®le charg√©, arr√™ter la surveillance
                    logger.debug("[EMBEDDINGS] No model loaded, stopping monitor")
                    break

                idle_time = time.time() - self._last_access_time

                if idle_time >= self._timeout_seconds:
                    logger.info(
                        f"[EMBEDDINGS] Model idle for {idle_time / 60:.1f} min, "
                        f"auto-unloading..."
                    )
                    self._unload_model_internal()
                    break

    def stop_monitor(self):
        """Arr√™te le thread de surveillance."""
        self._stop_monitor.set()
        if self._monitor_thread is not None:
            self._monitor_thread.join(timeout=5)

    def get_status(self) -> dict:
        """Retourne le statut actuel du gestionnaire."""
        with self._model_lock:
            if self._model is None:
                return {
                    "model_loaded": False,
                    "model_name": None,
                    "device": None,
                    "idle_seconds": None,
                    "timeout_seconds": self._timeout_seconds
                }

            return {
                "model_loaded": True,
                "model_name": self._model_name,
                "device": self._device or "auto",
                "idle_seconds": int(time.time() - self._last_access_time),
                "timeout_seconds": self._timeout_seconds
            }

    # =========================================================================
    # Mode Burst - Basculement vers EC2 Spot
    # =========================================================================

    def enable_burst_mode(self, embeddings_url: str, timeout: int = 60):
        """
        Active le mode Burst : embeddings calcul√©s sur EC2 Spot.

        En mode Burst :
        - Les appels encode() sont redirig√©s vers le service distant
        - Le mod√®le local est d√©charg√© pour lib√©rer le GPU

        Args:
            embeddings_url: URL du service embeddings (ex: http://ec2-xxx:8001)
            timeout: Timeout HTTP en secondes (d√©faut: 60)
        """
        self._burst_mode = True
        self._burst_endpoint = embeddings_url.rstrip("/")
        self._burst_timeout = timeout

        # D√©charger le mod√®le local pour lib√©rer le GPU
        self.unload_model()

        logger.info(f"[EMBEDDINGS] üöÄ Burst mode ENABLED ‚Üí {embeddings_url}")

    def disable_burst_mode(self):
        """
        D√©sactive le mode Burst, retour au GPU local.
        Le mod√®le local sera recharg√© au prochain appel encode().
        """
        if not self._burst_mode:
            logger.debug("[EMBEDDINGS] Burst mode already disabled")
            return

        self._burst_mode = False
        self._burst_endpoint = None

        logger.info("[EMBEDDINGS] ‚èπÔ∏è Burst mode DISABLED ‚Üí Local GPU")

    def is_burst_mode_active(self) -> bool:
        """V√©rifie si le mode Burst est actif."""
        return self._burst_mode and self._burst_endpoint is not None

    def get_burst_status(self) -> Dict[str, Any]:
        """Retourne le statut du mode Burst."""
        return {
            "burst_mode": self._burst_mode,
            "burst_endpoint": self._burst_endpoint,
            "burst_timeout": self._burst_timeout if self._burst_mode else None
        }

    def get_max_text_chars(self) -> int:
        """
        Retourne la taille max de texte accept√©e par l'encoder (burst ou local).

        Utilis√© par le rechunker pour ajuster dynamiquement target_chars.
        Lit la config burst si disponible, sinon 1500 par d√©faut.
        """
        try:
            from knowbase.ingestion.burst.provider_switch import get_burst_concurrency_config
            config = get_burst_concurrency_config()
            return config.get("embedding_max_text_chars", 1500)
        except ImportError:
            return 1500

    def get_sentence_embedding_dimension(self) -> Optional[int]:
        """Retourne la dimension des embeddings (charge le mod√®le si n√©cessaire)."""
        # En mode burst, on ne peut pas conna√Ætre la dimension sans appeler le service
        if self._burst_mode:
            return None
        model = self.get_model()
        return model.get_sentence_embedding_dimension()


# Instance singleton
_manager: Optional[EmbeddingModelManager] = None


def get_embedding_manager() -> EmbeddingModelManager:
    """R√©cup√®re le gestionnaire singleton."""
    global _manager
    if _manager is None:
        _manager = EmbeddingModelManager()
    return _manager


def get_sentence_transformer(
    model_name: Optional[str] = None,
    device: Optional[str] = None,
    cache_folder: Optional[str] = None,
):
    """
    Fonction de compatibilit√© avec l'ancienne API.

    Utilise maintenant le gestionnaire avec d√©chargement automatique.
    """
    manager = get_embedding_manager()
    return manager.get_model(
        model_name=model_name,
        device=device,
        cache_folder=cache_folder
    )


def unload_embedding_model():
    """Force le d√©chargement du mod√®le d'embedding."""
    manager = get_embedding_manager()
    manager.unload_model()


def get_embedding_status() -> dict:
    """Retourne le statut du mod√®le d'embedding."""
    manager = get_embedding_manager()
    return manager.get_status()
