AWSTemplateFormatVersion: '2010-09-09'
Description: 'OSMOSE Burst - EC2 Spot Fleet for Qwen 2.5 14B AWQ + Embeddings'

# ============================================================================
# OSMOSE KnowWhere - Burst Mode Infrastructure v2.0
#
# Optimisé pour Qwen 2.5 14B AWQ sur instances g6/g6e avec Deep Learning AMI.
# L'AMI Deep Learning inclut les drivers NVIDIA préinstallés = boot rapide.
#
# Modèles:
#   - vLLM: Qwen/Qwen2.5-14B-Instruct-AWQ (~8GB VRAM)
#   - TEI: intfloat/multilingual-e5-large (~2GB VRAM)
#   - Total: ~10-12GB sur GPU 24GB (L4/A10G)
#
# Usage:
#   1. BurstOrchestrator crée ce stack
#   2. Instance boot avec DLAMI (drivers ready)
#   3. Docker démarre vLLM + TEI
#   4. Pipeline local appelle les APIs
#   5. Stack supprimé à la fin du batch
# ============================================================================

Parameters:
  BatchId:
    Type: String
    Description: Identifiant unique du batch

  # === Modèles ===
  VllmModel:
    Type: String
    Default: "Qwen/Qwen2.5-14B-Instruct-AWQ"
    Description: Modèle HuggingFace pour vLLM (AWQ quantifié)

  EmbeddingsModel:
    Type: String
    Default: "intfloat/multilingual-e5-large"
    Description: Modèle HuggingFace pour embeddings

  # === Configuration vLLM pour AWQ ===
  VllmQuantization:
    Type: String
    Default: "awq"
    AllowedValues: ["awq", "awq_marlin", "gptq", "squeezellm", "none"]
    Description: Méthode de quantification AWQ

  VllmDtype:
    Type: String
    Default: "half"
    AllowedValues: ["half", "float16", "bfloat16", "auto"]
    Description: Type de données pour inférence

  VllmGpuMemoryUtilization:
    Type: String
    Default: "0.80"
    Description: Fraction mémoire GPU pour vLLM (0.0-1.0) - 0.80 ≈ 18.4GB vLLM + ~4.6GB libre pour TEI float16

  VllmMaxModelLen:
    Type: Number
    Default: 16384
    Description: Longueur max du contexte (16384 pour prompts longs d'extraction Pass 1)

  VllmMaxNumSeqs:
    Type: Number
    Default: 64
    Description: Nombre max de séquences concurrentes (augmenté pour batching)

  # === Optimisations vLLM (2026-01-27) ===
  VllmEnablePrefixCaching:
    Type: String
    Default: "true"
    AllowedValues: ["true", "false"]
    Description: Activer le prefix caching (réutilise le cache KV du prompt système)

  VllmEnableChunkedPrefill:
    Type: String
    Default: "true"
    AllowedValues: ["true", "false"]
    Description: Activer le chunked prefill (traitement par morceaux des longs prompts)

  VllmMaxNumBatchedTokens:
    Type: Number
    Default: 4096
    Description: Nombre max de tokens par batch

  # === Configuration TEI (Text Embeddings Inference) ===
  TeiPayloadLimit:
    Type: Number
    Default: 10000000
    Description: Taille max payload HTTP en bytes (10 MB, évite 413 Payload Too Large)

  TeiMaxClientBatchSize:
    Type: Number
    Default: 32
    Description: Nombre max de textes par requête HTTP

  TeiMaxBatchTokens:
    Type: Number
    Default: 16384
    Description: Tokens max par batch total (32 textes × 512 tokens)

  # === Ports ===
  VllmPort:
    Type: Number
    Default: 8000
    Description: Port pour vLLM API

  EmbeddingsPort:
    Type: Number
    Default: 8001
    Description: Port pour Embeddings API

  # === Spot Configuration ===
  SpotMaxPrice:
    Type: String
    Default: "1.20"
    Description: Prix maximum Spot par heure (g6.2xlarge ~$0.70-0.90)

  # === Réseau ===
  VpcId:
    Type: String
    Default: ""
    Description: VPC ID (vide = VPC par défaut)

  SubnetId:
    Type: String
    Default: ""
    Description: Subnet ID (vide = subnet par défaut)

  # === AMI ===
  # AMI Golden OSMOSE v7 - vLLM v0.6.6.post1 compatible CUDA 13/Driver 580
  AmiId:
    Type: String
    Default: "ami-099cbd44d5be7612b"
    Description: AMI ID (Golden v7 - vLLM v0.6.6.post1 + TEI 89-1.6 GPU, compatible Deep Learning AMI 2026)

  # === SSH ===
  KeyName:
    Type: String
    Default: "osmose-burst-key"
    Description: Nom de la Key Pair EC2 pour accès SSH

Conditions:
  UseDefaultVpc: !Equals [!Ref VpcId, ""]
  UseDefaultSubnet: !Equals [!Ref SubnetId, ""]

Resources:
  # ============================================================================
  # Security Group
  # ============================================================================
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Sub "OSMOSE Burst Security Group - ${BatchId}"
      VpcId: !If [UseDefaultVpc, !Ref "AWS::NoValue", !Ref VpcId]
      SecurityGroupIngress:
        # vLLM API
        - IpProtocol: tcp
          FromPort: !Ref VllmPort
          ToPort: !Ref VllmPort
          CidrIp: 0.0.0.0/0
          Description: vLLM API (OpenAI compatible)
        # Embeddings API
        - IpProtocol: tcp
          FromPort: !Ref EmbeddingsPort
          ToPort: !Ref EmbeddingsPort
          CidrIp: 0.0.0.0/0
          Description: Text Embeddings Inference API
        # Health endpoint
        - IpProtocol: tcp
          FromPort: 8080
          ToPort: 8080
          CidrIp: 0.0.0.0/0
          Description: Health check endpoint
        # SSH (debug)
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0
          Description: SSH access (debug)
      Tags:
        - Key: Name
          Value: !Sub "knowwhere-burst-sg-${BatchId}"
        - Key: Project
          Value: KnowWhere
        - Key: Component
          Value: Burst

  # ============================================================================
  # IAM Roles
  # ============================================================================
  SpotFleetRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "knowwhere-burst-fleet-${BatchId}"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: spotfleet.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole

  InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "knowwhere-burst-instance-${BatchId}"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        # Accès lecture S3 pour modèles cache (optionnel)
        - arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
      Policies:
        - PolicyName: CloudWatchLogs
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "*"

  InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Sub "knowwhere-burst-profile-${BatchId}"
      Roles:
        - !Ref InstanceRole

  # ============================================================================
  # Spot Fleet - g6/g6e pour Qwen 14B AWQ
  # ============================================================================
  SpotFleet:
    Type: AWS::EC2::SpotFleet
    Properties:
      SpotFleetRequestConfigData:
        IamFleetRole: !GetAtt SpotFleetRole.Arn
        TargetCapacity: 1
        AllocationStrategy: priceCapacityOptimized  # Priorise prix avec capacité acceptable
        Type: maintain
        TerminateInstancesWithExpiration: true
        SpotPrice: !Ref SpotMaxPrice
        ReplaceUnhealthyInstances: true

        LaunchSpecifications:
          # ================================================================
          # g6.2xlarge - NVIDIA L4 24GB (Préféré - moins cher, plus stable)
          # Prix: ~$0.47-0.48/h vs g6e à ~$1.17-1.70/h
          # GPU L4 24GB suffit pour Qwen 14B AWQ (~8GB) + TEI (~2GB)
          # ================================================================
          - InstanceType: g6.2xlarge
            # AMI Golden OSMOSE v7 - vLLM v0.6.6.post1 compatible CUDA 13
            ImageId: !Ref AmiId
            KeyName: !Ref KeyName
            # NetworkInterfaces avec SubnetId explicite (requis pour SpotFleet)
            NetworkInterfaces:
              - DeviceIndex: 0
                SubnetId: subnet-3320d458
                Groups:
                  - !GetAtt SecurityGroup.GroupId
                AssociatePublicIpAddress: true
                DeleteOnTermination: true
            IamInstanceProfile:
              Arn: !GetAtt InstanceProfile.Arn
            TagSpecifications:
              - ResourceType: instance
                Tags:
                  - Key: Name
                    Value: !Sub "knowwhere-burst-${BatchId}"
                  - Key: Project
                    Value: KnowWhere
                  - Key: Component
                    Value: Burst
                  - Key: BatchId
                    Value: !Ref BatchId
                  - Key: Model
                    Value: !Ref VllmModel
            UserData:
              Fn::Base64: !Sub |
                #!/bin/bash
                set -ex

                # ============================================
                # OSMOSE Burst Instance Bootstrap
                # Deep Learning AMI - Drivers déjà installés
                # ============================================

                exec > >(tee /var/log/user-data.log) 2>&1
                echo "=== OSMOSE Burst Bootstrap ==="
                echo "Starting at $(date)"
                echo "Instance type: $(curl -s http://169.254.169.254/latest/meta-data/instance-type)"

                # Variables
                VLLM_MODEL="${VllmModel}"
                EMBEDDINGS_MODEL="${EmbeddingsModel}"
                VLLM_PORT="${VllmPort}"
                EMB_PORT="${EmbeddingsPort}"
                VLLM_QUANTIZATION="${VllmQuantization}"
                VLLM_DTYPE="${VllmDtype}"
                VLLM_GPU_MEM="${VllmGpuMemoryUtilization}"
                VLLM_MAX_MODEL_LEN="${VllmMaxModelLen}"
                VLLM_MAX_NUM_SEQS="${VllmMaxNumSeqs}"
                VLLM_ENABLE_PREFIX_CACHING="${VllmEnablePrefixCaching}"
                VLLM_ENABLE_CHUNKED_PREFILL="${VllmEnableChunkedPrefill}"
                VLLM_MAX_NUM_BATCHED_TOKENS="${VllmMaxNumBatchedTokens}"
                TEI_PAYLOAD_LIMIT="${TeiPayloadLimit}"
                TEI_MAX_CLIENT_BATCH_SIZE="${TeiMaxClientBatchSize}"
                TEI_MAX_BATCH_TOKENS="${TeiMaxBatchTokens}"

                # ============================================
                # Docker Setup (DLAMI a Docker préinstallé)
                # ============================================
                echo "Setting up Docker..."

                # Sur Ubuntu DLAMI, Docker est déjà installé
                if ! command -v docker &> /dev/null; then
                    apt-get update && apt-get install -y docker.io
                fi

                systemctl start docker
                systemctl enable docker

                # NVIDIA Container Toolkit (devrait être présent sur DLAMI)
                if ! command -v nvidia-container-toolkit &> /dev/null; then
                    echo "Installing NVIDIA Container Toolkit..."
                    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
                    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -
                    curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
                        tee /etc/apt/sources.list.d/nvidia-docker.list
                    apt-get update && apt-get install -y nvidia-container-toolkit
                    nvidia-ctk runtime configure --runtime=docker
                    systemctl restart docker
                fi

                # Vérifier GPU disponible
                echo "Checking GPU..."
                nvidia-smi

                # ============================================
                # Start vLLM with AWQ quantization
                # ============================================
                echo "Starting vLLM server..."
                echo "  Model: $VLLM_MODEL"
                echo "  Quantization: $VLLM_QUANTIZATION"
                echo "  GPU Memory: $VLLM_GPU_MEM"

                # Construire les arguments vLLM
                VLLM_ARGS="--model $VLLM_MODEL"
                VLLM_ARGS="$VLLM_ARGS --max-model-len $VLLM_MAX_MODEL_LEN"
                VLLM_ARGS="$VLLM_ARGS --gpu-memory-utilization $VLLM_GPU_MEM"
                VLLM_ARGS="$VLLM_ARGS --max-num-seqs $VLLM_MAX_NUM_SEQS"
                VLLM_ARGS="$VLLM_ARGS --trust-remote-code"

                # Ajouter quantization si spécifiée (obligatoire pour AWQ)
                if [ "$VLLM_QUANTIZATION" != "none" ]; then
                    VLLM_ARGS="$VLLM_ARGS --quantization $VLLM_QUANTIZATION"
                fi

                # Ajouter dtype
                VLLM_ARGS="$VLLM_ARGS --dtype $VLLM_DTYPE"

                echo "vLLM args: $VLLM_ARGS"

                # ============================================
                # Créer container vLLM avec modèle local (EBS)
                # Le modèle est déjà téléchargé dans /models sur l'AMI
                # ============================================
                VLLM_MODEL_PATH="/models/Qwen--Qwen2.5-14B-Instruct-AWQ"

                echo "Creating vLLM container with local model..."
                echo "Model path: $VLLM_MODEL_PATH"

                # Supprimer ancien container si existant
                docker rm -f vllm 2>/dev/null || true

                # Construire les arguments optionnels
                VLLM_EXTRA_ARGS=""
                if [ "$VLLM_ENABLE_PREFIX_CACHING" = "true" ]; then
                    VLLM_EXTRA_ARGS="$VLLM_EXTRA_ARGS --enable-prefix-caching"
                    echo "  Prefix caching: ENABLED"
                fi
                if [ "$VLLM_ENABLE_CHUNKED_PREFILL" = "true" ]; then
                    VLLM_EXTRA_ARGS="$VLLM_EXTRA_ARGS --enable-chunked-prefill --max-num-batched-tokens $VLLM_MAX_NUM_BATCHED_TOKENS"
                    echo "  Chunked prefill: ENABLED (max_batched_tokens=$VLLM_MAX_NUM_BATCHED_TOKENS)"
                fi

                # Créer nouveau container avec modèle local + optimisations
                docker run -d --gpus all --restart unless-stopped \
                    -p $VLLM_PORT:8000 --name vllm \
                    -v $VLLM_MODEL_PATH:/model:ro \
                    vllm/vllm-openai:v0.6.6.post1 \
                    --model /model \
                    --quantization $VLLM_QUANTIZATION \
                    --dtype $VLLM_DTYPE \
                    --gpu-memory-utilization $VLLM_GPU_MEM \
                    --max-model-len $VLLM_MAX_MODEL_LEN \
                    --max-num-seqs $VLLM_MAX_NUM_SEQS \
                    --trust-remote-code \
                    $VLLM_EXTRA_ARGS

                # Recréer container TEI avec paramètres configurables
                # (remplace le container pré-créé dans l'AMI Golden)
                TEI_MODEL_PATH="/models/intfloat--multilingual-e5-large"
                docker rm -f embeddings 2>/dev/null || true

                echo "Creating TEI container (GPU 89-1.6)..."
                echo "  payload-limit: $TEI_PAYLOAD_LIMIT"
                echo "  max-client-batch-size: $TEI_MAX_CLIENT_BATCH_SIZE"
                echo "  max-batch-tokens: $TEI_MAX_BATCH_TOKENS"

                docker run -d --restart unless-stopped --gpus all \
                    -p $EMB_PORT:80 --name embeddings \
                    -v $TEI_MODEL_PATH:/model:ro \
                    ghcr.io/huggingface/text-embeddings-inference:89-1.6 \
                    --model-id /model \
                    --max-client-batch-size $TEI_MAX_CLIENT_BATCH_SIZE \
                    --max-batch-tokens $TEI_MAX_BATCH_TOKENS \
                    --payload-limit $TEI_PAYLOAD_LIMIT \
                    --dtype float16 \
                    --port 80

                # Attendre que les containers soient prêts
                echo "Waiting for containers to start..."
                sleep 10
                docker ps

                # ============================================
                # Health Check Endpoint
                # ============================================
                echo "Creating health check endpoint..."

                cat > /opt/health.py << 'HEALTHEOF'
                #!/usr/bin/env python3
                """
                Health endpoint pour OSMOSE Burst.
                Vérifie que vLLM ET TEI sont prêts avant de retourner healthy.
                Inclut aussi le statut d'interruption Spot (polling metadata AWS).
                """
                from http.server import HTTPServer, BaseHTTPRequestHandler
                import json
                import urllib.request
                import urllib.error
                import os

                VLLM_PORT = int(os.environ.get("VLLM_PORT", 8000))
                EMB_PORT = int(os.environ.get("EMB_PORT", 8001))
                METADATA_URL = "http://169.254.169.254/latest/meta-data/spot/instance-action"

                class HealthHandler(BaseHTTPRequestHandler):
                    def do_GET(self):
                        vllm_ok = False
                        vllm_msg = "not checked"
                        emb_ok = False
                        emb_msg = "not checked"
                        spot_interruption = None

                        # Check vLLM - /health retourne 200 quand modèle chargé
                        try:
                            resp = urllib.request.urlopen(
                                f"http://localhost:{VLLM_PORT}/health",
                                timeout=10
                            )
                            vllm_ok = resp.status == 200
                            vllm_msg = "ready" if vllm_ok else f"status {resp.status}"
                        except urllib.error.URLError as e:
                            vllm_msg = f"connection error: {e.reason}"
                        except Exception as e:
                            vllm_msg = f"error: {str(e)}"

                        # Check TEI
                        try:
                            resp = urllib.request.urlopen(
                                f"http://localhost:{EMB_PORT}/health",
                                timeout=10
                            )
                            emb_ok = resp.status == 200
                            emb_msg = "ready" if emb_ok else f"status {resp.status}"
                        except urllib.error.URLError as e:
                            emb_msg = f"connection error: {e.reason}"
                        except Exception as e:
                            emb_msg = f"error: {str(e)}"

                        # Check Spot Interruption (AWS metadata)
                        # Retourne 404 si pas d'interruption prévue, 200 avec JSON si interruption
                        try:
                            resp = urllib.request.urlopen(METADATA_URL, timeout=2)
                            if resp.status == 200:
                                spot_interruption = json.loads(resp.read().decode())
                                print(f"⚠️ SPOT INTERRUPTION DETECTED: {spot_interruption}")
                        except urllib.error.HTTPError as e:
                            if e.code != 404:  # 404 = pas d'interruption (normal)
                                print(f"Spot metadata error: {e.code}")
                        except Exception:
                            pass  # Pas d'interruption ou erreur réseau

                        status = {
                            "vllm": {"healthy": vllm_ok, "message": vllm_msg},
                            "embeddings": {"healthy": emb_ok, "message": emb_msg},
                            "healthy": vllm_ok and emb_ok,
                            "spot_interruption": spot_interruption  # None ou {"action": "terminate", "time": "..."}
                        }

                        if status["healthy"]:
                            self.send_response(200)
                        else:
                            self.send_response(503)

                        self.send_header("Content-Type", "application/json")
                        self.end_headers()
                        self.wfile.write(json.dumps(status, indent=2).encode())

                    def log_message(self, format, *args):
                        # Log requests pour debug
                        print(f"Health check: {args[0]}")

                if __name__ == "__main__":
                    print(f"Health endpoint starting on port 8080...")
                    print(f"  vLLM port: {VLLM_PORT}")
                    print(f"  Embeddings port: {EMB_PORT}")
                    print(f"  Spot metadata: {METADATA_URL}")
                    server = HTTPServer(("", 8080), HealthHandler)
                    server.serve_forever()
                HEALTHEOF

                chmod +x /opt/health.py

                # Lancer health endpoint avec les bons ports
                VLLM_PORT=$VLLM_PORT EMB_PORT=$EMB_PORT nohup python3 /opt/health.py > /var/log/health.log 2>&1 &

                # ============================================
                # Monitor les containers au démarrage
                # ============================================
                echo "Waiting for containers to start..."
                sleep 30

                echo "Container status:"
                docker ps -a

                echo "vLLM logs (last 20 lines):"
                docker logs vllm --tail 20 || true

                echo "TEI logs (last 20 lines):"
                docker logs embeddings --tail 20 || true

                # ============================================
                # Done
                # ============================================
                echo "=== Bootstrap complete at $(date) ==="
                echo "vLLM: http://localhost:$VLLM_PORT"
                echo "Embeddings: http://localhost:$EMB_PORT"
                echo "Health: http://localhost:8080 (includes Spot interruption status)"

          # ================================================================
          # g6.xlarge - NVIDIA L4 24GB (Fallback si 2xlarge indispo)
          # Moins de vCPU mais même GPU - suffisant pour notre usage
          # ================================================================
          - InstanceType: g6.xlarge
            ImageId: !Ref AmiId
            KeyName: !Ref KeyName
            NetworkInterfaces:
              - DeviceIndex: 0
                SubnetId: subnet-3320d458
                Groups:
                  - !GetAtt SecurityGroup.GroupId
                AssociatePublicIpAddress: true
                DeleteOnTermination: true
            IamInstanceProfile:
              Arn: !GetAtt InstanceProfile.Arn
            TagSpecifications:
              - ResourceType: instance
                Tags:
                  - Key: Name
                    Value: !Sub "knowwhere-burst-${BatchId}"
                  - Key: Project
                    Value: KnowWhere
            # Même UserData que g6.2xlarge (référence via Sub)
            UserData:
              Fn::Base64: !Sub |
                #!/bin/bash
                set -ex
                exec > >(tee /var/log/user-data.log) 2>&1
                echo "=== OSMOSE Burst Bootstrap (g6.xlarge) ==="

                VLLM_MODEL="${VllmModel}"
                EMBEDDINGS_MODEL="${EmbeddingsModel}"
                VLLM_PORT="${VllmPort}"
                EMB_PORT="${EmbeddingsPort}"
                VLLM_QUANTIZATION="${VllmQuantization}"
                VLLM_DTYPE="${VllmDtype}"
                VLLM_GPU_MEM="${VllmGpuMemoryUtilization}"
                VLLM_MAX_MODEL_LEN="${VllmMaxModelLen}"
                VLLM_MAX_NUM_SEQS="${VllmMaxNumSeqs}"
                VLLM_ENABLE_PREFIX_CACHING="${VllmEnablePrefixCaching}"
                VLLM_ENABLE_CHUNKED_PREFILL="${VllmEnableChunkedPrefill}"
                VLLM_MAX_NUM_BATCHED_TOKENS="${VllmMaxNumBatchedTokens}"
                TEI_PAYLOAD_LIMIT="${TeiPayloadLimit}"
                TEI_MAX_CLIENT_BATCH_SIZE="${TeiMaxClientBatchSize}"
                TEI_MAX_BATCH_TOKENS="${TeiMaxBatchTokens}"

                # Docker setup
                if ! command -v docker &> /dev/null; then
                    apt-get update && apt-get install -y docker.io
                fi
                systemctl start docker && systemctl enable docker

                # NVIDIA toolkit
                if ! command -v nvidia-container-toolkit &> /dev/null; then
                    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
                    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -
                    curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
                        tee /etc/apt/sources.list.d/nvidia-docker.list
                    apt-get update && apt-get install -y nvidia-container-toolkit
                    nvidia-ctk runtime configure --runtime=docker
                    systemctl restart docker
                fi

                nvidia-smi

                # ============================================
                # Créer container vLLM avec modèle local (EBS)
                # ============================================
                VLLM_MODEL_PATH="/models/Qwen--Qwen2.5-14B-Instruct-AWQ"
                echo "Creating vLLM container with local model: $VLLM_MODEL_PATH"

                # Construire les arguments optionnels
                VLLM_EXTRA_ARGS=""
                if [ "$VLLM_ENABLE_PREFIX_CACHING" = "true" ]; then
                    VLLM_EXTRA_ARGS="$VLLM_EXTRA_ARGS --enable-prefix-caching"
                fi
                if [ "$VLLM_ENABLE_CHUNKED_PREFILL" = "true" ]; then
                    VLLM_EXTRA_ARGS="$VLLM_EXTRA_ARGS --enable-chunked-prefill --max-num-batched-tokens $VLLM_MAX_NUM_BATCHED_TOKENS"
                fi

                docker rm -f vllm 2>/dev/null || true
                docker run -d --gpus all --restart unless-stopped \
                    -p $VLLM_PORT:8000 --name vllm \
                    -v $VLLM_MODEL_PATH:/model:ro \
                    vllm/vllm-openai:v0.6.6.post1 \
                    --model /model \
                    --quantization $VLLM_QUANTIZATION \
                    --dtype $VLLM_DTYPE \
                    --gpu-memory-utilization $VLLM_GPU_MEM \
                    --max-model-len $VLLM_MAX_MODEL_LEN \
                    --max-num-seqs $VLLM_MAX_NUM_SEQS \
                    --trust-remote-code \
                    $VLLM_EXTRA_ARGS

                # Recréer container TEI avec paramètres configurables
                TEI_MODEL_PATH="/models/intfloat--multilingual-e5-large"
                docker rm -f embeddings 2>/dev/null || true
                docker run -d --restart unless-stopped --gpus all \
                    -p $EMB_PORT:80 --name embeddings \
                    -v $TEI_MODEL_PATH:/model:ro \
                    ghcr.io/huggingface/text-embeddings-inference:89-1.6 \
                    --model-id /model \
                    --max-client-batch-size $TEI_MAX_CLIENT_BATCH_SIZE \
                    --max-batch-tokens $TEI_MAX_BATCH_TOKENS \
                    --payload-limit $TEI_PAYLOAD_LIMIT \
                    --dtype float16 \
                    --port 80

                sleep 10
                docker ps

                # Health endpoint (with Spot interruption detection)
                cat > /opt/health.py << 'EOF'
                #!/usr/bin/env python3
                from http.server import HTTPServer, BaseHTTPRequestHandler
                import json, urllib.request, urllib.error, os
                VLLM_PORT = int(os.environ.get("VLLM_PORT", 8000))
                EMB_PORT = int(os.environ.get("EMB_PORT", 8001))
                SPOT_URL = "http://169.254.169.254/latest/meta-data/spot/instance-action"
                class H(BaseHTTPRequestHandler):
                    def do_GET(self):
                        v = e = False
                        spot = None
                        try: v = urllib.request.urlopen(f"http://localhost:{VLLM_PORT}/health", timeout=10).status == 200
                        except: pass
                        try: e = urllib.request.urlopen(f"http://localhost:{EMB_PORT}/health", timeout=10).status == 200
                        except: pass
                        try:
                            r = urllib.request.urlopen(SPOT_URL, timeout=2)
                            if r.status == 200: spot = json.loads(r.read().decode())
                        except urllib.error.HTTPError: pass
                        except: pass
                        self.send_response(200 if v and e else 503)
                        self.send_header("Content-Type", "application/json")
                        self.end_headers()
                        self.wfile.write(json.dumps({"vllm": {"healthy": v}, "embeddings": {"healthy": e}, "healthy": v and e, "spot_interruption": spot}).encode())
                    def log_message(self, *a): pass
                HTTPServer(("", 8080), H).serve_forever()
                EOF
                chmod +x /opt/health.py
                VLLM_PORT=$VLLM_PORT EMB_PORT=$EMB_PORT nohup python3 /opt/health.py > /var/log/health.log 2>&1 &

                echo "Bootstrap complete (Health endpoint includes Spot interruption status)"

Outputs:
  SpotFleetId:
    Description: ID du Spot Fleet
    Value: !Ref SpotFleet

  SecurityGroupId:
    Description: ID du Security Group
    Value: !Ref SecurityGroup

  VllmPort:
    Description: Port vLLM
    Value: !Ref VllmPort

  EmbeddingsPort:
    Description: Port Embeddings
    Value: !Ref EmbeddingsPort

  VllmModel:
    Description: Modèle vLLM utilisé
    Value: !Ref VllmModel

  VllmQuantization:
    Description: Méthode de quantification
    Value: !Ref VllmQuantization
