import base64
import json
import os
import shutil
import uuid
from datetime import datetime, timezone
from pathlib import Path
from knowbase.common.logging import setup_logging

from openai.types.chat import (
    ChatCompletionMessageParam,
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
)
from pdf2image import convert_from_path
from qdrant_client.models import PointStruct
from knowbase.common.clients import (
    ensure_qdrant_collection,
    get_qdrant_client,
    get_sentence_transformer,
)
from knowbase.common.llm_router import LLMRouter, TaskType


from knowbase.config.paths import ensure_directories
from knowbase.config.settings import get_settings


# =========================
# Path resolution
# =========================
settings = get_settings()

DATA_ROOT = settings.data_dir
DOCS_IN = settings.docs_in_dir
DOCS_DONE = settings.docs_done_dir
SLIDES_PNG = settings.slides_dir / "pdf"
STATUS_DIR = settings.status_dir
LOGS_DIR = settings.logs_dir
MODELS_DIR = settings.models_dir


def ensure_dirs():
    ensure_directories([
        DOCS_IN,
        DOCS_DONE,
        SLIDES_PNG,
        STATUS_DIR,
        LOGS_DIR,
        MODELS_DIR,
    ])


ensure_dirs()

# =============================
# Configuration & initialisation
# =============================
COLLECTION_NAME = settings.qdrant_collection
GPT_MODEL = settings.gpt_model
MODEL_NAME = os.getenv("PDF_EMB_MODEL", settings.embeddings_model)
PUBLIC_URL = os.getenv("PUBLIC_URL", "knowbase.ngrok.app")


# ====================
# Logging
# ====================
logger = setup_logging(LOGS_DIR, "ingest_pdf_debug.log", enable_console=False)


def banner_paths():
    def exists_dir(p: Path) -> str:
        return f"{p}  (exists={p.exists()}, is_dir={p.is_dir()})"

    logger.info("=== PDF INGEST START ===")
    logger.info(f"DATA_ROOT:    {exists_dir(DATA_ROOT)}")
    logger.info(f"DOCS_IN:      {exists_dir(DOCS_IN)}")
    logger.info(f"DOCS_DONE:    {exists_dir(DOCS_DONE)}")
    logger.info(f"SLIDES_PNG:   {exists_dir(SLIDES_PNG)}")
    logger.info(f"STATUS_DIR:   {exists_dir(STATUS_DIR)}")
    logger.info(f"LOGS_DIR:     {exists_dir(LOGS_DIR)}")
    logger.info(f"MODELS_DIR:   {exists_dir(MODELS_DIR)}")
    logger.info(f"PUBLIC_URL:   {PUBLIC_URL}")


# Note: banner_paths() est appel√© dans process_pdf() si n√©cessaire, pas au niveau module
# pour √©viter la cr√©ation du fichier de log au d√©marrage

# ===================
# Clients & Models
# ===================
llm_router = LLMRouter()
qdrant_client = get_qdrant_client()
model = get_sentence_transformer(MODEL_NAME)
EMB_SIZE = model.get_sentence_embedding_dimension() or 768
ensure_qdrant_collection(COLLECTION_NAME, int(EMB_SIZE))

# ==============
# Utilities
# ==============
def clean_gpt_response(raw: str) -> str:
    s = (raw or "").strip()
    if s.startswith("```json"):
        s = s[len("```json") :].strip()
    if s.startswith("```"):
        s = s[len("```") :].strip()
    if s.endswith("```"):
        s = s[:-3].strip()
    return s


def encode_image_base64(img_path: Path) -> str:
    with open(img_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")


def extract_text_from_pdf(pdf_path: Path) -> str:
    txt_output = pdf_path.with_suffix(".txt")
    logger.info(f"üìë pdftotext: {pdf_path.name}")
    try:
        import subprocess

        # Supprimer les warnings stderr (comme "Invalid Font Weight") qui polluent les logs
        subprocess.run(
            ["pdftotext", str(pdf_path), str(txt_output)],
            check=True,
            stderr=subprocess.DEVNULL
        )
    except Exception as e:
        logger.error(f"‚ùå pdftotext failed: {e}")
        return ""
    text = ""
    try:
        text = txt_output.read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not read {txt_output}: {e}")
    logger.debug(f"Extracted text length: {len(text)}")
    return text


# ==================
# Pipeline Steps
# ==================
def analyze_pdf_metadata(pdf_text: str, source_name: str) -> dict:
    logger.info(f"üîç GPT: analyse des m√©tadonn√©es ‚Äî {source_name}")
    try:
        system_message: ChatCompletionSystemMessageParam = {
            "role": "system",
            "content": "You are a metadata extraction assistant.",
        }
        user_message: ChatCompletionUserMessageParam = {
            "role": "user",
            "content": (
                f"You're analyzing a PDF document: '{source_name}'.\n"
                f"Below is the raw text extracted from it:\n\n{pdf_text[:8000]}\n\n"
                "Extract the following high-level metadata and return a single JSON object with fields:\n"
                "- title\n- objective\n- main_solution\n- supporting_solutions\n"
                "- mentioned_solutions\n- document_type\n- audience\n- source_date\n- language\n\n"
                "IMPORTANT: For the field 'main_solution', always use the official SAP canonical solution name as published on the SAP website or documentation. "
                "Do not use acronyms, abbreviations, or local variants. If the document uses a non-canonical name, map it to the official SAP name. "
                "If you are unsure, leave the field empty."
            ),
        }
        messages: list[ChatCompletionMessageParam] = [system_message, user_message]
        raw = llm_router.complete(TaskType.METADATA_EXTRACTION, messages)
        cleaned = clean_gpt_response(raw)
        meta = json.loads(cleaned) if cleaned else {}
        logger.debug(
            f"META (keys): {list(meta.keys()) if isinstance(meta, dict) else 'n/a'}"
        )
        return meta if isinstance(meta, dict) else {}
    except Exception as e:
        logger.error(f"‚ùå GPT metadata error: {e}")
        return {}


def ask_gpt_page_analysis_text_only(
    page_text: str,
    source_name: str,
    page_index: int,
    custom_prompt: str | None = None,
):
    """
    Analyse une page PDF en utilisant uniquement le texte extrait, sans Vision.
    Plus rapide et moins co√ªteux que la version avec Vision.
    Retourne une structure unifi√©e avec concepts, facts, entities, relations (comme PPTX).
    """
    logger.info(f"üß† GPT [TEXT-ONLY]: analyse page {page_index}")
    try:
        # D√©tecter la langue du contenu (simple heuristique bas√©e sur mots courants)
        content_lower = page_text.lower()
        english_indicators = ['the ', ' and ', ' is ', ' are ', ' to ', ' of ', ' in ', ' for ', ' with ', ' that ']
        french_indicators = [' le ', ' la ', ' les ', ' et ', ' est ', ' sont ', ' de ', ' dans ', ' pour ', ' avec ', ' que ']

        english_count = sum(content_lower.count(word) for word in english_indicators)
        french_count = sum(content_lower.count(word) for word in french_indicators)

        detected_language = "ENGLISH" if english_count > french_count else "FRENCH"
        logger.debug(f"Page {page_index}: Langue d√©tect√©e = {detected_language} (EN:{english_count} vs FR:{french_count})")

        # Instructions de langue (forcer anglais pour Neo4j)
        language_instructions = (
            f"‚ö†Ô∏è CRITICAL LANGUAGE INSTRUCTIONS (DETECTED CONTENT LANGUAGE: {detected_language}):\n"
            f"- For ENTITIES and RELATIONS: ALWAYS use ENGLISH for entity names, relation types, and entity descriptions (for Knowledge Graph consistency)\n"
            f"- For CONCEPTS and FACTS: Use {detected_language} (the language of the actual content below, NOT the context description)\n"
            f"- IMPORTANT: Ignore any French/English text in context descriptions above - only look at the ACTUAL CONTENT language below\n\n"
        )

        # Utiliser custom_prompt si fourni, sinon prompt par d√©faut
        if custom_prompt:
            analysis_text = language_instructions + custom_prompt.replace("{slide_content}", page_text).replace("{slide_index}", str(page_index)).replace("{source_name}", source_name)
        else:
            # Prompt √©tendu pour extraire concepts, facts, entities, relations (format unifi√© comme PPTX)
            analysis_text = (
                language_instructions +
                f"You are analyzing page {page_index} from '{source_name}'.\n"
                f"Page content:\n{page_text}\n\n"
                "Extract structured knowledge from this page and return a JSON object with 4 keys:\n"
                "- `concepts`: Array of concept blocks, each with:\n"
                "  - `full_explanation`: string (detailed description)\n"
                "  - `meta`: object with `type`, `level`, `topic`\n"
                "- `facts`: Array of factual statements, each with:\n"
                "  - `subject`: string\n"
                "  - `predicate`: string\n"
                "  - `value`: string/number\n"
                "  - `confidence`: number (0-1)\n"
                "  - `fact_type`: string (optional)\n"
                "- `entities`: Array of named entities, each with:\n"
                "  - `name`: string\n"
                "  - `entity_type`: string (e.g., 'Product', 'Company', 'Technology')\n"
                "  - `description`: string (optional)\n"
                "- `relations`: Array of relationships between entities, each with:\n"
                "  - `source`: string (entity name)\n"
                "  - `relation_type`: string\n"
                "  - `target`: string (entity name)\n"
                "  - `description`: string (optional)\n\n"
                "Return only valid JSON."
            )

        system_message: ChatCompletionSystemMessageParam = {
            "role": "system",
            "content": (
                "You are an expert assistant that analyzes document content deeply. "
                "Extract concepts, facts, entities, and relations from text. "
                "CRITICAL: For entities/relations use ENGLISH. For concepts/facts use source document language."
            ),
        }
        user_message: ChatCompletionUserMessageParam = {
            "role": "user",
            "content": analysis_text,
        }
        messages: list[ChatCompletionMessageParam] = [system_message, user_message]

        # Utiliser TaskType.LONG_TEXT_SUMMARY au lieu de VISION (LLM plus rapide et moins co√ªteux)
        raw = llm_router.complete(TaskType.LONG_TEXT_SUMMARY, messages, temperature=0.2, max_tokens=8000)
        logger.debug(f"Page {page_index} [TEXT-ONLY]: LLM raw response (first 500 chars): {raw[:500] if raw else 'EMPTY'}")

        cleaned = clean_gpt_response(raw)
        logger.debug(f"Page {page_index} [TEXT-ONLY]: cleaned response (first 500 chars): {cleaned[:500] if cleaned else 'EMPTY'}")

        # Parser la r√©ponse JSON
        response_data = json.loads(cleaned) if cleaned else {}

        # Support format unifi√© 4 outputs
        if isinstance(response_data, dict):
            concepts = response_data.get("concepts", [])
            facts_data = response_data.get("facts", [])
            entities_data = response_data.get("entities", [])
            relations_data = response_data.get("relations", [])
        elif isinstance(response_data, list):
            # Fallback: ancien format (liste de chunks)
            concepts = response_data
            facts_data = []
            entities_data = []
            relations_data = []
        else:
            logger.warning(f"Page {page_index} [TEXT-ONLY]: Format JSON inattendu: {type(response_data)}")
            concepts = []
            facts_data = []
            entities_data = []
            relations_data = []

        logger.debug(f"Page {page_index} [TEXT-ONLY]: {len(concepts)} concepts + {len(facts_data)} facts + {len(entities_data)} entities + {len(relations_data)} relations")

        return {
            "concepts": concepts,
            "facts": facts_data,
            "entities": entities_data,
            "relations": relations_data,
        }
    except Exception as e:
        logger.error(f"‚ùå GPT page {page_index} [TEXT-ONLY] error: {e}")
        return {
            "concepts": [],
            "facts": [],
            "entities": [],
            "relations": [],
        }


def ask_gpt_block_analysis_text_only(
    block_content: str,
    block_type: str,
    block_title: str | None,
    source_name: str,
    block_index: int,
    custom_prompt: str | None = None,
):
    """
    Analyse un BLOC S√âMANTIQUE (section, paragraph, table, list) extrait par MegaParse.
    Plus intelligent que l'analyse page par page car travaille sur des unit√©s coh√©rentes.

    Args:
        block_content: Texte du bloc s√©mantique
        block_type: Type du bloc ("section", "paragraph", "table", "list")
        block_title: Titre du bloc (si section)
        source_name: Nom du document source
        block_index: Index du bloc dans le document
        custom_prompt: Prompt personnalis√© (optionnel)

    Returns:
        Dict avec 4 cl√©s: concepts, facts, entities, relations
    """
    logger.info(f"üß† LLM [KNOWLEDGE_EXTRACTION]: analyse bloc #{block_index} [{block_type}]")

    try:
        # D√©tecter la langue du contenu (simple heuristique bas√©e sur mots courants)
        content_lower = block_content.lower()
        english_indicators = ['the ', ' and ', ' is ', ' are ', ' to ', ' of ', ' in ', ' for ', ' with ', ' that ']
        french_indicators = [' le ', ' la ', ' les ', ' et ', ' est ', ' sont ', ' de ', ' dans ', ' pour ', ' avec ', ' que ']

        english_count = sum(content_lower.count(word) for word in english_indicators)
        french_count = sum(content_lower.count(word) for word in french_indicators)

        detected_language = "ENGLISH" if english_count > french_count else "FRENCH"
        logger.debug(f"Bloc {block_index}: Langue d√©tect√©e = {detected_language} (EN:{english_count} vs FR:{french_count})")

        # Instructions de langue (toujours appliqu√©es, m√™me avec custom_prompt)
        language_instructions = (
            f"‚ö†Ô∏è CRITICAL LANGUAGE INSTRUCTIONS (DETECTED CONTENT LANGUAGE: {detected_language}):\n"
            f"- For ENTITIES and RELATIONS: ALWAYS use ENGLISH for entity names, relation types, and entity descriptions (for Knowledge Graph consistency)\n"
            f"- For CONCEPTS and FACTS: Use {detected_language} (the language of the actual content below, NOT the context description)\n"
            f"- IMPORTANT: Ignore any French/English text in context descriptions above - only look at the ACTUAL CONTENT language below\n"
            f"- Example: If content is in English, write 'Security awareness training is essential...' NOT 'La formation √† la sensibilisation...'\n\n"
        )

        # Prompt adapt√© au type de bloc
        if custom_prompt:
            # Utiliser le prompt personnalis√© du document type avec pr√©fixe langue
            analysis_text = (
                language_instructions +
                custom_prompt
                .replace("{slide_content}", block_content)
                .replace("{slide_index}", str(block_index))
                .replace("{source_name}", source_name)
            )
        else:
            # Prompt par d√©faut optimis√© pour blocs s√©mantiques
            block_context = f"Block type: {block_type}"
            if block_title:
                block_context += f"\nBlock title: {block_title}"

            analysis_text = (
                language_instructions +
                f"You are analyzing a semantic block (block #{block_index}) from '{source_name}'.\n"
                f"{block_context}\n\n"
                f"Block content:\n{block_content}\n\n"
                "Extract structured knowledge from this semantic block and return a JSON object with 4 keys:\n\n"
                "1. `concepts`: Array of concept blocks (main ideas, explanations, definitions)\n"
                "   Each concept object:\n"
                "   - `full_explanation`: string (detailed description of the concept)\n"
                "   - `meta`: object with:\n"
                "     - `type`: string (e.g., 'definition', 'process', 'architecture', 'feature')\n"
                "     - `level`: number (importance: 1=critical, 2=important, 3=detail)\n"
                "     - `topic`: string (main topic/domain)\n\n"
                "2. `facts`: Array of factual statements\n"
                "   Each fact object:\n"
                "   - `subject`: string (what the fact is about)\n"
                "   - `predicate`: string (the relationship or property)\n"
                "   - `value`: string/number (the value or target)\n"
                "   - `confidence`: number (0-1, how certain is this fact)\n"
                "   - `fact_type`: string (e.g., 'specification', 'requirement', 'statistic')\n\n"
                "3. `entities`: Array of named entities (products, technologies, companies, etc.)\n"
                "   Each entity object:\n"
                "   - `name`: string (canonical name)\n"
                "   - `entity_type`: string (e.g., 'SOLUTION', 'COMPONENT', 'TECHNOLOGY', 'PRODUCT')\n"
                "   - `description`: string (brief description, optional)\n\n"
                "4. `relations`: Array of relationships between entities\n"
                "   Each relation object:\n"
                "   - `source`: string (MUST match exactly the 'name' of an entity in the 'entities' array above)\n"
                "   - `relation_type`: string (use semantic types like PART_OF, CONTAINS, USES, REQUIRES, IMPLEMENTS, SUPPORTS, EXTENDS, MENTIONS, INTEGRATES_WITH, DEPENDS_ON, REPLACES, PROVIDES)\n"
                "   - `target`: string (MUST match exactly the 'name' of an entity in the 'entities' array above)\n"
                "   - `description`: string (optional context explaining the relationship)\n\n"
                "CRITICAL INSTRUCTIONS FOR RELATIONS:\n"
                "- **ONLY create relations BETWEEN entities you listed in 'entities'** - both 'source' and 'target' must exist in your entities list\n"
                "- If two concepts are related but one is not an entity you identified, DO NOT create a relation\n"
                "- Extract ALL relationships that are **explicitly stated or clearly implied** in the text\n"
                "- Only create relations that you can justify from the source content - DO NOT invent connections\n"
                "- Common relation patterns to look for:\n"
                "  * If 'X is part of Y' or 'Y includes X' ‚Üí create (X, PART_OF, Y)\n"
                "  * If 'X uses Y' or 'X requires Y' ‚Üí create (X, USES/REQUIRES, Y)\n"
                "  * If 'X and Y' mentioned together in same context ‚Üí create (X, MENTIONS, Y)\n"
                "  * If 'X replaces Y' or 'X is successor of Y' ‚Üí create (X, REPLACES, Y)\n"
                "- It's OK if some entities have no relations if they're truly standalone mentions\n"
                "- Quality over quantity: 1 accurate relation is better than 3 invented ones\n"
                "- When in doubt, use MENTIONS for co-occurring entities rather than inventing a specific relation type\n\n"
                "IMPORTANT:\n"
                "- This is a SEMANTIC BLOCK, not just a random page. Extract knowledge that is COMPLETE within this block.\n"
                "- Focus on concepts that are fully explained in this block.\n"
                "- Extract all entities mentioned, even if they appear in relations.\n"
                "- Relations should connect entities that are both mentioned in this block or the broader document context.\n"
                "- Be thorough: extract ALL concepts, facts, entities, and relations present.\n"
                "- Remember: Accuracy is paramount. Only extract what you can verify from the text.\n\n"
                "Return ONLY valid JSON (no markdown, no code blocks)."
            )

        system_message: ChatCompletionSystemMessageParam = {
            "role": "system",
            "content": (
                "You are an expert knowledge extraction assistant specialized in analyzing technical and business documents. "
                "Your task is to extract structured knowledge (concepts, facts, entities, relations) from semantic blocks. "
                "CRITICAL LANGUAGE RULE: For entities and relations, ALWAYS use ENGLISH (for Knowledge Graph consistency). "
                "For concepts and facts, use the source document's language (for semantic search accuracy). "
                "Focus on ACCURACY first, then completeness. Extract relationships that are clearly stated or logically implied in the text. "
                "IMPORTANT: Only create relations BETWEEN entities you identified - both source and target must exist in your entities list. "
                "Only create relations you can justify from the source content - never invent connections. "
                "When entities appear together in context, use MENTIONS relation as a safe default. "
                "Prefer quality over quantity: accurate extraction builds trust in the knowledge graph. "
                "Always return valid JSON with all 4 keys, even if some arrays are empty."
            ),
        }
        user_message: ChatCompletionUserMessageParam = {
            "role": "user",
            "content": analysis_text,
        }
        messages: list[ChatCompletionMessageParam] = [system_message, user_message]

        # Log du prompt pour debug (uniquement les 600 premiers chars)
        logger.debug(f"Bloc {block_index} [PROMPT]: {analysis_text[:600]}")

        # Utiliser TaskType.KNOWLEDGE_EXTRACTION (les param√®tres viennent du YAML)
        raw = llm_router.complete(
            TaskType.KNOWLEDGE_EXTRACTION,
            messages
            # temperature et max_tokens sont d√©finis dans config/llm_models.yaml
        )

        logger.debug(f"Bloc {block_index} [KNOWLEDGE_EXTRACTION]: LLM raw response (first 500 chars): {raw[:500] if raw else 'EMPTY'}")

        cleaned = clean_gpt_response(raw)
        response_data = json.loads(cleaned) if cleaned else {}

        # Parser la r√©ponse
        if isinstance(response_data, dict):
            concepts = response_data.get("concepts", [])
            facts_data = response_data.get("facts", [])
            entities_data = response_data.get("entities", [])
            relations_data = response_data.get("relations", [])
        else:
            logger.warning(f"Bloc {block_index} [KNOWLEDGE_EXTRACTION]: Format JSON inattendu: {type(response_data)}")
            concepts = []
            facts_data = []
            entities_data = []
            relations_data = []

        logger.info(
            f"Bloc {block_index} [KNOWLEDGE_EXTRACTION]: "
            f"{len(concepts)} concepts + {len(facts_data)} facts + "
            f"{len(entities_data)} entities + {len(relations_data)} relations"
        )

        # TODO: Impl√©menter l'ingestion des facts, entities, relations dans Neo4j/Graphiti si activ√©
        # Pour l'instant, on retourne juste les concepts pour Qdrant (comme avant)

        return {
            "concepts": concepts,
            "facts": facts_data,
            "entities": entities_data,
            "relations": relations_data,
        }

    except Exception as e:
        logger.error(f"‚ùå LLM bloc {block_index} [KNOWLEDGE_EXTRACTION] error: {e}")
        return {
            "concepts": [],
            "facts": [],
            "entities": [],
            "relations": [],
        }


def transform_fact_for_neo4j(fact_data: dict) -> dict:
    """
    Transforme un fact extrait par LLM vers le format FactCreate Neo4j.

    Le LLM g√©n√®re: {subject, predicate, value (string/number), confidence, fact_type (texte libre)}
    FactCreate attend: {subject, predicate, object (string), value (float), unit, fact_type (enum), ...}

    Args:
        fact_data: Dictionnaire du fact extrait par LLM

    Returns:
        Dict compatible avec FactCreate ou None si transformation impossible
    """
    import re

    subject = fact_data.get("subject", "").strip()
    predicate = fact_data.get("predicate", "").strip()
    raw_value = fact_data.get("value", "")
    confidence = float(fact_data.get("confidence", 0.8))
    llm_fact_type = fact_data.get("fact_type", "general").lower()

    if not subject or not predicate or not raw_value:
        return None

    # Convertir raw_value en string si ce n'est pas d√©j√† le cas
    value_str = str(raw_value).strip()

    # Parser la valeur pour extraire nombre + unit√©
    # Exemples: "99.7%", "100 users", "5 GB", "enabled", "SAP HANA"
    numeric_value = 0.0
    unit = ""
    object_str = value_str
    value_type = "text"  # Par d√©faut

    # Tentative d'extraction de valeur num√©rique
    # Pattern: nombre (entier ou d√©cimal) suivi optionnellement d'une unit√©
    number_pattern = r'^\s*(-?[\d,]+\.?\d*)\s*([%\w\s/]*?)\s*$'
    match = re.match(number_pattern, value_str)

    if match:
        try:
            # Extraire le nombre (enlever les virgules de s√©paration de milliers)
            num_str = match.group(1).replace(',', '')
            numeric_value = float(num_str)
            unit = match.group(2).strip() if match.group(2) else ""
            value_type = "numeric"
        except (ValueError, AttributeError):
            # Si √©chec de conversion, garder les valeurs par d√©faut
            numeric_value = 0.0
            unit = ""
            value_type = "text"

    # Si pas d'unit√© mais valeur num√©rique trouv√©e, mettre unit√© vide
    if value_type == "numeric" and not unit:
        unit = ""

    # Si pas de valeur num√©rique, mettre unit√© vide
    if value_type == "text":
        unit = ""

    # Mapper fact_type texte libre vers enum FactType
    # LLM g√©n√®re: specification, requirement, statistic, feature, etc.
    # FactType enum: SERVICE_LEVEL, CAPACITY, PRICING, FEATURE, COMPLIANCE, GENERAL
    fact_type_mapping = {
        "service": "SERVICE_LEVEL",
        "sla": "SERVICE_LEVEL",
        "availability": "SERVICE_LEVEL",
        "uptime": "SERVICE_LEVEL",
        "capacity": "CAPACITY",
        "performance": "CAPACITY",
        "limit": "CAPACITY",
        "size": "CAPACITY",
        "storage": "CAPACITY",
        "pricing": "PRICING",
        "cost": "PRICING",
        "price": "PRICING",
        "fee": "PRICING",
        "feature": "FEATURE",
        "capability": "FEATURE",
        "functionality": "FEATURE",
        "compliance": "COMPLIANCE",
        "regulation": "COMPLIANCE",
        "standard": "COMPLIANCE",
        "certification": "COMPLIANCE",
        "requirement": "FEATURE",  # Les requirements sont souvent des features
        "specification": "FEATURE",
        "statistic": "GENERAL",
        "integration": "FEATURE",
    }

    fact_type_enum = "GENERAL"  # Par d√©faut
    for keyword, enum_value in fact_type_mapping.items():
        if keyword in llm_fact_type:
            fact_type_enum = enum_value
            break

    return {
        "subject": subject[:200],
        "predicate": predicate[:100],
        "object": object_str[:500],
        "value": numeric_value,
        "unit": unit[:50] if unit else "",
        "value_type": value_type,
        "fact_type": fact_type_enum,
        "confidence": min(max(confidence, 0.0), 1.0),
    }


def ingest_knowledge_to_neo4j(
    facts: list,
    entities: list,
    relations: list,
    document_id: str,
    source_name: str,
    tenant_id: str = "default"
) -> dict:
    """
    Ing√®re facts, entities et relations dans Neo4j.

    Args:
        facts: Liste des facts extraits
        entities: Liste des entities extraites
        relations: Liste des relations extraites
        document_id: ID du document source
        source_name: Nom du document source
        tenant_id: Tenant ID (d√©faut: "default")

    Returns:
        Dict avec statistiques d'ingestion
    """
    from knowbase.api.services.knowledge_graph_service import KnowledgeGraphService
    from knowbase.api.services.facts_service import FactsService
    from knowbase.api.schemas.knowledge_graph import EntityCreate, RelationCreate
    from knowbase.api.schemas.facts import FactCreate

    stats = {
        "entities_created": 0,
        "facts_created": 0,
        "relations_created": 0,
        "errors": 0
    }

    try:
        # Initialiser les services avec tenant_id
        kg_service = KnowledgeGraphService(tenant_id=tenant_id)
        facts_service = FactsService(tenant_id=tenant_id)

        # 1. Ingest entities
        for entity_data in entities:
            try:
                # Valider et normaliser les donn√©es
                name = entity_data.get("name", "").strip()
                entity_type = entity_data.get("entity_type", "UNKNOWN").strip().upper()
                description = entity_data.get("description", "")

                # Description min 10 chars requise par le sch√©ma
                if not description or len(description) < 10:
                    description = f"{name} ({entity_type})"

                if not name or len(name) < 1:
                    logger.warning(f"‚ö†Ô∏è Entity skipped: name trop court")
                    continue

                entity_create = EntityCreate(
                    name=name,
                    entity_type=entity_type,
                    description=description[:500],  # Max 500 chars
                    source_document_id=document_id,
                    tenant_id=tenant_id
                )

                kg_service.create_entity(entity_create)
                stats["entities_created"] += 1

            except Exception as e:
                logger.debug(f"‚ö†Ô∏è Entity creation error: {e}")
                stats["errors"] += 1

        # 2. Ingest facts
        for fact_data in facts:
            try:
                # Transformer le fact du format LLM vers le format Neo4j
                transformed_fact = transform_fact_for_neo4j(fact_data)

                if not transformed_fact:
                    # Fact invalide, passer au suivant
                    continue

                fact_create = FactCreate(
                    subject=transformed_fact["subject"],
                    predicate=transformed_fact["predicate"],
                    object=transformed_fact["object"],
                    value=transformed_fact["value"],
                    unit=transformed_fact["unit"],
                    value_type=transformed_fact["value_type"],
                    fact_type=transformed_fact["fact_type"],
                    confidence=transformed_fact["confidence"],
                    source_document=source_name,
                    source_chunk_id=None,  # Peut √™tre renseign√© si disponible
                    extraction_method="llm_knowledge_extraction",
                    extraction_model="gpt-4o-mini"  # Correspond √† la config
                )

                facts_service.create_fact(fact_create)
                stats["facts_created"] += 1

            except Exception as e:
                logger.debug(f"‚ö†Ô∏è Fact creation error: {e}")
                stats["errors"] += 1

        # 3. Ingest relations (filtr√©es : seulement entre entit√©s identifi√©es)
        # Construire un set des noms d'entit√©s pour v√©rification rapide
        entity_names_set = {e.get("name", "").strip() for e in entities if e.get("name")}

        for relation_data in relations:
            try:
                source = relation_data.get("source", "").strip()
                target = relation_data.get("target", "").strip()
                relation_type = relation_data.get("relation_type", "RELATED_TO").strip().upper()
                description = relation_data.get("description", "")

                if not source or not target:
                    continue

                # V√©rifier que source ET target sont dans les entit√©s identifi√©es
                # (le LLM devrait respecter cette r√®gle gr√¢ce au prompt, mais on v√©rifie par s√©curit√©)
                if source not in entity_names_set:
                    logger.debug(f"‚ö†Ô∏è Relation ignor√©e: source '{source}' n'est pas dans les entit√©s identifi√©es")
                    stats["relations_skipped"] = stats.get("relations_skipped", 0) + 1
                    continue

                if target not in entity_names_set:
                    logger.debug(f"‚ö†Ô∏è Relation ignor√©e: target '{target}' n'est pas dans les entit√©s identifi√©es")
                    stats["relations_skipped"] = stats.get("relations_skipped", 0) + 1
                    continue

                # Les deux entit√©s existent dans la liste, cr√©er la relation
                relation_create = RelationCreate(
                    source=source[:200],
                    target=target[:200],
                    relation_type=relation_type,
                    description=description[:500] if description else None,
                    source_document_id=document_id,
                    tenant_id=tenant_id
                )

                kg_service.create_relation(relation_create)
                stats["relations_created"] += 1

            except Exception as e:
                logger.debug(f"‚ö†Ô∏è Relation creation error: {e}")
                stats["errors"] += 1

        if stats["entities_created"] > 0 or stats["facts_created"] > 0 or stats["relations_created"] > 0:
            logger.info(
                f"üìä Neo4j: {stats['entities_created']} entities, "
                f"{stats['facts_created']} facts, {stats['relations_created']} relations "
                f"({stats['errors']} errors)"
            )

    except Exception as e:
        logger.error(f"‚ùå Neo4j ingestion error: {e}")
        stats["errors"] += 1

    return stats


def ask_gpt_slide_analysis(
    image_path: Path,
    slide_text: str,
    source_name: str,
    slide_index: int,
    custom_prompt: str | None = None,
):
    logger.info(f"üß† GPT [VISION]: analyse page {slide_index}")
    try:
        image_b64 = encode_image_base64(image_path)

        # D√©tecter la langue du contenu (simple heuristique bas√©e sur mots courants)
        content_lower = slide_text.lower()
        english_indicators = ['the ', ' and ', ' is ', ' are ', ' to ', ' of ', ' in ', ' for ', ' with ', ' that ']
        french_indicators = [' le ', ' la ', ' les ', ' et ', ' est ', ' sont ', ' de ', ' dans ', ' pour ', ' avec ', ' que ']

        english_count = sum(content_lower.count(word) for word in english_indicators)
        french_count = sum(content_lower.count(word) for word in french_indicators)

        detected_language = "ENGLISH" if english_count > french_count else "FRENCH"
        logger.debug(f"Page {slide_index} [VISION]: Langue d√©tect√©e = {detected_language} (EN:{english_count} vs FR:{french_count})")

        # Instructions de langue (forcer anglais pour Neo4j)
        language_instructions = (
            f"‚ö†Ô∏è CRITICAL LANGUAGE INSTRUCTIONS (DETECTED CONTENT LANGUAGE: {detected_language}):\n"
            f"- For ENTITIES and RELATIONS: ALWAYS use ENGLISH for entity names, relation types, and entity descriptions (for Knowledge Graph consistency)\n"
            f"- For CONCEPTS and FACTS: Use {detected_language} (the language of the actual content below, NOT the context description)\n"
            f"- IMPORTANT: Ignore any French/English text in context descriptions above - only look at the ACTUAL CONTENT language below\n\n"
        )

        # Utiliser custom_prompt si fourni, sinon prompt par d√©faut
        if custom_prompt:
            analysis_text = language_instructions + custom_prompt.replace("{slide_content}", slide_text).replace("{slide_index}", str(slide_index)).replace("{source_name}", source_name)
        else:
            analysis_text = (
                language_instructions +
                f"You are analyzing page {slide_index} from '{source_name}'.\n"
                f"Page content:\n{slide_text}\n\n"
                "Extract 1‚Äì5 standalone content blocks. For each, return:\n"
                "- `text`\n- `meta` with `type`, `level`, `topic`\n\n"
                "Return only a JSON array."
            )

        prompt: ChatCompletionUserMessageParam = {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{image_b64}"},
                },
                {
                    "type": "text",
                    "text": analysis_text,
                },
            ],
        }
        system_message: ChatCompletionSystemMessageParam = {
            "role": "system",
            "content": (
                "You are an expert assistant that analyzes PDF pages. "
                "CRITICAL: For entities/relations use ENGLISH. For concepts/facts use source document language."
            ),
        }
        messages: list[ChatCompletionMessageParam] = [system_message, prompt]
        raw = llm_router.complete(TaskType.VISION, messages)
        logger.debug(f"Page {slide_index}: LLM raw response (first 500 chars): {raw[:500] if raw else 'EMPTY'}")
        cleaned = clean_gpt_response(raw)
        logger.debug(f"Page {slide_index}: cleaned response (first 500 chars): {cleaned[:500] if cleaned else 'EMPTY'}")
        data = json.loads(cleaned) if cleaned else []
        if not isinstance(data, list):
            logger.warning(f"Page {slide_index}: LLM returned non-list data, type={type(data)}")
            data = []
        logger.debug(f"Page {slide_index}: chunks returned = {len(data)}")
        return data
    except Exception as e:
        logger.error(f"‚ùå GPT page {slide_index} error: {e}")
        return []


def ingest_chunks(chunks, doc_metadata, file_uid, page_index):
    points = []
    for chunk in chunks:
        text = (chunk.get("text") or "").strip()
        meta = chunk.get("meta", {}) or {}
        if not text or len(text) < 20:
            continue
        try:
            emb = model.encode([f"passage: {text}"], normalize_embeddings=True)[
                0
            ].tolist()
            payload = {
                "text": text,
                "language": "en",
                "ingested_at": datetime.now(timezone.utc).isoformat(timespec="seconds"),
                "gpt_chunked": True,
                "page_index": page_index,
                "page_image_url": f"{PUBLIC_URL}/static/pdf_slides/{file_uid}_page_{page_index}.png",
                "source_file_url": f"{PUBLIC_URL}/static/pdfs/{file_uid}.pdf",
            }
            payload.update(doc_metadata)
            payload.update(meta)
            points.append(
                PointStruct(id=str(uuid.uuid4()), vector=emb, payload=payload)
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Embedding error (page {page_index}): {e}")

    if points:
        try:
            qdrant_client.upsert(collection_name=COLLECTION_NAME, points=points)
            logger.info(f"‚úÖ Page {page_index}: {len(points)} chunk(s) ing√©r√©s")
        except Exception as e:
            logger.error(f"‚ùå Qdrant upsert failed (page {page_index}): {e}")


def process_pdf(pdf_path: Path, document_type_id: str | None = None, use_vision: bool = True):
    # Reconfigurer logger pour le contexte RQ worker avec lazy file creation
    global logger
    logger = setup_logging(LOGS_DIR, "ingest_pdf_debug.log", enable_console=False)

    # Premier log r√©el - c'est ici que le fichier sera cr√©√©
    logger.info(f"üöÄ Traitement: {pdf_path.name}")
    logger.info(f"üìã Document Type ID: {document_type_id or 'default'}")
    logger.info(f"üîç Mode extraction: {'VISION (GPT-4 avec images)' if use_vision else 'TEXT-ONLY (LLM rapide)'}")
    status_file = STATUS_DIR / f"{pdf_path.stem}.status"
    try:
        status_file.write_text("processing")

        meta_path = pdf_path.with_suffix(".meta.json")
        user_meta = {}
        if meta_path.exists():
            try:
                user_meta = json.loads(meta_path.read_text(encoding="utf-8"))
                logger.info("üìé Meta utilisateur d√©tect√©e")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Meta invalide: {e}")

        # Ajouter document_type_id aux m√©tadonn√©es si fourni
        if document_type_id:
            user_meta["document_type_id"] = document_type_id

        # En mode TEXT-ONLY, MegaParse extrait le texte (comme PPTX)
        # En mode VISION, on utilise pdftotext pour avoir le texte complet
        pdf_text = None
        if use_vision:
            pdf_text = extract_text_from_pdf(pdf_path)
            gpt_meta = analyze_pdf_metadata(pdf_text, pdf_path.name)
            doc_meta = {**user_meta, **gpt_meta}
        else:
            # En TEXT-ONLY, on extrait les m√©tadonn√©es apr√®s MegaParse
            doc_meta = user_meta

        # G√©n√©rer prompt contextualis√© si document_type_id fourni
        custom_prompt = None
        if document_type_id:
            try:
                from knowbase.api.services.document_type_service import DocumentTypeService
                from knowbase.db import SessionLocal

                logger.info(f"üéØ G√©n√©ration du prompt contextualis√© pour document_type_id: {document_type_id}")
                session = SessionLocal()
                try:
                    doc_type_service = DocumentTypeService(session)
                    custom_prompt = doc_type_service.generate_extraction_prompt(
                        document_type_id=document_type_id,
                        slide_content="{slide_content}"
                    )
                    logger.info(f"‚úÖ Prompt contextualis√© g√©n√©r√© ({len(custom_prompt)} caract√®res)")
                finally:
                    session.close()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de g√©n√©rer le prompt contextualis√©: {e}")
                custom_prompt = None

        total_chunks = 0

        if use_vision:
            # ===== MODE VISION : Traitement page par page avec images =====
            logger.info("üñºÔ∏è Mode VISION: G√©n√©ration PNG des pages")
            images = convert_from_path(str(pdf_path))
            image_paths = {}
            for i, img in enumerate(images, start=1):
                img_path = SLIDES_PNG / f"{pdf_path.stem}_page_{i}.png"
                img.save(img_path, "PNG")
                image_paths[i] = img_path

            for page_index, img_path in image_paths.items():
                logger.info(f"Page {page_index}/{len(image_paths)} [VISION]")

                # Utiliser GPT-4 Vision avec l'image
                chunks = ask_gpt_slide_analysis(
                    img_path, pdf_text, pdf_path.name, page_index, custom_prompt
                )
                logger.info(f"Page {page_index} [VISION]: chunks = {len(chunks)}")
                ingest_chunks(chunks, doc_meta, pdf_path.stem, page_index)
                total_chunks += len(chunks)

                # Envoyer heartbeat toutes les 3 pages (pour documents longs)
                if page_index % 3 == 0:
                    try:
                        from knowbase.ingestion.queue.jobs import send_worker_heartbeat
                        send_worker_heartbeat()
                    except Exception:
                        pass  # Ignorer si pas dans un contexte RQ

        else:
            # ===== MODE TEXT-ONLY : D√©coupage intelligent avec MegaParse =====
            from knowbase.ingestion.parsers.megaparse_pdf import parse_pdf_with_megaparse

            logger.info("üìö Mode TEXT-ONLY: D√©coupage MegaParse en blocs s√©mantiques")

            try:
                semantic_blocks = parse_pdf_with_megaparse(pdf_path, use_vision=False)
                logger.info(f"‚úÖ MegaParse: {len(semantic_blocks)} blocs s√©mantiques extraits")

                # Extraire le texte complet depuis les blocs pour les m√©tadonn√©es
                megaparse_text = "\n\n".join(block.get('content', '') for block in semantic_blocks)
                gpt_meta = analyze_pdf_metadata(megaparse_text[:8000], pdf_path.name)  # Limiter √† 8000 chars
                doc_meta = {**user_meta, **gpt_meta}
                logger.info(f"‚úÖ M√©tadonn√©es extraites depuis MegaParse")

            except Exception as e:
                logger.error(f"‚ùå MegaParse √©chou√©: {e}")
                logger.warning("‚ö†Ô∏è Fallback: utilisation de l'ancienne m√©thode page par page")
                # Fallback sur l'ancienne m√©thode si MegaParse √©choue
                semantic_blocks = []

            if semantic_blocks:
                # Traiter chaque bloc s√©mantique (au lieu de chaque page)
                for block_index, block in enumerate(semantic_blocks, start=1):
                    # Garbage collection p√©riodique pour lib√©rer la m√©moire
                    if block_index % 100 == 0:
                        import gc
                        gc.collect()
                        logger.info(f"üßπ Garbage collection effectu√© apr√®s {block_index} blocs")

                    block_type = block.get('block_type', 'text')
                    block_title = block.get('title')
                    block_content = block.get('content', '')

                    if not block_content or len(block_content) < 20:
                        logger.debug(f"Bloc {block_index} ignor√© (trop court: {len(block_content)} chars)")
                        continue

                    logger.info(
                        f"Bloc {block_index}/{len(semantic_blocks)} [{block_type}]: "
                        f"{block_title or 'Sans titre'[:50]}"
                    )

                    # Analyser chaque bloc s√©mantique avec Claude Haiku
                    result = ask_gpt_block_analysis_text_only(
                        block_content=block_content,
                        block_type=block_type,
                        block_title=block_title,
                        source_name=pdf_path.name,
                        block_index=block_index,
                        custom_prompt=custom_prompt
                    )

                    concepts = result.get("concepts", [])
                    facts = result.get("facts", [])
                    entities = result.get("entities", [])
                    relations = result.get("relations", [])

                    logger.info(
                        f"Bloc {block_index} [TEXT-ONLY]: {len(concepts)} concepts + "
                        f"{len(facts)} facts + {len(entities)} entities + {len(relations)} relations"
                    )

                    # Ingest concepts dans Qdrant (comme avant)
                    # Ajouter m√©tadonn√©es du bloc s√©mantique
                    chunks_compat = [
                        {
                            "text": c.get("full_explanation", ""),
                            "meta": {
                                **c.get("meta", {}),
                                "block_type": block_type,
                                "block_title": block_title,
                                "block_index": block_index,
                                "page_range": block.get('page_range', (1, 1)),
                            }
                        }
                        for c in concepts
                    ]
                    ingest_chunks(chunks_compat, doc_meta, pdf_path.stem, block_index)
                    total_chunks += len(chunks_compat)

                    # Ing√©rer facts, entities, relations dans Neo4j
                    if facts or entities or relations:
                        ingest_knowledge_to_neo4j(
                            facts=facts,
                            entities=entities,
                            relations=relations,
                            document_id=pdf_path.stem,
                            source_name=pdf_path.name
                        )

                    # Heartbeat tous les 5 blocs (au lieu de 3 pages)
                    if block_index % 5 == 0:
                        try:
                            from knowbase.ingestion.queue.jobs import send_worker_heartbeat
                            send_worker_heartbeat()
                        except Exception:
                            pass

            else:
                # Fallback sur l'ancienne m√©thode page par page si MegaParse a √©chou√©
                logger.warning("‚ö†Ô∏è Utilisation m√©thode page par page (fallback)")

                # Extraire le texte avec pdftotext pour le fallback
                if pdf_text is None:
                    pdf_text = extract_text_from_pdf(pdf_path)
                    gpt_meta = analyze_pdf_metadata(pdf_text, pdf_path.name)
                    doc_meta = {**user_meta, **gpt_meta}

                for page_index in range(1, 100):  # Limite arbitraire
                    result = ask_gpt_page_analysis_text_only(
                        pdf_text, pdf_path.name, page_index, custom_prompt
                    )
                    concepts = result.get("concepts", [])
                    if not concepts:
                        break
                    logger.info(f"Page {page_index} [TEXT-ONLY FALLBACK]: {len(concepts)} concepts")
                    chunks_compat = [{"text": c.get("full_explanation", ""), "meta": c.get("meta", {})} for c in concepts]
                    ingest_chunks(chunks_compat, doc_meta, pdf_path.stem, page_index)
                    total_chunks += len(chunks_compat)

        # ===== OSMOSE Pure - Traitement s√©mantique UNIQUEMENT =====
        # REMPLACE l'ingestion legacy (Qdrant "knowbase" + Neo4j entities/relations)
        # Tout passe maintenant par le Proto-KG (concepts canoniques cross-linguals)
        logger.info("=" * 80)
        logger.info("[OSMOSE PURE] Lancement du traitement s√©mantique (remplace ingestion legacy)")
        logger.info("=" * 80)

        try:
            from knowbase.ingestion.osmose_integration import process_document_with_osmose

            # R√©cup√©rer le texte complet (disponible dans megaparse_text ou pdf_text)
            full_text = None
            if not use_vision and 'megaparse_text' in locals():
                full_text = megaparse_text
            elif pdf_text:
                full_text = pdf_text

            if full_text and len(full_text) >= 100:
                # Appeler OSMOSE Pure de mani√®re asynchrone
                # AUCUN storage legacy (ni Qdrant "knowbase", ni Neo4j entities/relations)
                import asyncio
                osmose_result = asyncio.run(
                    process_document_with_osmose(
                        document_id=pdf_path.stem,
                        document_title=pdf_path.name,
                        document_path=pdf_path,
                        text_content=full_text,
                        tenant_id="default"
                    )
                )

                if osmose_result.osmose_success:
                    logger.info("=" * 80)
                    logger.info(
                        f"[OSMOSE PURE] ‚úÖ Traitement r√©ussi:\n"
                        f"  - {osmose_result.canonical_concepts} concepts canoniques\n"
                        f"  - {osmose_result.concept_connections} connexions cross-documents\n"
                        f"  - {osmose_result.topics_segmented} topics segment√©s\n"
                        f"  - Proto-KG: {osmose_result.proto_kg_concepts_stored} concepts + {osmose_result.proto_kg_relations_stored} relations + {osmose_result.proto_kg_embeddings_stored} embeddings\n"
                        f"  - Dur√©e: {osmose_result.osmose_duration_seconds:.1f}s"
                    )
                    logger.info("=" * 80)
                else:
                    logger.error(f"[OSMOSE PURE] ‚ùå Traitement √©chou√©: {osmose_result.osmose_error}")
                    raise Exception(f"OSMOSE processing failed: {osmose_result.osmose_error}")

            else:
                error_msg = f"Text too short ({len(full_text) if full_text else 0} chars)"
                logger.error(f"[OSMOSE PURE] ‚ùå {error_msg}")
                raise Exception(error_msg)

        except Exception as e:
            # En mode OSMOSE Pure, une erreur OSMOSE = √©chec complet de l'ingestion
            logger.error(f"[OSMOSE PURE] ‚ùå Erreur traitement s√©mantique: {e}", exc_info=True)
            status_file.write_text("error")
            raise  # Re-raise pour arr√™ter le traitement

        # ===== Fin OSMOSE Pure =====

        try:
            DOCS_DONE.mkdir(parents=True, exist_ok=True)
            shutil.move(str(pdf_path), str(DOCS_DONE / f"{pdf_path.stem}.pdf"))
            if meta_path.exists():
                shutil.move(
                    str(meta_path), str(DOCS_DONE / f"{pdf_path.stem}.meta.json")
                )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è D√©placement termin√© avec avertissement: {e}")

        status_file.write_text("done")
        logger.info(f"‚úÖ Termin√©: {pdf_path.name} ‚Äî total chunks: {total_chunks}")

    except Exception as e:
        logger.error(f"‚ùå Erreur durant {pdf_path.name}: {e}")
        try:
            status_file.write_text("error")
        except Exception:
            pass


def main():
    ensure_dirs()
    logger.info("üîé Scan du dossier DOCS_IN")
    if not DOCS_IN.exists():
        logger.error(f"‚ùå DOCS_IN n'existe pas: {DOCS_IN}")
        return

    files = list(DOCS_IN.glob("*.pdf"))
    logger.info(f"üì¶ Fichiers .pdf d√©tect√©s: {len(files)}")
    if not files:
        logger.info("‚ÑπÔ∏è Aucun .pdf √† traiter")
        return

    for file in files:
        logger.info(f"‚û°Ô∏è Fichier d√©tect√©: {file.name}")
        process_pdf(file)


if __name__ == "__main__":
    main()
