"""
Pipeline intelligent pour le remplissage automatique de RFP Excel.

Nouvelle approche bas√©e sur l'analyse LLM :
1. Extraction des lignes avec contenu (non merg√©es)
2. Analyse LLM batch pour filtrer/reformuler/fusionner les questions
3. Traitement optimis√© avec mapping vers Excel

Am√©liorations par rapport √† fill_excel_pipeline.py :
- √âvite les erreurs sur cellules merg√©es
- Meilleure d√©tection des questions vs en-t√™tes
- Fusion des questions multi-lignes
- Reformulation optimis√©e pour la recherche vectorielle
- G√©n√©ralisation des noms d'entreprises (sauf SAP)
"""

import json
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass

import pandas as pd
from openai.types.chat import ChatCompletionUserMessageParam
from openpyxl import load_workbook
from qdrant_client.models import FieldCondition, Filter, MatchValue, ScoredPoint

from knowbase.common.logging import setup_logging
from knowbase.common.clients import (
    get_qdrant_client,
    get_sentence_transformer,
)
from knowbase.common.llm_router import LLMRouter, TaskType, get_llm_router

from knowbase.config.settings import get_settings
from knowbase.config.paths import ensure_directories


class NoAnswerFoundError(Exception):
    """Exception lev√©e quand aucune r√©ponse pertinente n'est trouv√©e."""

    pass


@dataclass
class ExtractedRow:
    """Repr√©sente une ligne extraite du fichier Excel."""

    excel_row: int  # Num√©ro de ligne Excel (1-based)
    question_text: str  # Texte de la question
    question_cell_merged: bool  # Si la cellule question est merg√©e
    answer_cell_merged: bool  # Si la cellule r√©ponse est merg√©e


@dataclass
class ProcessedQuestion:
    """Repr√©sente une question apr√®s analyse LLM."""

    excel_rows: List[int]  # Lignes Excel sources (peut √™tre plusieurs si fusion)
    original_texts: List[str]  # Textes originaux
    category: str  # "QUESTION", "HEADER", "UNCLEAR"
    processed_question: Optional[str]  # Question reformul√©e (si QUESTION)
    dependency_type: str  # "INDEPENDENT", "DEPENDENT"
    referenced_rows: List[int]  # Lignes r√©f√©renc√©es (si DEPENDENT)
    transformations: List[str]  # Liste des transformations appliqu√©es
    company_replacements: Dict[str, str]  # Remplacements effectu√©s
    confidence: float  # Confiance du LLM (0-1)


# === CONFIGURATION ===
settings = get_settings()
LOGS_DIR = settings.logs_dir
ensure_directories([LOGS_DIR])
REJECT_LOG = LOGS_DIR / "rejected_chunks.log"
NO_MATCH_LOG = LOGS_DIR / "questions_no_match.log"
logger = setup_logging(
    LOGS_DIR, "smart_fill_excel_debug.log", "smart_fill_excel_pipeline"
)

# Mod√®les et clients
EMB_MODEL_NAME = settings.embeddings_model
QA_COLLECTION_NAME = settings.qdrant_qa_collection
MAIN_COLLECTION_NAME = settings.qdrant_collection

# Seuils de confiance pour la recherche en cascade
QA_CONFIDENCE_THRESHOLD = 0.85
KB_CONFIDENCE_THRESHOLD = 0.70

# Entreprises √† pr√©server (SAP et solutions SAP)
PRESERVE_COMPANIES = {
    "SAP",
    "S/4HANA",
    "SuccessFactors",
    "Ariba",
    "Concur",
    "Fieldglass",
    "SAP HANA",
    "Business One",
    "ByDesign",
    "Qualtrics",
}

# Clients globaux
model = get_sentence_transformer(EMB_MODEL_NAME)
llm_router = get_llm_router()
qdrant_client = get_qdrant_client()


def load_meta(meta_path: str | Path) -> Dict[str, Any]:
    """Charge le fichier de m√©tadonn√©es."""
    with open(meta_path, "r", encoding="utf-8") as f:
        return json.load(f)


def excel_col_letter_to_index(col_letter: str) -> int:
    """Convertit une lettre de colonne Excel (A, B, AA) en index 0-based."""
    col_letter = col_letter.upper()
    col_idx = 0
    for i, c in enumerate(reversed(col_letter)):
        col_idx += (ord(c) - ord("A") + 1) * (26**i)
    return col_idx - 1


def extract_excel_rows(
    xlsx_path: str | Path, target_sheet: str, question_col_idx: int, answer_col_idx: int
) -> List[ExtractedRow]:
    """
    Extrait toutes les lignes avec du contenu dans la colonne question.
    Identifie aussi les cellules merg√©es pour √©viter les erreurs d'√©criture.
    """
    wb = load_workbook(xlsx_path)
    ws = wb[target_sheet]
    df = pd.read_excel(xlsx_path, header=None, sheet_name=target_sheet)

    # Identifier les cellules merg√©es
    merged_cells: set[str] = set()
    for rng in ws.merged_cells.ranges:
        for row in range(rng.min_row, rng.max_row + 1):
            for col in range(rng.min_col, rng.max_col + 1):
                merged_cells.add(ws.cell(row=row, column=col).coordinate)

    extracted_rows = []

    for row_idx in range(len(df)):
        try:
            # V√©rifier si la cellule question a du contenu
            raw_question = df.iat[row_idx, question_col_idx]
            if pd.isna(raw_question) or str(raw_question).strip() == "":
                continue

            question_text = str(raw_question).strip()
            excel_row = row_idx + 1  # Conversion en 1-based pour Excel

            # V√©rifier si les cellules sont merg√©es
            question_cell_coord = ws.cell(
                row=excel_row, column=question_col_idx + 1
            ).coordinate
            answer_cell_coord = ws.cell(
                row=excel_row, column=answer_col_idx + 1
            ).coordinate

            question_cell_merged = question_cell_coord in merged_cells
            answer_cell_merged = answer_cell_coord in merged_cells

            extracted_row = ExtractedRow(
                excel_row=excel_row,
                question_text=question_text,
                question_cell_merged=question_cell_merged,
                answer_cell_merged=answer_cell_merged,
            )

            extracted_rows.append(extracted_row)
            logger.debug(
                f"Ligne {excel_row}: '{question_text[:50]}...' - Q_merged: {question_cell_merged}, A_merged: {answer_cell_merged}"
            )

        except Exception as e:
            logger.debug(f"Erreur extraction ligne {row_idx}: {e}")
            continue

    logger.info(
        f"Extraction termin√©e: {len(extracted_rows)} lignes avec contenu trouv√©es"
    )
    return extracted_rows


def analyze_questions_with_llm(
    extracted_rows: List[ExtractedRow],
) -> List[ProcessedQuestion]:
    """
    Analyse batch des questions avec LLM pour :
    1. Filtrer questions vs en-t√™tes
    2. Fusionner questions multi-lignes
    3. Reformuler pour optimiser la recherche
    4. G√©n√©raliser les noms d'entreprises (sauf SAP)
    """
    if not extracted_rows:
        return []

    # Pr√©parer les donn√©es pour le LLM
    questions_data = []
    for i, row in enumerate(extracted_rows):
        questions_data.append(
            {
                "index": i,
                "excel_row": row.excel_row,
                "text": row.question_text,
                "merged_cells": row.question_cell_merged or row.answer_cell_merged,
            }
        )

    # Prompt pour l'analyse intelligente
    preserve_companies_str = ", ".join(PRESERVE_COMPANIES)

    prompt = f"""You are analyzing questions from an RFP Excel file for SAP solutions.

TASK: Analyze each row and return a structured analysis with intelligent question dependency handling.

COMPANIES TO PRESERVE: {preserve_companies_str}
- NEVER replace these (they refer to us as solution provider)
- Replace OTHER company names with "customer", "client", "customer organization"

INSTRUCTIONS:

1. CLASSIFY each row as:
   - "QUESTION": Real question or statement requiring validation
   - "HEADER": Section title, chapter header, separator
   - "UNCLEAR": Ambiguous cases

2. HANDLE QUESTION DEPENDENCIES:
   - Analyze each question in sequence
   - If a question references a previous question (using "If yes/no", "In case of", "For the above", etc.), mark it as DEPENDENT
   - DEPENDENT questions should be reformulated to include context from the referenced question
   - INDEPENDENT questions should be processed standalone

   Examples:
   Row 23: "Does your application require technical components on servers?" ‚Üí INDEPENDENT
   Row 24: "If yes, please provide details" ‚Üí DEPENDENT on Row 23

   Result:
   - Question 1 (standalone): "Does your application require technical components on customer servers?"
   - Question 2 (with context): "If your application requires technical components on customer servers, please provide details of the components and technical flows"

3. REFORMULATE questions:
   - For INDEPENDENT: Optimize for semantic search, make standalone
   - For DEPENDENT: Include full context from referenced question
   - Keep the same language as original
   - Make questions comprehensive and searchable

4. GENERALIZE company names:
   - Replace specific companies (except {preserve_companies_str}) with generic terms
   - Example: "Microsoft servers" ‚Üí "customer servers"

DEPENDENCY DETECTION PATTERNS:
- "If yes/no..." ‚Üí References previous question
- "In case of..." ‚Üí May reference previous context
- "For the above..." ‚Üí References previous question
- "Additionally..." ‚Üí May be follow-up
- "Please specify..." ‚Üí May be sub-question
- Question ending with "?" followed by instruction ‚Üí Likely independent questions

INPUT DATA:
{json.dumps(questions_data, indent=2, ensure_ascii=False)}

OUTPUT FORMAT (valid JSON):
{{
  "processed_questions": [
    {{
      "excel_rows": [45],
      "original_texts": ["Original question text"],
      "category": "QUESTION|HEADER|UNCLEAR",
      "processed_question": "Reformulated question (null if HEADER)",
      "dependency_type": "INDEPENDENT|DEPENDENT",
      "referenced_rows": [23],
      "transformations": ["reformulated", "generalized", "contextualized"],
      "company_replacements": {{"CompanyName": "customer"}},
      "confidence": 0.95
    }}
  ]
}}

IMPORTANT:
- Process questions sequentially to detect dependencies
- For DEPENDENT questions, include full context from referenced questions
- Return ONLY valid JSON, no other text"""

    try:
        user_message: ChatCompletionUserMessageParam = {
            "role": "user",
            "content": prompt,
        }

        logger.info(f"Envoi de {len(extracted_rows)} questions au LLM pour analyse...")
        content = llm_router.complete(TaskType.RFP_QUESTION_ANALYSIS, [user_message])

        if not isinstance(content, str):
            raise ValueError("LLM response is not a string")

        # Nettoyer les balises markdown avant parsing JSON
        if content.startswith('```json'):
            content = content.replace('```json\n', '').replace('\n```', '')
        elif content.startswith('```'):
            content = content.replace('```\n', '').replace('\n```', '')

        # V√©rifier que le JSON est complet avant parsing
        content = content.strip()
        if not content.endswith('}'):
            logger.warning(f"JSON tronqu√© d√©tect√©. Longueur: {len(content)} caract√®res")
            logger.debug(f"JSON se termine par: {content[-50:]}")
            raise ValueError("R√©ponse LLM tronqu√©e - JSON incomplet")

        # Parser la r√©ponse JSON
        response_data = json.loads(content)
        processed_questions = []

        for item in response_data.get("processed_questions", []):
            processed_question = ProcessedQuestion(
                excel_rows=item.get("excel_rows", []),
                original_texts=item.get("original_texts", []),
                category=item.get("category", "UNCLEAR"),
                processed_question=item.get("processed_question"),
                dependency_type=item.get("dependency_type", "INDEPENDENT"),
                referenced_rows=item.get("referenced_rows", []),
                transformations=item.get("transformations", []),
                company_replacements=item.get("company_replacements", {}),
                confidence=item.get("confidence", 0.0),
            )
            processed_questions.append(processed_question)

        # Statistiques et logs d√©taill√©s
        questions_count = sum(
            1 for q in processed_questions if q.category == "QUESTION"
        )
        headers_count = sum(1 for q in processed_questions if q.category == "HEADER")
        dependent_count = sum(
            1 for q in processed_questions if q.dependency_type == "DEPENDENT"
        )
        independent_count = sum(
            1 for q in processed_questions if q.dependency_type == "INDEPENDENT"
        )

        logger.info(
            f"Analyse LLM termin√©e: {questions_count} questions, {headers_count} en-t√™tes identifi√©s"
        )
        logger.info(
            f"D√©pendances: {independent_count} ind√©pendantes, {dependent_count} d√©pendantes"
        )

        # Log des d√©pendances d√©tect√©es
        for q in processed_questions:
            if q.dependency_type == "DEPENDENT" and q.referenced_rows:
                logger.info(
                    f"Question d√©pendante ligne {q.excel_rows[0]} ‚Üí r√©f√©rence lignes {q.referenced_rows}"
                )
                logger.debug(f"  Original: {q.original_texts[0][:60]}...")
                logger.debug(
                    f"  Reformul√©e: {q.processed_question[:80]}..."
                    if q.processed_question
                    else ""
                )

        return processed_questions

    except Exception as e:
        logger.error(f"Erreur analyse LLM: {e}")
        # Fallback: traiter toutes les lignes comme des questions
        fallback_questions = []
        for row in extracted_rows:
            if not (row.question_cell_merged or row.answer_cell_merged):
                fallback_question = ProcessedQuestion(
                    excel_rows=[row.excel_row],
                    original_texts=[row.question_text],
                    category="QUESTION",
                    processed_question=row.question_text,
                    dependency_type="INDEPENDENT",
                    referenced_rows=[],
                    transformations=["fallback"],
                    company_replacements={},
                    confidence=0.5,
                )
                fallback_questions.append(fallback_question)

        logger.warning(
            f"Fallback activ√©: {len(fallback_questions)} questions trait√©es sans analyse LLM"
        )
        return fallback_questions


def search_and_answer_questions(
    processed_questions: List[ProcessedQuestion],
    solution: str,
    extend_search_to_kb: bool = False,
) -> Dict[int, str]:
    """
    Recherche et g√©n√®re les r√©ponses pour les questions valid√©es.
    Retourne un mapping excel_row -> answer pour les lignes √† remplir.
    """
    # Importer les fonctions de recherche du pipeline original
    from knowbase.ingestion.pipelines.fill_excel_pipeline import (
        search_hybrid,
        filter_chunks_with_gpt,
        build_gpt_answer,
        NoAnswerFoundError,
    )

    answers = {}
    valid_questions = [
        q
        for q in processed_questions
        if q.category == "QUESTION" and q.processed_question
    ]

    logger.info(f"Traitement de {len(valid_questions)} questions valid√©es...")

    for i, question_data in enumerate(valid_questions):
        question = question_data.processed_question
        excel_row = question_data.excel_rows[
            0
        ]  # Prendre la premi√®re ligne pour l'√©criture

        logger.info(
            f"üí¨ Question {i+1}/{len(valid_questions)} (ligne {excel_row}): {question[:80]}..."
        )

        try:
            # Recherche vectorielle
            results, source = search_hybrid(
                question, solution, extend_search_to_kb=extend_search_to_kb
            )

            if not results:
                logger.info(f"Aucun chunk trouv√© pour: {question}")
                with open(NO_MATCH_LOG, "a", encoding="utf-8") as nomatch_log:
                    nomatch_log.write(f"{question}\n")
                continue

            # Filtrage avec LLM
            filtered_chunks = filter_chunks_with_gpt(question, results)
            if not filtered_chunks:
                logger.info(f"Aucun chunk pertinent apr√®s filtrage pour: {question}")
                continue

            # G√©n√©ration de la r√©ponse
            answer = build_gpt_answer(question, filtered_chunks)

            if len(answer.strip()) < 20:
                logger.info(f"R√©ponse trop courte ignor√©e pour: {question}")
                continue

            answers[excel_row] = answer
            logger.info(f"‚úÖ R√©ponse g√©n√©r√©e pour ligne {excel_row}")

        except NoAnswerFoundError as e:
            logger.info(f"Aucune r√©ponse pertinente pour: {question}")
            with open(NO_MATCH_LOG, "a", encoding="utf-8") as nomatch_log:
                nomatch_log.write(f"{question}\n")
            continue
        except Exception as e:
            logger.error(f"Erreur traitement question ligne {excel_row}: {e}")
            continue

    logger.info(f"G√©n√©ration termin√©e: {len(answers)} r√©ponses cr√©√©es")
    return answers


def write_answers_to_excel(
    xlsx_path: str | Path,
    target_sheet: str,
    answers: Dict[int, str],
    answer_col_idx: int,
) -> None:
    """
    √âcrit les r√©ponses dans le fichier Excel aux lignes sp√©cifi√©es.
    """
    wb = load_workbook(xlsx_path)
    ws = wb[target_sheet]

    logger.info(f"√âcriture de {len(answers)} r√©ponses dans Excel...")

    for excel_row, answer in answers.items():
        try:
            ws.cell(row=excel_row, column=answer_col_idx + 1, value=answer)
            logger.debug(
                f"R√©ponse √©crite ligne {excel_row}, colonne {answer_col_idx + 1}"
            )
        except Exception as e:
            logger.error(f"Erreur √©criture ligne {excel_row}: {e}")
            continue

    wb.save(xlsx_path)
    logger.info(f"Fichier Excel sauvegard√©: {xlsx_path}")


def main(xlsx_path: str | Path, meta_path: str | Path, progress_callback=None) -> None:
    """Point d'entr√©e principal du pipeline intelligent."""

    meta = load_meta(meta_path)
    solution = meta.get("solution", "").strip()
    if not solution:
        logger.error("Meta 'solution' manquante ou invalide.")
        return

    extend_search_to_kb = meta.get("extend_search_to_kb", False)
    if isinstance(extend_search_to_kb, str):
        extend_search_to_kb = extend_search_to_kb.lower() in ("true", "1", "yes")

    logger.info(f"üöÄ D√©marrage pipeline intelligent - Solution: {solution}")
    logger.info(
        f"üîç Mode recherche: {'Q/A + KB' if extend_search_to_kb else 'Q/A uniquement'}"
    )

    # Progression
    if progress_callback:
        progress_callback(
            "Analyse du fichier", 10, 100, "Chargement du fichier Excel..."
        )

    # Configuration des colonnes
    wb = load_workbook(xlsx_path)
    target_sheet = None

    # D√©terminer l'onglet cible
    if "sheet_name" in meta and isinstance(meta["sheet_name"], str):
        specified_sheet = meta["sheet_name"].strip()
        if specified_sheet and specified_sheet in wb.sheetnames:
            if wb[specified_sheet].sheet_state == "visible":
                target_sheet = specified_sheet
                logger.info(f"üìã Onglet sp√©cifi√©: {specified_sheet}")

    if not target_sheet:
        visible_sheets = [s for s in wb.sheetnames if wb[s].sheet_state == "visible"]
        if not visible_sheets:
            logger.error("Aucun onglet visible dans le fichier Excel.")
            return
        target_sheet = visible_sheets[0]
        logger.info(f"üìã Onglet par d√©faut: {target_sheet}")

    question_col_idx = excel_col_letter_to_index(meta["question_col"])
    answer_col_idx = excel_col_letter_to_index(meta["answer_col"])

    # √âtape 1: Extraction des lignes
    if progress_callback:
        progress_callback(
            "Extraction", 20, 100, "Extraction des lignes avec contenu..."
        )

    extracted_rows = extract_excel_rows(
        xlsx_path, target_sheet, question_col_idx, answer_col_idx
    )

    if not extracted_rows:
        logger.warning("Aucune ligne avec contenu trouv√©e")
        if progress_callback:
            progress_callback("Termin√©", 100, 100, "Aucune ligne √† traiter")
        return

    # √âtape 2: Analyse LLM
    if progress_callback:
        progress_callback(
            "Analyse IA",
            40,
            100,
            f"Analyse intelligente de {len(extracted_rows)} lignes...",
        )

    processed_questions = analyze_questions_with_llm(extracted_rows)

    # √âtape 3: Recherche et r√©ponses
    if progress_callback:
        progress_callback(
            "G√©n√©ration r√©ponses", 60, 100, "Recherche et g√©n√©ration des r√©ponses..."
        )

    answers = search_and_answer_questions(
        processed_questions, solution, extend_search_to_kb
    )

    # √âtape 4: √âcriture Excel
    if progress_callback:
        progress_callback(
            "Sauvegarde", 90, 100, f"√âcriture de {len(answers)} r√©ponses..."
        )

    write_answers_to_excel(xlsx_path, target_sheet, answers, answer_col_idx)

    # Finalisation
    if progress_callback:
        progress_callback(
            "Termin√©", 100, 100, f"Pipeline termin√©: {len(answers)} r√©ponses g√©n√©r√©es"
        )

    # Statistiques finales
    questions_count = sum(1 for q in processed_questions if q.category == "QUESTION")
    headers_count = sum(1 for q in processed_questions if q.category == "HEADER")

    logger.info(f"üéâ Pipeline intelligent termin√©:")
    logger.info(f"  - {len(extracted_rows)} lignes extraites")
    logger.info(f"  - {questions_count} questions identifi√©es")
    logger.info(f"  - {headers_count} en-t√™tes filtr√©s")
    logger.info(f"  - {len(answers)} r√©ponses g√©n√©r√©es")


if __name__ == "__main__":
    import sys

    if len(sys.argv) != 3:
        print("Usage: python smart_fill_excel_pipeline.py <questions.xlsx> <meta.json>")
        logger.error(
            "Usage: python smart_fill_excel_pipeline.py <questions.xlsx> <meta.json>"
        )
    else:
        main(sys.argv[1], sys.argv[2])
