"""
üåä OSMOSE Semantic Intelligence V2.1 - Embeddings Multilingues

Gestionnaire embeddings cross-lingual avec multilingual-e5-large
"""

from sentence_transformers import SentenceTransformer
from typing import List, Optional
import numpy as np
from functools import lru_cache
import logging

logger = logging.getLogger(__name__)


class MultilingualEmbedder:
    """
    Embeddings multilingues avec cache.

    Utilise multilingual-e5-large (1024 dimensions) pour:
    - Cross-lingual similarity (FR ‚Üî EN ‚Üî DE)
    - Canonicalization de concepts
    - Topic clustering

    Phase 1 V2.1 - Semaine 1
    """

    def __init__(self, config):
        """
        Initialise le gestionnaire d'embeddings.

        Args:
            config: Configuration SemanticConfig avec config.embeddings
        """
        self.config = config

        # Charger le mod√®le multilingual-e5-large
        logger.info(f"[OSMOSE] Loading embeddings model: {config.embeddings.model}...")
        self.model = SentenceTransformer(
            config.embeddings.model,
            device=config.embeddings.device
        )
        logger.info(
            f"[OSMOSE] ‚úÖ Embeddings model loaded: {config.embeddings.model} "
            f"({config.embeddings.dimension}D, device: {config.embeddings.device})"
        )

    @lru_cache(maxsize=1000)
    def encode_cached(self, text: str) -> np.ndarray:
        """
        Encode un texte avec cache LRU.

        Utile pour concepts r√©p√©t√©s (ex: "ISO 27001" appara√Æt 50 fois).

        Args:
            text: Texte √† encoder

        Returns:
            np.ndarray: Vecteur embedding (1024D)
        """
        embedding = self.model.encode(
            text,
            convert_to_numpy=True,
            normalize_embeddings=self.config.embeddings.normalize
        )
        return embedding

    def encode(self, texts: List[str], prefix_type: Optional[str] = None) -> np.ndarray:
        """
        Encode un batch de textes.

        Args:
            texts: Liste de textes √† encoder
            prefix_type: Type de pr√©fixe e5 ("query", "passage", ou None)
                        Recommandation OpenAI: +2-5% pr√©cision retrieval

        Returns:
            np.ndarray: Matrice embeddings (N x 1024)
        """
        if not texts:
            return np.array([])

        # Ajouter pr√©fixes e5 si demand√© (recommandation OpenAI)
        if prefix_type == "query":
            texts = [f"query: {text}" for text in texts]
        elif prefix_type == "passage":
            texts = [f"passage: {text}" for text in texts]

        embeddings = self.model.encode(
            texts,
            batch_size=self.config.embeddings.batch_size,
            convert_to_numpy=True,
            normalize_embeddings=self.config.embeddings.normalize,
            show_progress_bar=False
        )

        logger.debug(
            f"[OSMOSE] Encoded {len(texts)} texts (prefix={prefix_type}) ‚Üí {embeddings.shape}"
        )

        return embeddings

    def encode_single(self, text: str, use_cache: bool = True) -> np.ndarray:
        """
        Encode un texte unique.

        Args:
            text: Texte √† encoder
            use_cache: Utiliser le cache LRU (default: True)

        Returns:
            np.ndarray: Vecteur embedding (1024D)
        """
        if use_cache:
            return self.encode_cached(text)
        else:
            return self.model.encode(
                text,
                convert_to_numpy=True,
                normalize_embeddings=self.config.embeddings.normalize
            )

    def similarity(
        self,
        text1: str,
        text2: str
    ) -> float:
        """
        Calcule la similarit√© cosine entre deux textes.

        Cross-lingual : "authentication" (EN) vs "authentification" (FR) ‚Üí ~0.92

        Args:
            text1: Premier texte
            text2: Deuxi√®me texte

        Returns:
            float: Similarit√© cosine [0.0, 1.0]
        """
        emb1 = self.encode_cached(text1)
        emb2 = self.encode_cached(text2)

        # Cosine similarity (vecteurs d√©j√† normalis√©s si config.normalize=True)
        if self.config.embeddings.normalize:
            similarity = float(np.dot(emb1, emb2))
        else:
            similarity = float(
                np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            )

        return similarity

    def similarity_matrix(
        self,
        texts1: List[str],
        texts2: Optional[List[str]] = None
    ) -> np.ndarray:
        """
        Calcule la matrice de similarit√© entre deux ensembles de textes.

        Si texts2 est None, calcule la similarit√© entre tous les textes de texts1.

        Args:
            texts1: Premier ensemble de textes
            texts2: Deuxi√®me ensemble (optionnel)

        Returns:
            np.ndarray: Matrice similarit√© (N x M)
        """
        emb1 = self.encode(texts1)

        if texts2 is None:
            emb2 = emb1
        else:
            emb2 = self.encode(texts2)

        # Similarit√© cosine matricielle
        if self.config.embeddings.normalize:
            # Vecteurs d√©j√† normalis√©s ‚Üí produit scalaire = cosine
            similarity_matrix = np.dot(emb1, emb2.T)
        else:
            # Normaliser puis produit scalaire
            emb1_norm = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)
            emb2_norm = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)
            similarity_matrix = np.dot(emb1_norm, emb2_norm.T)

        logger.debug(
            f"[OSMOSE] Similarity matrix: {similarity_matrix.shape}"
        )

        return similarity_matrix

    def find_similar(
        self,
        query_text: str,
        candidate_texts: List[str],
        threshold: float = 0.8,
        top_k: int = 5,
        use_e5_prefixes: bool = False
    ) -> List[tuple]:
        """
        Trouve les textes les plus similaires au query.

        Utile pour canonicalization (trouver concepts similaires cross-lingual).

        Args:
            query_text: Texte de recherche
            candidate_texts: Liste de textes candidats
            threshold: Seuil de similarit√© minimum
            top_k: Nombre max de r√©sultats
            use_e5_prefixes: Utiliser pr√©fixes e5 "query:" et "passage:" (+2-5% pr√©cision)

        Returns:
            List[tuple]: Liste de (index, text, similarity) tri√©e par similarit√© d√©croissante
        """
        if not candidate_texts:
            return []

        # Utiliser pr√©fixes e5 si demand√© (recommandation OpenAI)
        if use_e5_prefixes:
            query_emb = self.model.encode(
                f"query: {query_text}",
                convert_to_numpy=True,
                normalize_embeddings=self.config.embeddings.normalize
            )
            candidate_embs = self.encode(candidate_texts, prefix_type="passage")
        else:
            query_emb = self.encode_cached(query_text)
            candidate_embs = self.encode(candidate_texts)

        # Calcul similarit√©s
        if self.config.embeddings.normalize:
            similarities = np.dot(candidate_embs, query_emb)
        else:
            query_norm = query_emb / np.linalg.norm(query_emb)
            candidate_norms = candidate_embs / np.linalg.norm(candidate_embs, axis=1, keepdims=True)
            similarities = np.dot(candidate_norms, query_norm)

        # Filtrer par threshold et top_k
        results = []
        for idx, similarity in enumerate(similarities):
            if similarity >= threshold:
                results.append((idx, candidate_texts[idx], float(similarity)))

        # Trier par similarit√© d√©croissante
        results.sort(key=lambda x: x[2], reverse=True)

        return results[:top_k]

    def clear_cache(self):
        """
        Vide le cache LRU.

        Utile si m√©moire limit√©e ou apr√®s traitement d'un gros batch.
        """
        self.encode_cached.cache_clear()
        logger.info("[OSMOSE] Embeddings cache cleared")

    def get_cache_info(self):
        """
        Retourne les statistiques du cache LRU.

        Returns:
            CacheInfo: hits, misses, maxsize, currsize
        """
        return self.encode_cached.cache_info()


# ===================================
# FACTORY PATTERN
# ===================================

_embedder_instance: Optional[MultilingualEmbedder] = None


def get_embedder(config) -> MultilingualEmbedder:
    """
    R√©cup√®re l'instance singleton du gestionnaire embeddings.

    Args:
        config: Configuration SemanticConfig

    Returns:
        MultilingualEmbedder: Instance unique
    """
    global _embedder_instance

    if _embedder_instance is None:
        _embedder_instance = MultilingualEmbedder(config)

    return _embedder_instance
